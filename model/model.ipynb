{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"test_fold=3 #choose fold from[0,1,2,3,4] ","metadata":{"execution":{"iopub.status.busy":"2023-09-03T16:07:08.415319Z","iopub.execute_input":"2023-09-03T16:07:08.415816Z","iopub.status.idle":"2023-09-03T16:07:08.434465Z","shell.execute_reply.started":"2023-09-03T16:07:08.415774Z","shell.execute_reply":"2023-09-03T16:07:08.433298Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"\n!pip install openpyxl\n!pip install xlrd==2.0.1\n!pip install Pandas==1.3.5\n# !pip install Pandas==1.1.5\n!pip install  gdown\nimport tensorflow as tf\n\nimport numpy as np\nimport pandas as pd\n\ndef normalize1(X, means1=None, std1=None, means2=None, std2=None, feat_filt=None, norm='tanh_norm'):\n    if std1 is None:\n        std1 = np.nanstd(X, axis=0)\n    if feat_filt is None:\n        feat_filt = std1!=0\n    X = X[:,feat_filt]\n    X = np.ascontiguousarray(X)\n    if means1 is None:\n        means1 = np.mean(X, axis=0)\n    X = (X-means1)/std1[feat_filt]\n    if norm == 'norm':\n        return(X, means1, std1, feat_filt)\n    elif norm == 'tanh':\n        return(np.tanh(X), means1, std1, feat_filt)\n    elif norm == 'tanh_norm':\n        X = np.tanh(X)\n        if means2 is None:\n            means2 = np.mean(X, axis=0)\n        if std2 is None:\n            std2 = np.std(X, axis=0)\n        X = (X-means2)/std2\n        X[:,std2==0]=0\n        return(X, means1, std1, means2, std2, feat_filt) \n\n\n    \ndef get_data():\n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  smiles=pd.read_excel('pubchem.xls', header=None)\n  smiles=np.array(smiles)\n\n\n  !gdown --id 10ztxKtGSVU7p9yPoCsTnapVLThWpjjYT\n  data_to_repeat=pd.read_excel('oneil.xlsx', header=None)\n  data_to_repeat=np.array(data_to_repeat)\n  \n\n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  unique_drugs=pd.read_excel('pubchem.xls', header=None)\n  unique_drugs=np.array(unique_drugs)         \n\n  !gdown --id 1rVt2qEH-LMzk86ig6c8Qjll-08LhsxBv\n  feature_cell=pd.read_excel('cell_expression875.xlsx')\n  feature_cell=np.array(feature_cell)\n  feature_cell[:,1:]=(2**feature_cell[:,1:])-1\n    \n  !gdown --id 14upx46iIPcO80y_tcvbaS3uwMkZfKLVw\n  redkit_drug=pd.read_excel('redkit_drug.xlsx',header=None)\n  redkit_drug=np.array(redkit_drug)\n  \n    \n#   !gdown --id 1Yf9YQRSq3Bf0bIwncpoANS5Ui1H3W_fH\n#   graph=pd.read_excel('graph_emb1.xlsx',header=None)\n\n#   !gdown 1uB-9BAHzw2OKrH4T1BEsYNSgj__RoYVr\n#   graph=pd.read_excel('views0a0123.xlsx',header=None)  \n    \n  !gdown 1nkNG2Z38WQgM1Ap8Wrg5aVbjC-NNUO6S\n  graph=pd.read_excel('views0123.xlsx',header=None)\n\n  graph=np.array(graph)\n  \n    \n  !gdown --id 1HQpJQ6yeZYnGK6Uy0AyIvzYsNkzBAeOD\n  model = tf.keras.models.load_model(\"model.h5\")\n\n  !gdown --id 1AZX2h806qMcMp63hnJB81JFYMjzEChpA\n  unique_drugs1=pd.read_excel('unique394_drugl.xlsx', header=None)\n  unique_drugs1=np.array(unique_drugs1)\n\n\n  return smiles[1:,1],data_to_repeat[1:,:],unique_drugs[1:,0],feature_cell,redkit_drug,model,graph,unique_drugs1\n\n\n\ndef repeat_smiles1(data_to_repeat,unique_drugs,feature,unique_cell,redkit_drug,graph):\n  unique_redkit_feature=redkit_drug[:,1:]\n  unique_redkit_name=redkit_drug[:,0]\n  unique_feature=unique_cell[:,1:]\n  unique_name=unique_cell[:,0]\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  c=data_to_repeat[:,2]\n  unique_drugs=feature[:,0]\n  feature=feature[:,1:]  \n  f_drug1=[]\n  f_drug2=[]\n  feature_cell=[]\n  redkit_d1=[]\n  redkit_d2=[]\n  graph1=[]\n  graph2=[]\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    cc=c[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    cc1=[m for m, v in enumerate(unique_name) if cc in v]\n    r1= [m for m, v in enumerate(unique_redkit_name) if n1 in v]\n    r2=[m for m, v in enumerate(unique_redkit_name) if n2 in v]\n    f_drug1.append(feature[k1[0]])\n    f_drug2.append(feature[k2[0]])\n    graph1.append(graph[k1[0]])\n    graph2.append(graph[k2[0]])\n    feature_cell.append(unique_feature[cc1[0]])\n    redkit_d1.append(unique_redkit_feature[r1[0]])\n    redkit_d2.append(unique_redkit_feature[r2[0]])\n\n  return f_drug1,f_drug2,feature_cell,redkit_d1,redkit_d2,graph1,graph2\n\n\n\n\ndef train_test_input(f_drug1,f_drug2,cell_line,index_train,index_test,synery,class1,redkit_d1,redkit_d2,graph1,graph2):\n  train_f_drug1=[]\n  train_f_drug2=[]\n  train_cell_line=[]\n  train_synergy=[]\n  train_class=[]\n  train_redkit_d1=[]\n  train_redkit_d2=[]\n  test_f_drug1=[]\n  test_f_drug2=[]\n  test_cell_line=[]\n  test_synergy=[]\n  test_class=[]\n  test_redkit_d1=[]\n  test_redkit_d2=[]\n  train_graph1=[]\n  test_graph1=[]\n  train_graph2=[]\n  test_graph2=[]\n  for i in range(len(index_train)):\n      \n      train_f_drug1.append(f_drug1[index_train[i]])\n      train_f_drug2.append(f_drug2[index_train[i]])\n      train_cell_line.append(cell_line[index_train[i]])\n      train_synergy.append(synergy[index_train[i]])\n      train_class.append(class1[index_train[i]])\n      train_redkit_d1.append(redkit_d1[index_train[i]])\n      train_redkit_d2.append(redkit_d2[index_train[i]])\n      train_graph1.append(graph1[index_train[i]])\n      train_graph2.append(graph2[index_train[i]])\n\n  for ii in range(len(index_test)):\n      \n      test_f_drug1.append(f_drug1[index_test[ii]])\n      test_f_drug2.append(f_drug2[index_test[ii]])\n      test_cell_line.append(cell_line[index_test[ii]])\n      test_synergy.append(synergy[index_test[ii]])\n      test_class.append(class1[index_test[ii]])\n      test_redkit_d1.append(redkit_d1[index_test[ii]])\n      test_redkit_d2.append(redkit_d2[index_test[ii]])\n      test_graph1.append(graph1[index_test[ii]])\n      test_graph2.append(graph2[index_test[ii]])\n\n  return train_f_drug1,train_f_drug2,train_cell_line,test_f_drug1,test_f_drug2,test_cell_line,train_synergy,train_class,test_synergy,test_class,train_redkit_d1,train_redkit_d2,test_redkit_d1,test_redkit_d2,train_graph1,test_graph1,train_graph2,test_graph2\n\n\n\ndef preprocess(index_train,index_test):\n    index_train1=[]\n    index_test1=[]\n    index_train2=(index_train)[0]\n    index_test2=(index_test)[0]\n    for i in range(len((index_train2))):\n        index_train1.append((index_train2[i]))\n        \n    for ii in range(len(index_test2)):\n        index_test1.append((index_test2[ii]))\n        \n    return index_train1,index_test1\n\ndef get_data_me2(s):\n    \n    \n\n    !gdown 1C7Z2ziPdQVzH3omIdIfyJa7VUmog4IIk\n    labels = pd.read_csv('oneil.csv', index_col=0) \n    \n    h=len(np.array(labels))\n    #labels are duplicated for the two different ways of ordering in the data\n    labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['fold']!=test_fold)\n    \n\n    idx_test = np.where(labels['fold']==test_fold)\n#     \n#    \n    return idx_train,idx_test\n\n\n\n\n\ndef convert_tobin(cc):\n    cb=[]\n    for i in range(len(cc)):\n        if(cc[i]>=0.5):\n            cb.append(1)\n        else:\n            cb.append(0)\n    return cb\n\n\ndef norm1(train_cell_line,test_cell_line,norm=\"tanh_norm\"):\n# norm = \"norm\"\n    if norm == \"tanh_norm\":\n        train_cell_line, mean, std, mean2, std2, feat_filt = normalize1(train_cell_line, norm=norm)\n        test_cell_line, mean, std, mean2, std2, feat_filt = normalize1(test_cell_line, mean, std, mean2, std2, \n                                                              feat_filt=feat_filt, norm=norm)\n    else:\n        train_cell_line, mean, std, feat_filt = normalize1(train_cell_line, norm=norm)\n        test_cell_line, mean, std, feat_filt = normalize1(test_cell_line, mean, std, feat_filt=feat_filt, norm=norm)\n    \n    return train_cell_line,test_cell_line\n\n\n\n    \n     \n\nsmiles,data_to_repeat,unique_drugs,unique_cell,redkit_drug,ddi_model,graph,feature=get_data()\n\ndata_to_repeat=np.r_[data_to_repeat,data_to_repeat]\nl=int((data_to_repeat.shape[0])/2)\ndata_to_repeat[l:,0]=data_to_repeat[0:l,1]\ndata_to_repeat[l:,1]=data_to_repeat[0:l,0]\nsynergy=data_to_repeat[:,3]\n\nclass1=[]\nfor i in range(len(synergy)):\n if(synergy[i]>=30):\n    class1.append(1)\n elif(synergy[i]<0): \n    class1.append(0)\n else:\n    class1.append(2)    \n\n\nf_drug1,f_drug2,feature_cell,redkit_d1,redkit_d2,graph1,graph2=repeat_smiles1(data_to_repeat,unique_drugs,feature,unique_cell,redkit_drug,graph)\n\n\n\n\nindex_train,index_test=get_data_me2(test_fold)\n\nindex_train,index_test=preprocess(index_train,index_test)\ntrain_f_drug1,train_f_drug2,train_cell_line,test_f_drug1,test_f_drug2,test_cell_line,train_synergy,train_class,test_synergy,test_class,train_redkit_d1,train_redkit_d2,test_redkit_d1,test_redkit_d2,train_graph1,test_graph1,train_graph2,test_graph2=train_test_input(f_drug1,f_drug2,feature_cell,index_train,index_test,synergy,class1,redkit_d1,redkit_d2,graph1,graph2)\ntrain_cell_line=np.array(train_cell_line).astype(float)\ntest_cell_line=np.array(test_cell_line).astype(float)  \ntrain_f_drug1=np.array(train_f_drug1).astype(float)\ntest_f_drug1=np.array(test_f_drug1).astype(float)\ntrain_f_drug2=np.array(train_f_drug2).astype(float)\ntest_f_drug2=np.array(test_f_drug2).astype(float)\n\ntrain_graph1=np.array(train_graph1).astype(float)\ntest_graph1=np.array(test_graph1).astype(float) \ntrain_graph2=np.array(train_graph2).astype(float)\ntest_graph2=np.array(test_graph2).astype(float)\n\ntrain_cell_line,test_cell_line=norm1(train_cell_line,test_cell_line)\n\ntrain_f_drug1,test_f_drug1=norm1(train_f_drug1,test_f_drug1)\ntrain_f_drug2,test_f_drug2=norm1(train_f_drug2,test_f_drug2)\n\ntrain_graph1,test_graph1=norm1(train_graph1,test_graph1)\ntrain_graph2,test_graph2=norm1(train_graph2,test_graph2)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-03T16:07:08.480730Z","iopub.execute_input":"2023-09-03T16:07:08.481091Z","iopub.status.idle":"2023-09-03T16:09:16.836388Z","shell.execute_reply.started":"2023-09-03T16:07:08.481055Z","shell.execute_reply":"2023-09-03T16:09:16.834999Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (3.1.2)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl) (1.1.0)\nCollecting xlrd==2.0.1\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: xlrd\nSuccessfully installed xlrd-2.0.1\nCollecting Pandas==1.3.5\n  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from Pandas==1.3.5) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from Pandas==1.3.5) (2023.3)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from Pandas==1.3.5) (1.23.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.3->Pandas==1.3.5) (1.16.0)\nInstalling collected packages: Pandas\n  Attempting uninstall: Pandas\n    Found existing installation: pandas 1.5.3\n    Uninstalling pandas-1.5.3:\n      Successfully uninstalled pandas-1.5.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nbeatrix-jupyterlab 2023.621.222118 requires jupyter-server~=1.16, but you have jupyter-server 2.6.0 which is incompatible.\ncudf 23.6.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cuda 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nfeaturetools 1.26.0 requires pandas<2.0.0,>=1.5.0, but you have pandas 1.3.5 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.1 which is incompatible.\nraft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nwoodwork 0.24.0 requires pandas<2.0.0,>=1.4.3, but you have pandas 1.3.5 which is incompatible.\nxarray 2023.6.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\nydata-profiling 4.3.1 requires scipy<1.11,>=1.4.1, but you have scipy 1.11.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Pandas-1.3.5\nCollecting gdown\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.65.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.7.1\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 52.6MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=10ztxKtGSVU7p9yPoCsTnapVLThWpjjYT\nTo: /kaggle/working/oneil.xlsx\n100%|█████████████████████████████████████████| 850k/850k [00:00<00:00, 124MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 56.3MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1rVt2qEH-LMzk86ig6c8Qjll-08LhsxBv\nTo: /kaggle/working/cell_expression875.xlsx\n100%|█████████████████████████████████████████| 303k/303k [00:00<00:00, 121MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=14upx46iIPcO80y_tcvbaS3uwMkZfKLVw\nTo: /kaggle/working/redkit_drug.xlsx\n100%|███████████████████████████████████████| 68.6k/68.6k [00:00<00:00, 103MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1nkNG2Z38WQgM1Ap8Wrg5aVbjC-NNUO6S\nTo: /kaggle/working/views0123.xlsx\n100%|██████████████████████████████████████| 49.2k/49.2k [00:00<00:00, 86.5MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (uriginal): https://drive.google.com/uc?id=1HQpJQ6yeZYnGK6Uy0AyIvzYsNkzBAeOD\nFrom (redirected): https://drive.google.com/uc?id=1HQpJQ6yeZYnGK6Uy0AyIvzYsNkzBAeOD&confirm=t&uuid=766bdbd2-69a9-4a25-a567-7dba9606d927\nTo: /kaggle/working/model.h5\n100%|████████████████████████████████████████| 567M/567M [00:05<00:00, 97.3MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1AZX2h806qMcMp63hnJB81JFYMjzEChpA\nTo: /kaggle/working/unique394_drugl.xlsx\n100%|█████████████████████████████████████████| 162k/162k [00:00<00:00, 103MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1C7Z2ziPdQVzH3omIdIfyJa7VUmog4IIk\nTo: /kaggle/working/oneil.csv\n100%|█████████████████████████████████████████| 988k/988k [00:00<00:00, 127MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow\n!pip install spektral\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Minimum,Maximum,Add,Maximum,PReLU, Flatten,Reshape,Dropout, Input,Dense,Add,concatenate,BatchNormalization, Activation#,MultiHeadAttention,AdditiveAttention\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom spektral.layers import GATConv, GlobalAvgPool,GlobalMaxPool, GCNConv,GlobalAttentionPool\nfrom spektral.transforms import LayerPreprocess\nfrom sklearn.metrics import confusion_matrix, mean_squared_error,mean_absolute_error,r2_score#,AUC\nfrom scipy.stats import pearsonr,spearmanr\nfrom sklearn.metrics import roc_curve,auc,accuracy_score,precision_score,cohen_kappa_score,precision_recall_curve,average_precision_score,roc_auc_score\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras import regularizers\nnp.random.seed(10)\n\ntrain_cell_line=tf.convert_to_tensor(train_cell_line)\ntest_cell_line=tf.convert_to_tensor(test_cell_line)\ntrain_synergy=tf.convert_to_tensor(train_synergy)\ntrain_class=tf.convert_to_tensor(train_class)\ntest_synergy=tf.convert_to_tensor(test_synergy)\ntest_class=tf.convert_to_tensor(test_class)\ntest_redkit_d1=tf.convert_to_tensor(test_redkit_d1)\ntest_redkit_d2=tf.convert_to_tensor(test_redkit_d2)\ntrain_f_drug1=tf.convert_to_tensor(train_f_drug1)\ntest_f_drug1=tf.convert_to_tensor(test_f_drug1)\ntrain_f_drug2=tf.convert_to_tensor(train_f_drug2)\ntest_f_drug2=tf.convert_to_tensor(test_f_drug2)\ntrain_graph1=tf.convert_to_tensor(train_graph1)\ntest_graph1=tf.convert_to_tensor(test_graph1)\ntrain_graph2=tf.convert_to_tensor(train_graph2)\ntest_graph2=tf.convert_to_tensor(test_graph2)\n\n\n\ndef ddi_fun(train_redkit_d1,train_redkit_d2):\n    train_redkit_d1=np.array(train_redkit_d1).astype(float)\n    train_redkit_d2=np.array(train_redkit_d2).astype(float)\n    ddi_extractor = keras.Model(\n    inputs=ddi_model.inputs,\n    outputs=ddi_model.get_layer(name=\"features\").output,\n    )\n\n    train_ddi=ddi_extractor.predict([train_redkit_d1,train_redkit_d2])\n\n    return train_ddi\n\ntest_ddi=ddi_fun(test_redkit_d1,test_redkit_d2)\ntrain_ddi=ddi_fun(train_redkit_d1,train_redkit_d2)\ntrain_ddi=np.array(train_ddi).astype(float)\ntest_ddi=np.array(test_ddi).astype(float)\n\ntrain_ddi,test_ddi=norm1(train_ddi,test_ddi)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-03T16:09:16.839566Z","iopub.execute_input":"2023-09-03T16:09:16.840017Z","iopub.status.idle":"2023-09-03T16:09:59.237356Z","shell.execute_reply.started":"2023-09-03T16:09:16.839975Z","shell.execute_reply":"2023-09-03T16:09:59.236186Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.9.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.0)\nRequirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.31.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (0.2.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.11.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\nCollecting spektral\n  Downloading spektral-1.3.0-py3-none-any.whl (140 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from spektral) (1.2.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from spektral) (4.9.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from spektral) (3.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from spektral) (1.23.5)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from spektral) (1.3.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from spektral) (2.31.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from spektral) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from spektral) (1.11.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from spektral) (4.65.0)\nRequirement already satisfied: tensorflow>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from spektral) (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (3.9.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (16.0.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.31.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from pandas->spektral) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas->spektral) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (2023.5.7)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->spektral) (3.1.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow>=2.2.0->spektral) (0.2.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (3.4.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (2.3.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow>=2.2.0->spektral) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (3.2.2)\nInstalling collected packages: spektral\nSuccessfully installed spektral-1.3.0\n283/283 [==============================] - 3s 2ms/step\n1139/1139 [==============================] - 3s 2ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# import tensorflow_datasets as tfds\nimport tensorflow as tf\ndef scaled_dot_product_attention(q, k, v, mask=None):\n  \"\"\"Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead)\n  but it must be broadcastable for addition.\n\n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable\n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n  Returns:\n    output, attention_weights\n  \"\"\"\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)\n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits)#, axis=-1)#tf.nn.linear()#, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output,attention_weights \n\n#multi-head\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self,*, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n\n    assert d_model % self.num_heads == 0\n\n    self.depth = d_model // self.num_heads\n\n    self.wq = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wk = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wv = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n\n    self.dense = tf.keras.layers.Dense(d_model)#,activation='relu',use_bias='true')\n\n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n\n  def call(self, v, k, q, mask=None):\n    batch_size = tf.shape(q)[0]\n\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n#     q=PReLU()(q)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n#     k=PReLU()(k)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n#     v=PReLU()(v)\n\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention,\n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n    return output#, attention_weights\n\n\n\nclass CrossStitch(tf.keras.layers.Layer):\n\n    \"\"\"Cross-Stitch implementation according to arXiv:1604.03539\n    Implementation adapted from https://github.com/helloyide/Cross-stitch-Networks-for-Multi-task-Learning\"\"\"\n\n    def __init__(self, num_tasks, *args, **kwargs):\n        \"\"\"initialize class variables\"\"\"\n        self.num_tasks = num_tasks\n        super(CrossStitch, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        \"\"\"initialize the kernel and set the instance to 'built'\"\"\"\n        self.kernel = self.add_weight(name=\"kernel\",\n                                      shape=(self.num_tasks,\n                                             self.num_tasks),\n                                      initializer='identity',\n                                      trainable=True)\n        super(CrossStitch, self).build(input_shape)\n\n    def call(self, xl):\n        \"\"\"\n        called by TensorFlow when the model gets build. \n        Returns a stacked tensor with num_tasks channels in the 0 dimension, \n        which need to be unstacked.\n        \"\"\"\n        if (len(xl) != self.num_tasks):\n            # should not happen\n            raise ValueError()\n\n        out_values = []\n        for this_task in range(self.num_tasks):\n            this_weight = self.kernel[this_task, this_task]\n            out = tf.math.scalar_mul(this_weight, xl[this_task])\n            for other_task in range(self.num_tasks):\n                if this_task == other_task:\n                    continue  # already weighted!\n                other_weight = self.kernel[this_task, other_task]\n#                 out += tf.math.scalar_mul(other_weight, xl[other_task])\n            out_values.append(out)\n        # HACK!\n        # unless we stack, and then unstack the tensors, TF (2.0.0) can't follow\n        # the graph, so it aborts during model initialization.\n        # return tf.stack(out_values, axis=0)\n        return out_values[0],out_values[1]\n\n    def compute_output_shape(self, input_shape):\n        return [self.num_tasks] + input_shape\n\n    def get_config(self):\n        \"\"\"implemented so keras can save the model to json/yml\"\"\"\n        config = {\n            \"num_tasks\": self.num_tasks\n        }\n        base_config = super(CrossStitch, self).get_config()\n        return dict(list(config.items()) + list(base_config.items()))\n\n    \n \n\n\nimport keras.backend as K\nfrom keras.optimizers import Adam\n\nclass AdamW(Adam):\n    def __init__(self, learning_rate=0.001, weight_decay=0.025, **kwargs):\n        super(AdamW, self).__init__(learning_rate, **kwargs)\n        self.weight_decay = K.variable(weight_decay, name='weight_decay')\n\n    def get_updates(self, loss, params):\n        # Apply weight decay to the parameters\n        decay_updates = [\n            K.update_add(param, -self.weight_decay * param)\n            for param in params\n            if param.name.endswith('kernel:0')  # Apply weight decay only to kernel weights\n        ]\n\n        # Call the parent get_updates() method to get the remaining updates\n        updates = super(AdamW, self).get_updates(loss, params)\n\n        # Combine the weight decay updates and other updates\n        updates.extend(decay_updates)\n\n        return updates\n\n    \nimport keras.backend as K\nfrom keras.optimizers import Optimizer\n\nclass AMSGrad(Optimizer):\n    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, **kwargs):\n        super(AMSGrad, self).__init__(**kwargs)\n        self.learning_rate = learning_rate\n        self.beta_1 = beta_1\n        self.beta_2 = beta_2\n        self.epsilon = epsilon\n        self.iterations = K.variable(0)\n        self.m = None\n        self.v = None\n\ndef get_updates(self, loss, params):\n    grads = self.get_gradients(loss, params)\n    self.updates = [K.update_add(self.iterations, 1)]\n\n    t = self.iterations + 1\n    lr_t = self.learning_rate * (K.sqrt(1.0 - K.pow(self.beta_2, t)) / (1.0 - K.pow(self.beta_1, t)))\n\n    shapes = [K.int_shape(p) for p in params]\n    self.m = [K.zeros(shape) for shape in shapes]\n    self.v = [K.zeros(shape) for shape in shapes]\n\n    for p, g, m, v in zip(params, grads, self.m, self.v):\n        m_t = (self.beta_1 * m) + (1.0 - self.beta_1) * g\n        v_t = (self.beta_2 * v) + (1.0 - self.beta_2) * K.square(g)\n        v_hat = K.maximum(v, v_t)  # AMSGrad modification\n\n        p_t = p - lr_t * m_t / (K.sqrt(v_hat) + self.epsilon)\n\n        self.updates.append(K.update(m, m_t))\n        self.updates.append(K.update(v, v_t))\n        self.updates.append(K.update(p, p_t))\n\n    return self.updates\n","metadata":{"execution":{"iopub.status.busy":"2023-09-03T16:09:59.239480Z","iopub.execute_input":"2023-09-03T16:09:59.239897Z","iopub.status.idle":"2023-09-03T16:09:59.277766Z","shell.execute_reply.started":"2023-09-03T16:09:59.239859Z","shell.execute_reply":"2023-09-03T16:09:59.276847Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\ndef generate_network_att1(x_in1,x_in2,cell, inDrop, drop,ddi,graph1,graph2):\n    # fill the architecture params from dict\n\n\n    cell_layers = [512,265,128]\n\n    snp_layers = [512,128]\n\n    ddi_layers=[1024,512,256]\n    layers=[256,512,256]\n#     g_layers=[258,128]\n    g_layers=[327,128]\n    dsn1_layers = [1024,512,256]\n    dsn2_layers = [1024,512,256]\n   \n    l2_reg = 1e-3  # L2 regularization rate\n\n    \n\n    snp_layers = [2048,1024,2048]\n    # contruct two parallel networks\n    for l in range(len(dsn1_layers)):\n        if l == 0:\n            x_in1    = Input(shape=(x_in1.shape[1],),name=\"input1\")\n            middle_layer = Dense(int(dsn1_layers[l]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(x_in1)\n            middle_layer = Dropout(float(inDrop))(middle_layer) \n        elif l == (len(dsn1_layers)-1):\n            dsn1_output = Dense(int(dsn1_layers[l]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(middle_layer)\n        else:\n            middle_layer = Dense(int(dsn1_layers[l]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(middle_layer)\n            middle_layer = Dropout(float(drop))(middle_layer)\n\n    \n    concatModel1 =  dsn1_output\n\n    \n    for l in range(len(dsn2_layers)):\n        if l == 0:\n            x_in2    = Input(shape=(x_in2.shape[1],),name=\"input2\")\n            middle_layer = Dense(int(dsn2_layers[l]), activation='relu', use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(x_in2)\n            middle_layer = Dropout(float(inDrop))(middle_layer)\n        elif l == (len(dsn2_layers)-1):\n            dsn2_output = Dense(int(dsn2_layers[l]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(middle_layer)\n        else:\n            middle_layer = Dense(int(dsn2_layers[l]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(middle_layer)\n            middle_layer = Dropout(float(drop))(middle_layer)\n    \n    concatModel2 = dsn2_output\n    \n\n    for cell_layer in range(len(cell_layers)):\n      if cell_layer == 0:\n        input_cell    = Input(shape=(cell[0].shape[0],))  #\n\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(input_cell)\n        cellFC = Dropout(float(drop))(cellFC)\n\n      elif cell_layer == (len(cell_layers)-1):\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n\n      else:\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n        cellFC = Dropout(float(drop))(cellFC)\n#        \n\n\n    graph_in1=Input(shape=(graph1.shape[1],),name='graph1')\n\n    for g_layer in range(len(g_layers)):\n      if g_layer == 0:\n\n        graph_out1 = Dense(int(g_layers[g_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_in1)\n        graph_out1 = Dropout(float(drop))(graph_out1)\n#         \n        \n      elif g_layer == (len(g_layers)-1):\n        graph_out1 = Dense(int(g_layers[g_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_out1)\n#         \n      else:\n        graph_out1 = Dense(int(g_layers[g_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_out1)\n        graph_out1 = Dropout(float(drop))(graph_out1)\n\n    \n    \n    graph_in2=Input(shape=(graph2.shape[1],),name='graph2')\n\n    for g_layer in range(len(g_layers)):\n      if g_layer == 0:\n\n        graph_out2 = Dense(int(g_layers[g_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_in2)\n        graph_out2 = Dropout(float(drop))(graph_out2)\n        \n      elif g_layer == (len(g_layers)-1):\n        graph_out2 = Dense(int(g_layers[g_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_out2)\n\n\n      else:\n        graph_out2 = Dense(int(g_layers[g_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_out2)\n        graph_out2 = Dropout(float(drop))(graph_out2)\n    \n    concatModel1=concatenate([concatModel1,graph_out1])\n    concatModel2=concatenate([concatModel2,graph_out2])\n    \n    \n    for layer in range(len(layers)):\n      if layer == 0:\n\n        concatModel1 = Dense(int(layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel1)\n        concatModel1 = Dropout(float(drop))(concatModel1)\n\n        \n      elif layer == (len(layers)-1):\n        concatModel1 = Dense(int(layers[layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel1)\n\n      else:\n        concatModel1 = Dense(int(layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel1)\n        concatModel1 = Dropout(float(drop))(concatModel1)\n    \n    \n    for layer in range(len(layers)):\n      if layer == 0:\n\n        concatModel2 = Dense(int(layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel2)\n        concatModel2 = Dropout(float(drop))(concatModel2)\n        \n      elif layer == (len(layers)-1):\n        concatModel2 = Dense(int(layers[layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel2)\n\n      else:\n        concatModel2 = Dense(int(layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel2)\n        concatModel2 = Dropout(float(drop))(concatModel2)\n    \n    \n    ddi_out1=Input(shape=(ddi.shape[1],),name='ddi')\n\n    for ddi_layer in range(len(ddi_layers)):\n      if ddi_layer == 0:\n\n        ddi_out = Dense(int(ddi_layers[ddi_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(ddi_out1)\n        ddi_out = Dropout(float(drop))(ddi_out)\n\n        \n      elif ddi_layer == (len(ddi_layers)-1):\n        ddi_out = Dense(int(ddi_layers[ddi_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(ddi_out)\n\n      else:\n        ddi_out = Dense(int(ddi_layers[ddi_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(ddi_out)\n        ddi_out = Dropout(float(drop))(ddi_out)\n\n    \n    mlayer =MultiHeadAttention(d_model=cellFC.shape[1], num_heads=4)\n    cellFC1= mlayer(cellFC,ddi_out,ddi_out)\n    cellFC1 = Reshape([cellFC1.shape[2]])(cellFC1)\n    cellFC=concatenate([cellFC1,cellFC])\n\n    \n    concatModel=concatenate([concatModel2,concatModel1,cellFC])\n    concatModel=BatchNormalization()(concatModel)\n    \n        \n   \n#    \n    layer1 =MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task1= layer1(concatModel,concatModel,concatModel)\n    layer2 = MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task2= layer2(concatModel,concatModel,concatModel)\n    task11 = Reshape([a_task1.shape[2]])(a_task1)\n    task22 = Reshape([a_task2.shape[2]])(a_task2)\n    task1=concatenate([task11,concatModel])\n    task2=concatenate([task22,concatModel])\n\n    r_task1,r_task2 = CrossStitch(2)([task1,task2])\n\n    \n    r_task1=Dense(2048,activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    \n    r_task2=Dense(2048, activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n\n                             \n    r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n\n    \n    r_task1=concatenate([r_task1,task1])\n    r_task2=concatenate([r_task2,task2])\n\n    \n    \n    r_task1 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fsynergy1')(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(128, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fclass1')(r_task2)\n\n   \n    r_task1 = Dense(64, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fsynergy2')(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(64, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fclass2')(r_task2)\n#   \n\n    \n    snp_output1 = Dense(1, activation='linear',name='synergy')(r_task1)\n    snp_output2 = Dense(3, activation='sigmoid',name='class')(r_task2)\n    \n\n    model = Model(inputs=[x_in1,x_in2,input_cell,ddi_out1,graph_in1,graph_in2],outputs= [snp_output1,snp_output2])\n\n    print(model.summary())\n    return model\n\n\ndef trainer_att1(model, l_rate, train_f_drug1,train_f_drug2,train_cell_line,train_synergy,train_class,train_ddi,train_graph1,train_grpah2, epo, batch_size, earlyStop):\n\n    optimizer = AdamW(learning_rate=0.00001, weight_decay=0.025)\n\n    model.compile(optimizer=optimizer,loss={'synergy':'mse','class':'categorical_crossentropy'})\n\n    model.fit([train_f_drug1,train_f_drug2,train_cell_line,train_ddi,train_graph1,train_graph2],[train_synergy,train_class],shuffle=True, epochs=epo, batch_size=batch_size,verbose=1) \n                  \n\n    return model\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-09-03T16:09:59.280859Z","iopub.execute_input":"2023-09-03T16:09:59.281636Z","iopub.status.idle":"2023-09-03T16:09:59.337941Z","shell.execute_reply.started":"2023-09-03T16:09:59.281566Z","shell.execute_reply":"2023-09-03T16:09:59.336830Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#n\n\nl_rate = 0.00001\ninDrop = 0.2\ndrop = 0.2   \nmax_epoch =1000\nbatch_size = 64 \nearlyStop_patience = 20\n\n\n\nmodel_att1= generate_network_att1(train_f_drug1,train_f_drug2,train_cell_line, inDrop, drop,train_ddi,train_graph1,train_graph2)\n\ntrain_class1 = keras.utils.to_categorical(train_class, num_classes=3)\nmodel_att1=trainer_att1(model_att1, l_rate, train_f_drug1,train_f_drug2,train_cell_line,train_synergy,train_class1,train_ddi,train_graph1,train_graph2,max_epoch, batch_size,\n                                earlyStop_patience)\n\nap111,ap221= model_att1.predict( [test_f_drug1,test_f_drug2,test_cell_line,test_ddi,test_graph1,test_graph2])\n\npositive_negative_indices = np.where(test_class != 2)\nap221=ap221[positive_negative_indices]\ntest_class12=np.array(test_class)\ntest_class12=test_class12[positive_negative_indices]\nap221=ap221[:,1]\n\n\nap11=[]\ntest_synergy1=[]\nl=len(ap111)\nl1=int(l/2)\nfor i in range(l1) :\n    ap=(ap111[i]+ap111[i+l1])/2\n    ap11.append(ap)\n    aap=(test_synergy[i]+test_synergy[i+l1])/2\n    test_synergy1.append(aap)\n    \n    \nap22=[]\ntest_class1=[]\nl=len(ap221)\nl1=int(l/2)\nfor i in range(l1) :\n    ap=(ap221[i]+ap221[i+l1])/2\n    ap22.append(ap)\n    aap=(test_class12[i]+test_class12[i+l1])/2\n    test_class1.append(aap)\n    \n    \n    \nasynergy_error1=mean_squared_error(test_synergy1, ap11)\nasynergy_error11=mean_absolute_error(test_synergy1, ap11)\nasynergy_error21=r2_score(test_synergy1, ap11)\n\nprint(\"msynergy_mean_squared_error\",asynergy_error1)\nprint(\"mclass_mean_squared_error\",asynergy_error11)\nprint(\"msynergy_r2_score\",asynergy_error21)\n\nasynergy_pear1= pearsonr(test_synergy1, ap11)\nasynergy_spear1= spearmanr(test_synergy1, ap11)\nprint(\"msynergy_pear\",asynergy_pear1)\nprint(\"msynergy_spear\",asynergy_spear1)\n\naap2=convert_tobin(ap22)\n\naclass_error1=roc_auc_score(test_class1, ap22)\naclass_error11=accuracy_score(test_class1, aap2)\naclass_error21=cohen_kappa_score(test_class1, aap2)\n\nprint(\"msclass_roc_curve\",aclass_error1)\nprint(\"mclass_accuracy_scorer\",aclass_error11)\nprint(\"mclass_cohen_kappa_score\",aclass_error21)\n\n\naclass_pear1= precision_score(test_class1, aap2)\naclass_spear1= average_precision_score(test_class1, ap22)\nprint(\"mclass_precision_score\",aclass_pear1)\nprint(\"mclass_average_precision_score\",aclass_spear1)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-03T16:09:59.341527Z","iopub.execute_input":"2023-09-03T16:09:59.341868Z","iopub.status.idle":"2023-09-03T19:11:44.105604Z","shell.execute_reply.started":"2023-09-03T16:09:59.341842Z","shell.execute_reply":"2023-09-03T19:11:44.104541Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Model: \"model_2\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input2 (InputLayer)            [(None, 394)]        0           []                               \n                                                                                                  \n input1 (InputLayer)            [(None, 394)]        0           []                               \n                                                                                                  \n dense_3 (Dense)                (None, 1024)         404480      ['input2[0][0]']                 \n                                                                                                  \n dense (Dense)                  (None, 1024)         404480      ['input1[0][0]']                 \n                                                                                                  \n dropout_2 (Dropout)            (None, 1024)         0           ['dense_3[0][0]']                \n                                                                                                  \n graph2 (InputLayer)            [(None, 317)]        0           []                               \n                                                                                                  \n dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n                                                                                                  \n graph1 (InputLayer)            [(None, 317)]        0           []                               \n                                                                                                  \n dense_4 (Dense)                (None, 512)          524800      ['dropout_2[0][0]']              \n                                                                                                  \n dense_11 (Dense)               (None, 327)          103986      ['graph2[0][0]']                 \n                                                                                                  \n dense_1 (Dense)                (None, 512)          524800      ['dropout[0][0]']                \n                                                                                                  \n dense_9 (Dense)                (None, 327)          103986      ['graph1[0][0]']                 \n                                                                                                  \n input_1 (InputLayer)           [(None, 875)]        0           []                               \n                                                                                                  \n ddi (InputLayer)               [(None, 2048)]       0           []                               \n                                                                                                  \n dropout_3 (Dropout)            (None, 512)          0           ['dense_4[0][0]']                \n                                                                                                  \n dropout_7 (Dropout)            (None, 327)          0           ['dense_11[0][0]']               \n                                                                                                  \n dropout_1 (Dropout)            (None, 512)          0           ['dense_1[0][0]']                \n                                                                                                  \n dropout_6 (Dropout)            (None, 327)          0           ['dense_9[0][0]']                \n                                                                                                  \n dense_6 (Dense)                (None, 512)          448512      ['input_1[0][0]']                \n                                                                                                  \n dense_19 (Dense)               (None, 1024)         2098176     ['ddi[0][0]']                    \n                                                                                                  \n dense_5 (Dense)                (None, 256)          131328      ['dropout_3[0][0]']              \n                                                                                                  \n dense_12 (Dense)               (None, 128)          41984       ['dropout_7[0][0]']              \n                                                                                                  \n dense_2 (Dense)                (None, 256)          131328      ['dropout_1[0][0]']              \n                                                                                                  \n dense_10 (Dense)               (None, 128)          41984       ['dropout_6[0][0]']              \n                                                                                                  \n dropout_4 (Dropout)            (None, 512)          0           ['dense_6[0][0]']                \n                                                                                                  \n dropout_12 (Dropout)           (None, 1024)         0           ['dense_19[0][0]']               \n                                                                                                  \n concatenate_1 (Concatenate)    (None, 384)          0           ['dense_5[0][0]',                \n                                                                  'dense_12[0][0]']               \n                                                                                                  \n concatenate (Concatenate)      (None, 384)          0           ['dense_2[0][0]',                \n                                                                  'dense_10[0][0]']               \n                                                                                                  \n dense_7 (Dense)                (None, 265)          135945      ['dropout_4[0][0]']              \n                                                                                                  \n dense_20 (Dense)               (None, 512)          524800      ['dropout_12[0][0]']             \n                                                                                                  \n dense_16 (Dense)               (None, 256)          98560       ['concatenate_1[0][0]']          \n                                                                                                  \n dense_13 (Dense)               (None, 256)          98560       ['concatenate[0][0]']            \n                                                                                                  \n dropout_5 (Dropout)            (None, 265)          0           ['dense_7[0][0]']                \n                                                                                                  \n dropout_13 (Dropout)           (None, 512)          0           ['dense_20[0][0]']               \n                                                                                                  \n dropout_10 (Dropout)           (None, 256)          0           ['dense_16[0][0]']               \n                                                                                                  \n dropout_8 (Dropout)            (None, 256)          0           ['dense_13[0][0]']               \n                                                                                                  \n dense_8 (Dense)                (None, 128)          34048       ['dropout_5[0][0]']              \n                                                                                                  \n dense_21 (Dense)               (None, 256)          131328      ['dropout_13[0][0]']             \n                                                                                                  \n dense_17 (Dense)               (None, 512)          131584      ['dropout_10[0][0]']             \n                                                                                                  \n dense_14 (Dense)               (None, 512)          131584      ['dropout_8[0][0]']              \n                                                                                                  \n multi_head_attention (MultiHea  (None, None, 128)   98816       ['dense_8[0][0]',                \n dAttention)                                                      'dense_21[0][0]',               \n                                                                  'dense_21[0][0]']               \n                                                                                                  \n dropout_11 (Dropout)           (None, 512)          0           ['dense_17[0][0]']               \n                                                                                                  \n dropout_9 (Dropout)            (None, 512)          0           ['dense_14[0][0]']               \n                                                                                                  \n reshape (Reshape)              (None, 128)          0           ['multi_head_attention[0][0]']   \n                                                                                                  \n dense_18 (Dense)               (None, 256)          131328      ['dropout_11[0][0]']             \n                                                                                                  \n dense_15 (Dense)               (None, 256)          131328      ['dropout_9[0][0]']              \n                                                                                                  \n concatenate_2 (Concatenate)    (None, 256)          0           ['reshape[0][0]',                \n                                                                  'dense_8[0][0]']                \n                                                                                                  \n concatenate_3 (Concatenate)    (None, 768)          0           ['dense_18[0][0]',               \n                                                                  'dense_15[0][0]',               \n                                                                  'concatenate_2[0][0]']          \n                                                                                                  \n batch_normalization (BatchNorm  (None, 768)         3072        ['concatenate_3[0][0]']          \n alization)                                                                                       \n                                                                                                  \n multi_head_attention_1 (MultiH  (None, None, 768)   2362368     ['batch_normalization[0][0]',    \n eadAttention)                                                    'batch_normalization[0][0]',    \n                                                                  'batch_normalization[0][0]']    \n                                                                                                  \n multi_head_attention_2 (MultiH  (None, None, 768)   2362368     ['batch_normalization[0][0]',    \n eadAttention)                                                    'batch_normalization[0][0]',    \n                                                                  'batch_normalization[0][0]']    \n                                                                                                  \n reshape_1 (Reshape)            (None, 768)          0           ['multi_head_attention_1[0][0]'] \n                                                                                                  \n reshape_2 (Reshape)            (None, 768)          0           ['multi_head_attention_2[0][0]'] \n                                                                                                  \n concatenate_4 (Concatenate)    (None, 1536)         0           ['reshape_1[0][0]',              \n                                                                  'batch_normalization[0][0]']    \n                                                                                                  \n concatenate_5 (Concatenate)    (None, 1536)         0           ['reshape_2[0][0]',              \n                                                                  'batch_normalization[0][0]']    \n                                                                                                  \n cross_stitch (CrossStitch)     ((None, 1536),       4           ['concatenate_4[0][0]',          \n                                 (None, 1536))                    'concatenate_5[0][0]']          \n                                                                                                  \n dense_34 (Dense)               (None, 2048)         3147776     ['cross_stitch[0][0]']           \n                                                                                                  \n dense_35 (Dense)               (None, 2048)         3147776     ['cross_stitch[0][1]']           \n                                                                                                  \n cross_stitch_1 (CrossStitch)   ((None, 2048),       4           ['dense_34[0][0]',               \n                                 (None, 2048))                    'dense_35[0][0]']               \n                                                                                                  \n concatenate_6 (Concatenate)    (None, 3584)         0           ['cross_stitch_1[0][0]',         \n                                                                  'concatenate_4[0][0]']          \n                                                                                                  \n fsynergy1 (Dense)              (None, 128)          458880      ['concatenate_6[0][0]']          \n                                                                                                  \n p_re_lu (PReLU)                (None, 128)          128         ['fsynergy1[0][0]']              \n                                                                                                  \n concatenate_7 (Concatenate)    (None, 3584)         0           ['cross_stitch_1[0][1]',         \n                                                                  'concatenate_5[0][0]']          \n                                                                                                  \n fsynergy2 (Dense)              (None, 64)           8256        ['p_re_lu[0][0]']                \n                                                                                                  \n fclass1 (Dense)                (None, 128)          458880      ['concatenate_7[0][0]']          \n                                                                                                  \n p_re_lu_1 (PReLU)              (None, 64)           64          ['fsynergy2[0][0]']              \n                                                                                                  \n fclass2 (Dense)                (None, 64)           8256        ['fclass1[0][0]']                \n                                                                                                  \n synergy (Dense)                (None, 1)            65          ['p_re_lu_1[0][0]']              \n                                                                                                  \n class (Dense)                  (None, 3)            195         ['fclass2[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 18,565,817\nTrainable params: 18,564,281\nNon-trainable params: 1,536\n__________________________________________________________________________________________________\nNone\nEpoch 1/1000\n570/570 [==============================] - 17s 20ms/step - loss: 930.9172 - synergy_loss: 449.2471 - class_loss: 0.9371\nEpoch 2/1000\n570/570 [==============================] - 12s 21ms/step - loss: 851.6132 - synergy_loss: 398.3733 - class_loss: 0.8429\nEpoch 3/1000\n570/570 [==============================] - 11s 20ms/step - loss: 814.0858 - synergy_loss: 386.5547 - class_loss: 0.8176\nEpoch 4/1000\n570/570 [==============================] - 11s 20ms/step - loss: 786.5661 - synergy_loss: 382.2300 - class_loss: 0.8050\nEpoch 5/1000\n570/570 [==============================] - 12s 21ms/step - loss: 759.0169 - synergy_loss: 375.2603 - class_loss: 0.7934\nEpoch 6/1000\n570/570 [==============================] - 11s 20ms/step - loss: 737.0820 - synergy_loss: 371.2111 - class_loss: 0.7880\nEpoch 7/1000\n570/570 [==============================] - 12s 21ms/step - loss: 717.5902 - synergy_loss: 366.8564 - class_loss: 0.7827\nEpoch 8/1000\n570/570 [==============================] - 12s 20ms/step - loss: 699.0129 - synergy_loss: 360.8721 - class_loss: 0.7766\nEpoch 9/1000\n570/570 [==============================] - 12s 20ms/step - loss: 684.9963 - synergy_loss: 357.0236 - class_loss: 0.7727\nEpoch 10/1000\n570/570 [==============================] - 12s 21ms/step - loss: 672.9247 - synergy_loss: 352.8686 - class_loss: 0.7682\nEpoch 11/1000\n570/570 [==============================] - 11s 20ms/step - loss: 660.9066 - synergy_loss: 346.8839 - class_loss: 0.7667\nEpoch 12/1000\n570/570 [==============================] - 11s 20ms/step - loss: 651.6759 - synergy_loss: 342.0995 - class_loss: 0.7633\nEpoch 13/1000\n570/570 [==============================] - 12s 20ms/step - loss: 642.8094 - synergy_loss: 336.3196 - class_loss: 0.7573\nEpoch 14/1000\n570/570 [==============================] - 11s 19ms/step - loss: 634.3542 - synergy_loss: 329.8882 - class_loss: 0.7571\nEpoch 15/1000\n570/570 [==============================] - 11s 20ms/step - loss: 627.1821 - synergy_loss: 324.1163 - class_loss: 0.7505\nEpoch 16/1000\n570/570 [==============================] - 12s 21ms/step - loss: 623.8458 - synergy_loss: 321.6806 - class_loss: 0.7494\nEpoch 17/1000\n570/570 [==============================] - 11s 19ms/step - loss: 618.0725 - synergy_loss: 316.4815 - class_loss: 0.7438\nEpoch 18/1000\n570/570 [==============================] - 11s 20ms/step - loss: 615.0435 - synergy_loss: 313.9719 - class_loss: 0.7405\nEpoch 19/1000\n570/570 [==============================] - 12s 20ms/step - loss: 612.2523 - synergy_loss: 311.6044 - class_loss: 0.7378\nEpoch 20/1000\n570/570 [==============================] - 11s 20ms/step - loss: 605.6631 - synergy_loss: 305.4396 - class_loss: 0.7356\nEpoch 21/1000\n570/570 [==============================] - 12s 21ms/step - loss: 602.6006 - synergy_loss: 302.6790 - class_loss: 0.7321\nEpoch 22/1000\n570/570 [==============================] - 11s 20ms/step - loss: 600.6449 - synergy_loss: 301.0910 - class_loss: 0.7300\nEpoch 23/1000\n570/570 [==============================] - 11s 20ms/step - loss: 598.2821 - synergy_loss: 299.0667 - class_loss: 0.7275\nEpoch 24/1000\n570/570 [==============================] - 12s 20ms/step - loss: 592.9673 - synergy_loss: 294.0278 - class_loss: 0.7243\nEpoch 25/1000\n570/570 [==============================] - 11s 19ms/step - loss: 591.6343 - synergy_loss: 292.9433 - class_loss: 0.7235\nEpoch 26/1000\n570/570 [==============================] - 11s 19ms/step - loss: 586.6470 - synergy_loss: 288.2238 - class_loss: 0.7182\nEpoch 27/1000\n570/570 [==============================] - 11s 20ms/step - loss: 584.4194 - synergy_loss: 286.2391 - class_loss: 0.7160\nEpoch 28/1000\n570/570 [==============================] - 11s 19ms/step - loss: 580.3221 - synergy_loss: 282.3587 - class_loss: 0.7143\nEpoch 29/1000\n570/570 [==============================] - 11s 19ms/step - loss: 577.3342 - synergy_loss: 279.5645 - class_loss: 0.7125\nEpoch 30/1000\n570/570 [==============================] - 11s 20ms/step - loss: 576.8850 - synergy_loss: 279.3609 - class_loss: 0.7091\nEpoch 31/1000\n570/570 [==============================] - 11s 20ms/step - loss: 572.7162 - synergy_loss: 275.4241 - class_loss: 0.7052\nEpoch 32/1000\n570/570 [==============================] - 11s 19ms/step - loss: 569.9169 - synergy_loss: 272.8165 - class_loss: 0.7064\nEpoch 33/1000\n570/570 [==============================] - 11s 20ms/step - loss: 567.0328 - synergy_loss: 270.2117 - class_loss: 0.7026\nEpoch 34/1000\n570/570 [==============================] - 11s 19ms/step - loss: 564.0293 - synergy_loss: 267.3469 - class_loss: 0.7003\nEpoch 35/1000\n570/570 [==============================] - 11s 19ms/step - loss: 560.8818 - synergy_loss: 264.4354 - class_loss: 0.6951\nEpoch 36/1000\n570/570 [==============================] - 12s 20ms/step - loss: 560.5671 - synergy_loss: 264.2131 - class_loss: 0.6948\nEpoch 37/1000\n570/570 [==============================] - 11s 20ms/step - loss: 554.5290 - synergy_loss: 258.3787 - class_loss: 0.6901\nEpoch 38/1000\n570/570 [==============================] - 11s 20ms/step - loss: 555.1627 - synergy_loss: 259.2058 - class_loss: 0.6901\nEpoch 39/1000\n570/570 [==============================] - 11s 20ms/step - loss: 551.8703 - synergy_loss: 256.1209 - class_loss: 0.6855\nEpoch 40/1000\n570/570 [==============================] - 11s 19ms/step - loss: 549.1221 - synergy_loss: 253.5869 - class_loss: 0.6821\nEpoch 41/1000\n570/570 [==============================] - 12s 20ms/step - loss: 549.1997 - synergy_loss: 253.7752 - class_loss: 0.6800\nEpoch 42/1000\n570/570 [==============================] - 11s 19ms/step - loss: 546.3167 - synergy_loss: 251.0565 - class_loss: 0.6773\nEpoch 43/1000\n570/570 [==============================] - 11s 19ms/step - loss: 543.8699 - synergy_loss: 248.8104 - class_loss: 0.6739\nEpoch 44/1000\n570/570 [==============================] - 12s 20ms/step - loss: 541.5117 - synergy_loss: 246.6834 - class_loss: 0.6724\nEpoch 45/1000\n570/570 [==============================] - 11s 20ms/step - loss: 539.2052 - synergy_loss: 244.4874 - class_loss: 0.6700\nEpoch 46/1000\n570/570 [==============================] - 11s 19ms/step - loss: 535.4096 - synergy_loss: 240.8231 - class_loss: 0.6657\nEpoch 47/1000\n570/570 [==============================] - 12s 21ms/step - loss: 536.3923 - synergy_loss: 242.0766 - class_loss: 0.6654\nEpoch 48/1000\n570/570 [==============================] - 11s 20ms/step - loss: 538.3362 - synergy_loss: 244.2683 - class_loss: 0.6636\nEpoch 49/1000\n570/570 [==============================] - 11s 20ms/step - loss: 531.2142 - synergy_loss: 237.3261 - class_loss: 0.6599\nEpoch 50/1000\n570/570 [==============================] - 12s 21ms/step - loss: 531.3416 - synergy_loss: 237.5853 - class_loss: 0.6581\nEpoch 51/1000\n570/570 [==============================] - 11s 20ms/step - loss: 527.6496 - synergy_loss: 234.0946 - class_loss: 0.6544\nEpoch 52/1000\n570/570 [==============================] - 12s 20ms/step - loss: 529.8019 - synergy_loss: 236.4429 - class_loss: 0.6547\nEpoch 53/1000\n570/570 [==============================] - 12s 20ms/step - loss: 528.1387 - synergy_loss: 234.9161 - class_loss: 0.6504\nEpoch 54/1000\n570/570 [==============================] - 11s 20ms/step - loss: 525.4477 - synergy_loss: 232.4479 - class_loss: 0.6477\nEpoch 55/1000\n570/570 [==============================] - 12s 21ms/step - loss: 524.2939 - synergy_loss: 231.3694 - class_loss: 0.6458\nEpoch 56/1000\n570/570 [==============================] - 11s 20ms/step - loss: 518.4645 - synergy_loss: 225.8111 - class_loss: 0.6423\nEpoch 57/1000\n570/570 [==============================] - 11s 20ms/step - loss: 522.1038 - synergy_loss: 229.6179 - class_loss: 0.6460\nEpoch 58/1000\n570/570 [==============================] - 12s 21ms/step - loss: 518.1401 - synergy_loss: 225.8665 - class_loss: 0.6405\nEpoch 59/1000\n570/570 [==============================] - 11s 20ms/step - loss: 514.0505 - synergy_loss: 221.8854 - class_loss: 0.6401\nEpoch 60/1000\n570/570 [==============================] - 11s 20ms/step - loss: 512.6644 - synergy_loss: 220.6570 - class_loss: 0.6371\nEpoch 61/1000\n570/570 [==============================] - 12s 21ms/step - loss: 513.1171 - synergy_loss: 221.3244 - class_loss: 0.6355\nEpoch 62/1000\n570/570 [==============================] - 11s 19ms/step - loss: 514.6974 - synergy_loss: 223.0841 - class_loss: 0.6337\nEpoch 63/1000\n570/570 [==============================] - 11s 19ms/step - loss: 511.2771 - synergy_loss: 219.8330 - class_loss: 0.6279\nEpoch 64/1000\n570/570 [==============================] - 12s 20ms/step - loss: 511.2203 - synergy_loss: 219.9412 - class_loss: 0.6301\nEpoch 65/1000\n570/570 [==============================] - 11s 19ms/step - loss: 511.9129 - synergy_loss: 220.8720 - class_loss: 0.6276\nEpoch 66/1000\n570/570 [==============================] - 11s 20ms/step - loss: 508.6150 - synergy_loss: 217.6436 - class_loss: 0.6272\nEpoch 67/1000\n570/570 [==============================] - 11s 20ms/step - loss: 505.6882 - synergy_loss: 214.8957 - class_loss: 0.6225\nEpoch 68/1000\n570/570 [==============================] - 11s 19ms/step - loss: 502.6653 - synergy_loss: 211.9958 - class_loss: 0.6229\nEpoch 69/1000\n570/570 [==============================] - 12s 20ms/step - loss: 501.0312 - synergy_loss: 210.5564 - class_loss: 0.6183\nEpoch 70/1000\n570/570 [==============================] - 11s 19ms/step - loss: 499.9454 - synergy_loss: 209.6700 - class_loss: 0.6163\nEpoch 71/1000\n570/570 [==============================] - 11s 19ms/step - loss: 500.8687 - synergy_loss: 210.6964 - class_loss: 0.6149\nEpoch 72/1000\n570/570 [==============================] - 12s 21ms/step - loss: 501.6682 - synergy_loss: 211.7562 - class_loss: 0.6153\nEpoch 73/1000\n570/570 [==============================] - 11s 19ms/step - loss: 495.1757 - synergy_loss: 205.3456 - class_loss: 0.6114\nEpoch 74/1000\n570/570 [==============================] - 11s 19ms/step - loss: 495.7779 - synergy_loss: 206.1385 - class_loss: 0.6114\nEpoch 75/1000\n570/570 [==============================] - 12s 21ms/step - loss: 494.3857 - synergy_loss: 204.8721 - class_loss: 0.6077\nEpoch 76/1000\n570/570 [==============================] - 11s 20ms/step - loss: 494.9152 - synergy_loss: 205.5695 - class_loss: 0.6078\n570/570 [==============================] - 11s 19ms/step - loss: 480.4535 - synergy_loss: 192.5250 - class_loss: 0.5953\nEpoch 86/1000\n570/570 [==============================] - 11s 20ms/step - loss: 481.1755 - synergy_loss: 193.4066 - class_loss: 0.5932\nEpoch 87/1000\n570/570 [==============================] - 11s 19ms/step - loss: 476.7708 - synergy_loss: 189.1447 - class_loss: 0.5910\nEpoch 88/1000\n570/570 [==============================] - 11s 19ms/step - loss: 476.7858 - synergy_loss: 189.2913 - class_loss: 0.5893\nEpoch 89/1000\n570/570 [==============================] - 12s 20ms/step - loss: 475.7534 - synergy_loss: 188.3879 - class_loss: 0.5853\nEpoch 90/1000\n570/570 [==============================] - 11s 19ms/step - loss: 474.9067 - synergy_loss: 187.6750 - class_loss: 0.5920\nEpoch 91/1000\n570/570 [==============================] - 11s 19ms/step - loss: 472.3466 - synergy_loss: 185.2719 - class_loss: 0.5844\nEpoch 92/1000\n570/570 [==============================] - 12s 20ms/step - loss: 468.8256 - synergy_loss: 181.9203 - class_loss: 0.5818\nEpoch 93/1000\n570/570 [==============================] - 11s 19ms/step - loss: 469.4027 - synergy_loss: 182.6147 - class_loss: 0.5860\nEpoch 94/1000\n570/570 [==============================] - 11s 19ms/step - loss: 467.1615 - synergy_loss: 180.5031 - class_loss: 0.5813\nEpoch 95/1000\n570/570 [==============================] - 12s 20ms/step - loss: 465.3630 - synergy_loss: 178.8293 - class_loss: 0.5792\nEpoch 96/1000\n570/570 [==============================] - 11s 19ms/step - loss: 465.3224 - synergy_loss: 178.9146 - class_loss: 0.5803\nEpoch 97/1000\n570/570 [==============================] - 11s 19ms/step - loss: 459.4125 - synergy_loss: 173.0830 - class_loss: 0.5781\nEpoch 98/1000\n570/570 [==============================] - 11s 20ms/step - loss: 460.3870 - synergy_loss: 174.2200 - class_loss: 0.5724\nEpoch 99/1000\n570/570 [==============================] - 11s 19ms/step - loss: 456.5738 - synergy_loss: 170.5558 - class_loss: 0.5732\nEpoch 100/1000\n570/570 [==============================] - 11s 19ms/step - loss: 458.9225 - synergy_loss: 173.0249 - class_loss: 0.5718\nEpoch 101/1000\n 55/570 [=>............................] - ETA: 14s - loss: 466.8716 - synergy_loss: 180.8997 - class_loss: 0.5666","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"570/570 [==============================] - 11s 19ms/step - loss: 444.4756 - synergy_loss: 159.5120 - class_loss: 0.5660\nEpoch 108/1000\n570/570 [==============================] - 11s 19ms/step - loss: 444.0124 - synergy_loss: 159.1765 - class_loss: 0.5626\nEpoch 109/1000\n570/570 [==============================] - 11s 20ms/step - loss: 444.4140 - synergy_loss: 159.7099 - class_loss: 0.5629\nEpoch 110/1000\n570/570 [==============================] - 11s 19ms/step - loss: 443.3781 - synergy_loss: 158.8124 - class_loss: 0.5587\nEpoch 111/1000\n570/570 [==============================] - 11s 19ms/step - loss: 442.9231 - synergy_loss: 158.5319 - class_loss: 0.5617\nEpoch 112/1000\n570/570 [==============================] - 12s 20ms/step - loss: 439.7448 - synergy_loss: 155.4871 - class_loss: 0.5606\nEpoch 113/1000\n570/570 [==============================] - 11s 19ms/step - loss: 438.0224 - synergy_loss: 153.9136 - class_loss: 0.5587\nEpoch 114/1000\n570/570 [==============================] - 11s 19ms/step - loss: 435.1930 - synergy_loss: 151.2342 - class_loss: 0.5555\nEpoch 115/1000\n570/570 [==============================] - 11s 20ms/step - loss: 433.5835 - synergy_loss: 149.8193 - class_loss: 0.5574\nEpoch 116/1000\n570/570 [==============================] - 11s 19ms/step - loss: 431.6215 - synergy_loss: 147.9574 - class_loss: 0.5558\nEpoch 117/1000\n570/570 [==============================] - 11s 19ms/step - loss: 431.5723 - synergy_loss: 148.0716 - class_loss: 0.5508\nEpoch 118/1000\n570/570 [==============================] - 11s 20ms/step - loss: 429.5275 - synergy_loss: 146.1353 - class_loss: 0.5549\nEpoch 119/1000\n570/570 [==============================] - 11s 20ms/step - loss: 428.6893 - synergy_loss: 145.4318 - class_loss: 0.5511\nEpoch 120/1000\n570/570 [==============================] - 11s 19ms/step - loss: 426.0521 - synergy_loss: 143.0184 - class_loss: 0.5503\nEpoch 121/1000\n570/570 [==============================] - 11s 20ms/step - loss: 424.4993 - synergy_loss: 141.5890 - class_loss: 0.5495\nEpoch 122/1000\n570/570 [==============================] - 11s 19ms/step - loss: 424.1454 - synergy_loss: 141.3499 - class_loss: 0.5510\nEpoch 123/1000\n570/570 [==============================] - 11s 19ms/step - loss: 424.4883 - synergy_loss: 141.8077 - class_loss: 0.5467\nEpoch 124/1000\n570/570 [==============================] - 12s 20ms/step - loss: 420.5423 - synergy_loss: 138.0055 - class_loss: 0.5449\nEpoch 125/1000\n570/570 [==============================] - 11s 19ms/step - loss: 420.3853 - synergy_loss: 137.9831 - class_loss: 0.5422\nEpoch 126/1000\n570/570 [==============================] - 12s 20ms/step - loss: 418.9192 - synergy_loss: 136.6647 - class_loss: 0.5425\nEpoch 127/1000\n570/570 [==============================] - 11s 20ms/step - loss: 419.1518 - synergy_loss: 137.0761 - class_loss: 0.5432\nEpoch 128/1000\n570/570 [==============================] - 11s 19ms/step - loss: 417.1513 - synergy_loss: 135.2157 - class_loss: 0.5425\nEpoch 129/1000\n570/570 [==============================] - 11s 20ms/step - loss: 413.0469 - synergy_loss: 131.2898 - class_loss: 0.5385\nEpoch 130/1000\n570/570 [==============================] - 11s 19ms/step - loss: 416.6297 - synergy_loss: 135.0159 - class_loss: 0.5445\nEpoch 131/1000\n570/570 [==============================] - 11s 19ms/step - loss: 413.2257 - synergy_loss: 131.7669 - class_loss: 0.5379\nEpoch 132/1000\n570/570 [==============================] - 11s 20ms/step - loss: 411.8743 - synergy_loss: 130.5491 - class_loss: 0.5364\nEpoch 133/1000\n133/570 [======>.......................] - ETA: 8s - loss: 412.8028 - synergy_loss: 131.6038 - class_loss: 0.5344","output_type":"stream"},{"text":"IOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n","name":"stderr","output_type":"stream"},{"name":"stdout","text":"570/570 [==============================] - 12s 20ms/step - loss: 403.6844 - synergy_loss: 123.1691 - class_loss: 0.5299\nEpoch 139/1000\n570/570 [==============================] - 11s 19ms/step - loss: 404.1815 - synergy_loss: 123.7895 - class_loss: 0.5303\nEpoch 140/1000\n570/570 [==============================] - 11s 19ms/step - loss: 403.0649 - synergy_loss: 122.7994 - class_loss: 0.5272\nEpoch 141/1000\n570/570 [==============================] - 12s 20ms/step - loss: 403.5770 - synergy_loss: 123.4661 - class_loss: 0.5274\nEpoch 142/1000\n570/570 [==============================] - 11s 19ms/step - loss: 401.6380 - synergy_loss: 121.6596 - class_loss: 0.5300\nEpoch 143/1000\n570/570 [==============================] - 11s 19ms/step - loss: 401.5667 - synergy_loss: 121.7705 - class_loss: 0.5264\nEpoch 144/1000\n570/570 [==============================] - 12s 21ms/step - loss: 398.6132 - synergy_loss: 118.9709 - class_loss: 0.5267\nEpoch 145/1000\n570/570 [==============================] - 11s 19ms/step - loss: 397.6127 - synergy_loss: 118.1297 - class_loss: 0.5269\nEpoch 146/1000\n570/570 [==============================] - 11s 20ms/step - loss: 396.4827 - synergy_loss: 117.1452 - class_loss: 0.5245\nEpoch 147/1000\n570/570 [==============================] - 11s 19ms/step - loss: 395.5806 - synergy_loss: 116.4263 - class_loss: 0.5215\nEpoch 148/1000\n570/570 [==============================] - 11s 19ms/step - loss: 395.3785 - synergy_loss: 116.3432 - class_loss: 0.5198\nEpoch 149/1000\n570/570 [==============================] - 11s 20ms/step - loss: 394.0988 - synergy_loss: 115.1983 - class_loss: 0.5169\nEpoch 150/1000\n570/570 [==============================] - 11s 19ms/step - loss: 393.7729 - synergy_loss: 115.0357 - class_loss: 0.5210\nEpoch 151/1000\n570/570 [==============================] - 11s 19ms/step - loss: 393.0328 - synergy_loss: 114.4242 - class_loss: 0.5224\nEpoch 152/1000\n570/570 [==============================] - 11s 20ms/step - loss: 391.4704 - synergy_loss: 113.0140 - class_loss: 0.5180\nEpoch 153/1000\n570/570 [==============================] - 11s 19ms/step - loss: 390.4290 - synergy_loss: 112.1185 - class_loss: 0.5170\nEpoch 154/1000\n570/570 [==============================] - 11s 19ms/step - loss: 388.4713 - synergy_loss: 110.3564 - class_loss: 0.5152\nEpoch 155/1000\n570/570 [==============================] - 11s 20ms/step - loss: 388.4859 - synergy_loss: 110.5129 - class_loss: 0.5138\nEpoch 156/1000\n570/570 [==============================] - 11s 19ms/step - loss: 386.6818 - synergy_loss: 108.8680 - class_loss: 0.5127\nEpoch 157/1000\n570/570 [==============================] - 11s 19ms/step - loss: 386.8018 - synergy_loss: 109.1188 - class_loss: 0.5135\nEpoch 158/1000\n570/570 [==============================] - 12s 20ms/step - loss: 386.2982 - synergy_loss: 108.7700 - class_loss: 0.5122\nEpoch 159/1000\n570/570 [==============================] - 11s 20ms/step - loss: 384.7534 - synergy_loss: 107.3793 - class_loss: 0.5110\nEpoch 160/1000\n570/570 [==============================] - 11s 20ms/step - loss: 383.6830 - synergy_loss: 106.4587 - class_loss: 0.5106\nEpoch 161/1000\n570/570 [==============================] - 11s 20ms/step - loss: 382.4132 - synergy_loss: 105.3329 - class_loss: 0.5082\nEpoch 162/1000\n570/570 [==============================] - 11s 19ms/step - loss: 382.3096 - synergy_loss: 105.3937 - class_loss: 0.5080\nEpoch 163/1000\n570/570 [==============================] - 11s 19ms/step - loss: 381.2451 - synergy_loss: 104.4778 - class_loss: 0.5031\nEpoch 164/1000\n570/570 [==============================] - 11s 20ms/step - loss: 379.0488 - synergy_loss: 102.4299 - class_loss: 0.5020\nEpoch 165/1000\n570/570 [==============================] - 11s 19ms/step - loss: 379.7749 - synergy_loss: 103.3369 - class_loss: 0.5056\nEpoch 166/1000\n570/570 [==============================] - 11s 19ms/step - loss: 378.7531 - synergy_loss: 102.4687 - class_loss: 0.5035\nEpoch 167/1000\n570/570 [==============================] - 11s 20ms/step - loss: 377.1424 - synergy_loss: 101.0005 - class_loss: 0.5035\nEpoch 168/1000\n570/570 [==============================] - 11s 19ms/step - loss: 375.9293 - synergy_loss: 99.9289 - class_loss: 0.5034\nEpoch 169/1000\n570/570 [==============================] - 11s 19ms/step - loss: 376.2243 - synergy_loss: 100.3955 - class_loss: 0.5017\nEpoch 170/1000\n570/570 [==============================] - 12s 20ms/step - loss: 374.7868 - synergy_loss: 99.1348 - class_loss: 0.4989\nEpoch 171/1000\n570/570 [==============================] - 11s 19ms/step - loss: 373.6125 - synergy_loss: 98.0879 - class_loss: 0.4976\nEpoch 172/1000\n570/570 [==============================] - 12s 21ms/step - loss: 373.5844 - synergy_loss: 98.2224 - class_loss: 0.4979\nEpoch 173/1000\n570/570 [==============================] - 12s 20ms/step - loss: 372.3169 - synergy_loss: 97.1453 - class_loss: 0.4959\nEpoch 174/1000\n570/570 [==============================] - 12s 20ms/step - loss: 372.1565 - synergy_loss: 97.1035 - class_loss: 0.4969\nEpoch 175/1000\n570/570 [==============================] - 16s 29ms/step - loss: 371.2384 - synergy_loss: 96.3502 - class_loss: 0.4969\nEpoch 176/1000\n570/570 [==============================] - 13s 23ms/step - loss: 370.6420 - synergy_loss: 95.8926 - class_loss: 0.4945\nEpoch 177/1000\n570/570 [==============================] - 12s 21ms/step - loss: 368.9951 - synergy_loss: 94.4128 - class_loss: 0.4903\nEpoch 178/1000\n570/570 [==============================] - 11s 20ms/step - loss: 368.5955 - synergy_loss: 94.1739 - class_loss: 0.4893\nEpoch 179/1000\n570/570 [==============================] - 12s 20ms/step - loss: 369.1603 - synergy_loss: 94.8803 - class_loss: 0.4901\nEpoch 180/1000\n570/570 [==============================] - 12s 21ms/step - loss: 367.5693 - synergy_loss: 93.4389 - class_loss: 0.4905\nEpoch 181/1000\n570/570 [==============================] - 11s 20ms/step - loss: 369.1580 - synergy_loss: 95.1760 - class_loss: 0.4900\nEpoch 182/1000\n570/570 [==============================] - 11s 19ms/step - loss: 366.3077 - synergy_loss: 92.4747 - class_loss: 0.4917\nEpoch 183/1000\n570/570 [==============================] - 12s 20ms/step - loss: 365.1698 - synergy_loss: 91.5052 - class_loss: 0.4867\nEpoch 184/1000\n570/570 [==============================] - 11s 19ms/step - loss: 365.4705 - synergy_loss: 91.9524 - class_loss: 0.4843\nEpoch 185/1000\n570/570 [==============================] - 11s 19ms/step - loss: 363.8419 - synergy_loss: 90.4917 - class_loss: 0.4858\nEpoch 186/1000\n570/570 [==============================] - 12s 20ms/step - loss: 362.5153 - synergy_loss: 89.3166 - class_loss: 0.4847\nEpoch 187/1000\n570/570 [==============================] - 11s 19ms/step - loss: 362.3810 - synergy_loss: 89.3580 - class_loss: 0.4839\nEpoch 188/1000\n570/570 [==============================] - 11s 19ms/step - loss: 362.6302 - synergy_loss: 89.7406 - class_loss: 0.4829\nEpoch 189/1000\n570/570 [==============================] - 12s 20ms/step - loss: 361.1776 - synergy_loss: 88.4318 - class_loss: 0.4821\nEpoch 190/1000\n570/570 [==============================] - 11s 19ms/step - loss: 359.1762 - synergy_loss: 86.5965 - class_loss: 0.4804\nEpoch 191/1000\n570/570 [==============================] - 12s 20ms/step - loss: 359.2663 - synergy_loss: 86.8432 - class_loss: 0.4795\nEpoch 192/1000\n570/570 [==============================] - 11s 19ms/step - loss: 358.2095 - synergy_loss: 85.9371 - class_loss: 0.4783\nEpoch 193/1000\n570/570 [==============================] - 11s 19ms/step - loss: 357.0301 - synergy_loss: 84.9270 - class_loss: 0.4766\nEpoch 194/1000\n570/570 [==============================] - 11s 20ms/step - loss: 357.9274 - synergy_loss: 86.0129 - class_loss: 0.4784\nEpoch 195/1000\n570/570 [==============================] - 11s 19ms/step - loss: 355.2946 - synergy_loss: 83.5280 - class_loss: 0.4776\nEpoch 196/1000\n570/570 [==============================] - 11s 19ms/step - loss: 356.1938 - synergy_loss: 84.5968 - class_loss: 0.4749\nEpoch 197/1000\n570/570 [==============================] - 12s 20ms/step - loss: 355.7375 - synergy_loss: 84.2950 - class_loss: 0.4751\nEpoch 198/1000\n570/570 [==============================] - 11s 20ms/step - loss: 354.9110 - synergy_loss: 83.6323 - class_loss: 0.4730\nEpoch 199/1000\n570/570 [==============================] - 11s 19ms/step - loss: 353.3568 - synergy_loss: 82.2433 - class_loss: 0.4719\nEpoch 200/1000\n570/570 [==============================] - 11s 20ms/step - loss: 353.4947 - synergy_loss: 82.5222 - class_loss: 0.4732\nEpoch 201/1000\n570/570 [==============================] - 11s 19ms/step - loss: 352.7776 - synergy_loss: 81.9349 - class_loss: 0.4724\nEpoch 202/1000\n570/570 [==============================] - 11s 19ms/step - loss: 350.2186 - synergy_loss: 79.5452 - class_loss: 0.4693\nEpoch 203/1000\n570/570 [==============================] - 11s 20ms/step - loss: 349.4676 - synergy_loss: 78.9822 - class_loss: 0.4688\nEpoch 204/1000\n570/570 [==============================] - 11s 19ms/step - loss: 349.6818 - synergy_loss: 79.3307 - class_loss: 0.4681\nEpoch 205/1000\n570/570 [==============================] - 11s 19ms/step - loss: 348.5605 - synergy_loss: 78.3621 - class_loss: 0.4651\nEpoch 206/1000\n570/570 [==============================] - 12s 20ms/step - loss: 348.5728 - synergy_loss: 78.5625 - class_loss: 0.4685\nEpoch 207/1000\n570/570 [==============================] - 11s 19ms/step - loss: 348.0101 - synergy_loss: 78.1711 - class_loss: 0.4667\nEpoch 208/1000\n570/570 [==============================] - 11s 19ms/step - loss: 347.4609 - synergy_loss: 77.7848 - class_loss: 0.4636\nEpoch 209/1000\n570/570 [==============================] - 11s 20ms/step - loss: 345.6130 - synergy_loss: 76.0950 - class_loss: 0.4677\nEpoch 210/1000\n570/570 [==============================] - 11s 19ms/step - loss: 345.7117 - synergy_loss: 76.3821 - class_loss: 0.4656\nEpoch 211/1000\n570/570 [==============================] - 11s 19ms/step - loss: 344.6311 - synergy_loss: 75.4724 - class_loss: 0.4617\nEpoch 212/1000\n570/570 [==============================] - 11s 20ms/step - loss: 344.2616 - synergy_loss: 75.3058 - class_loss: 0.4593\nEpoch 213/1000\n570/570 [==============================] - 11s 19ms/step - loss: 342.7180 - synergy_loss: 73.9296 - class_loss: 0.4586\nEpoch 214/1000\n570/570 [==============================] - 12s 20ms/step - loss: 342.1530 - synergy_loss: 73.5426 - class_loss: 0.4585\nEpoch 215/1000\n570/570 [==============================] - 11s 19ms/step - loss: 343.0526 - synergy_loss: 74.6411 - class_loss: 0.4542\nEpoch 216/1000\n570/570 [==============================] - 11s 19ms/step - loss: 341.4330 - synergy_loss: 73.1915 - class_loss: 0.4585\nEpoch 217/1000\n570/570 [==============================] - 12s 20ms/step - loss: 340.6568 - synergy_loss: 72.5710 - class_loss: 0.4560\nEpoch 218/1000\n570/570 [==============================] - 11s 19ms/step - loss: 339.8018 - synergy_loss: 71.8604 - class_loss: 0.4567\nEpoch 219/1000\n570/570 [==============================] - 11s 19ms/step - loss: 339.9459 - synergy_loss: 72.1796 - class_loss: 0.4564\nEpoch 220/1000\n570/570 [==============================] - 12s 20ms/step - loss: 341.2288 - synergy_loss: 73.6387 - class_loss: 0.4572\nEpoch 221/1000\n570/570 [==============================] - 11s 19ms/step - loss: 339.4943 - synergy_loss: 72.0646 - class_loss: 0.4513\nEpoch 222/1000\n570/570 [==============================] - 11s 19ms/step - loss: 337.6522 - synergy_loss: 70.3698 - class_loss: 0.4533\nEpoch 223/1000\n570/570 [==============================] - 11s 20ms/step - loss: 337.1247 - synergy_loss: 70.0109 - class_loss: 0.4503\nEpoch 224/1000\n570/570 [==============================] - 11s 19ms/step - loss: 336.6279 - synergy_loss: 69.6864 - class_loss: 0.4515\nEpoch 225/1000\n570/570 [==============================] - 11s 19ms/step - loss: 336.9925 - synergy_loss: 70.2198 - class_loss: 0.4486\nEpoch 226/1000\n570/570 [==============================] - 12s 20ms/step - loss: 336.3260 - synergy_loss: 69.7355 - class_loss: 0.4496\nEpoch 227/1000\n570/570 [==============================] - 11s 19ms/step - loss: 335.4554 - synergy_loss: 69.0184 - class_loss: 0.4462\nEpoch 228/1000\n570/570 [==============================] - 11s 19ms/step - loss: 334.5931 - synergy_loss: 68.3733 - class_loss: 0.4460\nEpoch 229/1000\n570/570 [==============================] - 12s 20ms/step - loss: 332.9109 - synergy_loss: 66.8753 - class_loss: 0.4427\nEpoch 230/1000\n570/570 [==============================] - 11s 19ms/step - loss: 333.9649 - synergy_loss: 68.0938 - class_loss: 0.4468\nEpoch 231/1000\n570/570 [==============================] - 11s 19ms/step - loss: 332.9406 - synergy_loss: 67.2146 - class_loss: 0.4498\nEpoch 232/1000\n570/570 [==============================] - 11s 20ms/step - loss: 331.5490 - synergy_loss: 66.0036 - class_loss: 0.4480\nEpoch 233/1000\n570/570 [==============================] - 11s 19ms/step - loss: 332.1032 - synergy_loss: 66.7557 - class_loss: 0.4409\nEpoch 234/1000\n570/570 [==============================] - 11s 20ms/step - loss: 331.3955 - synergy_loss: 66.2126 - class_loss: 0.4435\nEpoch 235/1000\n570/570 [==============================] - 11s 19ms/step - loss: 330.6226 - synergy_loss: 65.6123 - class_loss: 0.4414\nEpoch 236/1000\n570/570 [==============================] - 11s 19ms/step - loss: 330.2916 - synergy_loss: 65.4482 - class_loss: 0.4404\nEpoch 237/1000\n570/570 [==============================] - 11s 20ms/step - loss: 329.7097 - synergy_loss: 65.0687 - class_loss: 0.4418\nEpoch 238/1000\n570/570 [==============================] - 11s 19ms/step - loss: 332.2248 - synergy_loss: 67.7864 - class_loss: 0.4364\nEpoch 239/1000\n570/570 [==============================] - 11s 19ms/step - loss: 326.7188 - synergy_loss: 62.4248 - class_loss: 0.4380\nEpoch 240/1000\n570/570 [==============================] - 12s 20ms/step - loss: 327.2186 - synergy_loss: 63.1049 - class_loss: 0.4344\nEpoch 241/1000\n570/570 [==============================] - 11s 19ms/step - loss: 327.7083 - synergy_loss: 63.7512 - class_loss: 0.4377\nEpoch 242/1000\n570/570 [==============================] - 11s 19ms/step - loss: 327.5181 - synergy_loss: 63.7275 - class_loss: 0.4362\nEpoch 243/1000\n570/570 [==============================] - 12s 20ms/step - loss: 326.7595 - synergy_loss: 63.1417 - class_loss: 0.4289\nEpoch 244/1000\n570/570 [==============================] - 11s 19ms/step - loss: 326.8726 - synergy_loss: 63.4176 - class_loss: 0.4297\nEpoch 245/1000\n570/570 [==============================] - 11s 19ms/step - loss: 324.8217 - synergy_loss: 61.5129 - class_loss: 0.4344\nEpoch 246/1000\n570/570 [==============================] - 12s 20ms/step - loss: 323.7652 - synergy_loss: 60.6313 - class_loss: 0.4291\nEpoch 247/1000\n570/570 [==============================] - 11s 19ms/step - loss: 323.9432 - synergy_loss: 60.9914 - class_loss: 0.4299\nEpoch 248/1000\n570/570 [==============================] - 11s 19ms/step - loss: 323.8743 - synergy_loss: 61.0966 - class_loss: 0.4273\nEpoch 249/1000\n570/570 [==============================] - 11s 20ms/step - loss: 322.8200 - synergy_loss: 60.2413 - class_loss: 0.4277\nEpoch 250/1000\n570/570 [==============================] - 11s 19ms/step - loss: 322.0242 - synergy_loss: 59.6283 - class_loss: 0.4270\nEpoch 251/1000\n570/570 [==============================] - 11s 19ms/step - loss: 321.9547 - synergy_loss: 59.7502 - class_loss: 0.4269\nEpoch 252/1000\n570/570 [==============================] - 12s 20ms/step - loss: 320.3202 - synergy_loss: 58.2867 - class_loss: 0.4260\nEpoch 253/1000\n570/570 [==============================] - 11s 19ms/step - loss: 320.3040 - synergy_loss: 58.4648 - class_loss: 0.4276\nEpoch 254/1000\n570/570 [==============================] - 11s 20ms/step - loss: 319.9828 - synergy_loss: 58.3230 - class_loss: 0.4237\nEpoch 255/1000\n570/570 [==============================] - 11s 20ms/step - loss: 320.4310 - synergy_loss: 58.9658 - class_loss: 0.4230\nEpoch 256/1000\n570/570 [==============================] - 11s 19ms/step - loss: 320.1978 - synergy_loss: 58.9158 - class_loss: 0.4253\nEpoch 257/1000\n570/570 [==============================] - 11s 20ms/step - loss: 318.8840 - synergy_loss: 57.7796 - class_loss: 0.4228\nEpoch 258/1000\n570/570 [==============================] - 11s 19ms/step - loss: 318.3213 - synergy_loss: 57.3922 - class_loss: 0.4195\nEpoch 259/1000\n570/570 [==============================] - 11s 19ms/step - loss: 318.9031 - synergy_loss: 58.1836 - class_loss: 0.4198\nEpoch 260/1000\n570/570 [==============================] - 11s 20ms/step - loss: 316.0775 - synergy_loss: 55.5491 - class_loss: 0.4193\nEpoch 261/1000\n570/570 [==============================] - 11s 19ms/step - loss: 317.8549 - synergy_loss: 57.4918 - class_loss: 0.4185\nEpoch 262/1000\n570/570 [==============================] - 11s 19ms/step - loss: 315.0449 - synergy_loss: 54.8710 - class_loss: 0.4173\nEpoch 263/1000\n570/570 [==============================] - 12s 20ms/step - loss: 316.5846 - synergy_loss: 56.6172 - class_loss: 0.4141\nEpoch 264/1000\n570/570 [==============================] - 11s 19ms/step - loss: 315.6422 - synergy_loss: 55.8636 - class_loss: 0.4155\nEpoch 265/1000\n570/570 [==============================] - 11s 19ms/step - loss: 315.7110 - synergy_loss: 56.1144 - class_loss: 0.4157\nEpoch 266/1000\n570/570 [==============================] - 11s 20ms/step - loss: 314.4258 - synergy_loss: 55.0096 - class_loss: 0.4111\nEpoch 267/1000\n570/570 [==============================] - 11s 19ms/step - loss: 314.2209 - synergy_loss: 54.9972 - class_loss: 0.4135\nEpoch 268/1000\n570/570 [==============================] - 11s 19ms/step - loss: 313.0277 - synergy_loss: 53.9752 - class_loss: 0.4119\nEpoch 269/1000\n570/570 [==============================] - 12s 20ms/step - loss: 312.8312 - synergy_loss: 53.9597 - class_loss: 0.4160\nEpoch 270/1000\n570/570 [==============================] - 11s 20ms/step - loss: 312.0652 - synergy_loss: 53.4068 - class_loss: 0.4093\nEpoch 271/1000\n570/570 [==============================] - 11s 19ms/step - loss: 311.7558 - synergy_loss: 53.2798 - class_loss: 0.4073\nEpoch 272/1000\n570/570 [==============================] - 11s 20ms/step - loss: 311.2977 - synergy_loss: 53.0183 - class_loss: 0.4072\nEpoch 273/1000\n570/570 [==============================] - 11s 19ms/step - loss: 309.8091 - synergy_loss: 51.7251 - class_loss: 0.4063\nEpoch 274/1000\n570/570 [==============================] - 11s 19ms/step - loss: 311.0399 - synergy_loss: 53.1406 - class_loss: 0.4065\nEpoch 275/1000\n570/570 [==============================] - 11s 20ms/step - loss: 310.7131 - synergy_loss: 53.0099 - class_loss: 0.4076\nEpoch 276/1000\n570/570 [==============================] - 11s 19ms/step - loss: 309.0189 - synergy_loss: 51.5165 - class_loss: 0.4050\nEpoch 277/1000\n570/570 [==============================] - 11s 20ms/step - loss: 308.8391 - synergy_loss: 51.5367 - class_loss: 0.4057\nEpoch 278/1000\n570/570 [==============================] - 11s 19ms/step - loss: 309.3878 - synergy_loss: 52.2690 - class_loss: 0.4049\nEpoch 279/1000\n570/570 [==============================] - 11s 19ms/step - loss: 307.4359 - synergy_loss: 50.5321 - class_loss: 0.4044\nEpoch 280/1000\n570/570 [==============================] - 11s 20ms/step - loss: 307.8828 - synergy_loss: 51.1440 - class_loss: 0.4016\nEpoch 281/1000\n570/570 [==============================] - 11s 19ms/step - loss: 307.8655 - synergy_loss: 51.3200 - class_loss: 0.4034\nEpoch 282/1000\n570/570 [==============================] - 11s 19ms/step - loss: 307.3928 - synergy_loss: 51.0478 - class_loss: 0.4043\nEpoch 283/1000\n570/570 [==============================] - 12s 20ms/step - loss: 306.0308 - synergy_loss: 49.8757 - class_loss: 0.4029\nEpoch 284/1000\n570/570 [==============================] - 11s 19ms/step - loss: 305.9150 - synergy_loss: 49.9264 - class_loss: 0.3975\nEpoch 285/1000\n570/570 [==============================] - 11s 19ms/step - loss: 306.3016 - synergy_loss: 50.5106 - class_loss: 0.3980\nEpoch 286/1000\n570/570 [==============================] - 11s 20ms/step - loss: 305.3005 - synergy_loss: 49.7309 - class_loss: 0.3963\nEpoch 287/1000\n570/570 [==============================] - 11s 19ms/step - loss: 303.9971 - synergy_loss: 48.6187 - class_loss: 0.3960\nEpoch 288/1000\n570/570 [==============================] - 11s 19ms/step - loss: 304.2059 - synergy_loss: 49.0322 - class_loss: 0.3939\nEpoch 289/1000\n570/570 [==============================] - 11s 20ms/step - loss: 303.9006 - synergy_loss: 48.9246 - class_loss: 0.3901\nEpoch 290/1000\n570/570 [==============================] - 11s 19ms/step - loss: 303.3402 - synergy_loss: 48.5459 - class_loss: 0.3965\nEpoch 291/1000\n570/570 [==============================] - 11s 19ms/step - loss: 304.2132 - synergy_loss: 49.6168 - class_loss: 0.3966\nEpoch 292/1000\n570/570 [==============================] - 12s 20ms/step - loss: 302.3453 - synergy_loss: 47.9361 - class_loss: 0.3906\nEpoch 293/1000\n570/570 [==============================] - 11s 19ms/step - loss: 302.1165 - synergy_loss: 47.8955 - class_loss: 0.3892\nEpoch 294/1000\n570/570 [==============================] - 11s 19ms/step - loss: 302.1152 - synergy_loss: 48.0689 - class_loss: 0.3910\nEpoch 295/1000\n570/570 [==============================] - 11s 20ms/step - loss: 300.2009 - synergy_loss: 46.3655 - class_loss: 0.3850\nEpoch 296/1000\n570/570 [==============================] - 11s 19ms/step - loss: 301.0778 - synergy_loss: 47.4259 - class_loss: 0.3910\nEpoch 297/1000\n570/570 [==============================] - 11s 19ms/step - loss: 300.0374 - synergy_loss: 46.5780 - class_loss: 0.3876\nEpoch 298/1000\n570/570 [==============================] - 12s 20ms/step - loss: 299.2477 - synergy_loss: 45.9966 - class_loss: 0.3901\nEpoch 299/1000\n570/570 [==============================] - 11s 19ms/step - loss: 299.3404 - synergy_loss: 46.2997 - class_loss: 0.3872\nEpoch 300/1000\n570/570 [==============================] - 11s 19ms/step - loss: 299.3571 - synergy_loss: 46.5295 - class_loss: 0.3830\nEpoch 301/1000\n570/570 [==============================] - 11s 19ms/step - loss: 299.0704 - synergy_loss: 46.4510 - class_loss: 0.3794\nEpoch 302/1000\n570/570 [==============================] - 11s 19ms/step - loss: 298.4018 - synergy_loss: 45.9988 - class_loss: 0.3842\nEpoch 303/1000\n570/570 [==============================] - 11s 20ms/step - loss: 298.7196 - synergy_loss: 46.4848 - class_loss: 0.3810\nEpoch 304/1000\n570/570 [==============================] - 11s 19ms/step - loss: 297.6332 - synergy_loss: 45.5802 - class_loss: 0.3826\nEpoch 305/1000\n570/570 [==============================] - 11s 19ms/step - loss: 297.8465 - synergy_loss: 46.0012 - class_loss: 0.3818\nEpoch 306/1000\n570/570 [==============================] - 11s 20ms/step - loss: 297.7271 - synergy_loss: 46.0770 - class_loss: 0.3792\nEpoch 307/1000\n570/570 [==============================] - 11s 19ms/step - loss: 295.8667 - synergy_loss: 44.4115 - class_loss: 0.3802\nEpoch 308/1000\n570/570 [==============================] - 11s 19ms/step - loss: 295.7076 - synergy_loss: 44.4691 - class_loss: 0.3781\nEpoch 309/1000\n570/570 [==============================] - 11s 20ms/step - loss: 294.8441 - synergy_loss: 43.8023 - class_loss: 0.3800\nEpoch 310/1000\n570/570 [==============================] - 11s 19ms/step - loss: 295.7574 - synergy_loss: 44.9185 - class_loss: 0.3777\nEpoch 311/1000\n570/570 [==============================] - 11s 19ms/step - loss: 294.2703 - synergy_loss: 43.6373 - class_loss: 0.3752\nEpoch 312/1000\n570/570 [==============================] - 11s 20ms/step - loss: 293.9890 - synergy_loss: 43.5699 - class_loss: 0.3737\nEpoch 313/1000\n570/570 [==============================] - 11s 19ms/step - loss: 294.7929 - synergy_loss: 44.5941 - class_loss: 0.3768\nEpoch 314/1000\n570/570 [==============================] - 11s 19ms/step - loss: 292.9274 - synergy_loss: 42.9502 - class_loss: 0.3727\nEpoch 315/1000\n570/570 [==============================] - 12s 20ms/step - loss: 293.2908 - synergy_loss: 43.5116 - class_loss: 0.3744\nEpoch 316/1000\n570/570 [==============================] - 11s 19ms/step - loss: 292.9546 - synergy_loss: 43.3856 - class_loss: 0.3715\nEpoch 317/1000\n570/570 [==============================] - 11s 19ms/step - loss: 292.2404 - synergy_loss: 42.9059 - class_loss: 0.3720\nEpoch 318/1000\n570/570 [==============================] - 11s 20ms/step - loss: 291.1658 - synergy_loss: 42.0460 - class_loss: 0.3712\nEpoch 319/1000\n570/570 [==============================] - 11s 19ms/step - loss: 290.7705 - synergy_loss: 41.8703 - class_loss: 0.3681\nEpoch 320/1000\n570/570 [==============================] - 11s 19ms/step - loss: 291.3771 - synergy_loss: 42.6901 - class_loss: 0.3688\nEpoch 321/1000\n570/570 [==============================] - 12s 20ms/step - loss: 291.1786 - synergy_loss: 42.6856 - class_loss: 0.3710\nEpoch 322/1000\n570/570 [==============================] - 11s 19ms/step - loss: 289.4041 - synergy_loss: 41.1148 - class_loss: 0.3674\nEpoch 323/1000\n570/570 [==============================] - 11s 19ms/step - loss: 289.7745 - synergy_loss: 41.7050 - class_loss: 0.3662\nEpoch 324/1000\n570/570 [==============================] - 11s 20ms/step - loss: 289.7418 - synergy_loss: 41.8715 - class_loss: 0.3681\nEpoch 325/1000\n570/570 [==============================] - 11s 19ms/step - loss: 289.7292 - synergy_loss: 42.0618 - class_loss: 0.3693\nEpoch 326/1000\n570/570 [==============================] - 12s 21ms/step - loss: 288.4734 - synergy_loss: 41.0293 - class_loss: 0.3634\nEpoch 327/1000\n570/570 [==============================] - 11s 19ms/step - loss: 288.8858 - synergy_loss: 41.6513 - class_loss: 0.3633\nEpoch 328/1000\n570/570 [==============================] - 11s 19ms/step - loss: 287.8308 - synergy_loss: 40.7970 - class_loss: 0.3614\nEpoch 329/1000\n570/570 [==============================] - 12s 20ms/step - loss: 287.4724 - synergy_loss: 40.6506 - class_loss: 0.3588\nEpoch 330/1000\n570/570 [==============================] - 11s 19ms/step - loss: 287.8159 - synergy_loss: 41.2060 - class_loss: 0.3615\nEpoch 331/1000\n570/570 [==============================] - 11s 19ms/step - loss: 286.6656 - synergy_loss: 40.2760 - class_loss: 0.3596\nEpoch 332/1000\n570/570 [==============================] - 11s 20ms/step - loss: 286.9091 - synergy_loss: 40.7321 - class_loss: 0.3559\nEpoch 333/1000\n570/570 [==============================] - 11s 19ms/step - loss: 286.1371 - synergy_loss: 40.1437 - class_loss: 0.3595\nEpoch 334/1000\n570/570 [==============================] - 11s 20ms/step - loss: 285.2525 - synergy_loss: 39.4878 - class_loss: 0.3552\nEpoch 335/1000\n570/570 [==============================] - 11s 20ms/step - loss: 285.3493 - synergy_loss: 39.8081 - class_loss: 0.3558\nEpoch 336/1000\n570/570 [==============================] - 11s 19ms/step - loss: 284.9112 - synergy_loss: 39.5979 - class_loss: 0.3544\nEpoch 337/1000\n570/570 [==============================] - 11s 20ms/step - loss: 284.0665 - synergy_loss: 38.9840 - class_loss: 0.3537\nEpoch 338/1000\n570/570 [==============================] - 11s 20ms/step - loss: 284.1079 - synergy_loss: 39.2418 - class_loss: 0.3546\nEpoch 339/1000\n570/570 [==============================] - 11s 19ms/step - loss: 283.3583 - synergy_loss: 38.7182 - class_loss: 0.3490\nEpoch 340/1000\n570/570 [==============================] - 11s 19ms/step - loss: 283.9944 - synergy_loss: 39.5670 - class_loss: 0.3536\nEpoch 341/1000\n570/570 [==============================] - 11s 20ms/step - loss: 283.2582 - synergy_loss: 39.0674 - class_loss: 0.3520\nEpoch 342/1000\n570/570 [==============================] - 11s 19ms/step - loss: 283.3086 - synergy_loss: 39.3186 - class_loss: 0.3461\nEpoch 343/1000\n570/570 [==============================] - 11s 19ms/step - loss: 281.8737 - synergy_loss: 38.0841 - class_loss: 0.3499\nEpoch 344/1000\n570/570 [==============================] - 11s 20ms/step - loss: 282.4118 - synergy_loss: 38.8341 - class_loss: 0.3498\nEpoch 345/1000\n570/570 [==============================] - 11s 20ms/step - loss: 281.9955 - synergy_loss: 38.6499 - class_loss: 0.3486\nEpoch 346/1000\n570/570 [==============================] - 12s 21ms/step - loss: 280.7730 - synergy_loss: 37.6390 - class_loss: 0.3441\nEpoch 347/1000\n570/570 [==============================] - 11s 20ms/step - loss: 281.4420 - synergy_loss: 38.5416 - class_loss: 0.3452\nEpoch 348/1000\n570/570 [==============================] - 11s 20ms/step - loss: 279.8840 - synergy_loss: 37.2126 - class_loss: 0.3449\nEpoch 349/1000\n570/570 [==============================] - 12s 21ms/step - loss: 280.3952 - synergy_loss: 37.9304 - class_loss: 0.3442\nEpoch 350/1000\n570/570 [==============================] - 11s 20ms/step - loss: 279.5692 - synergy_loss: 37.3057 - class_loss: 0.3437\nEpoch 351/1000\n570/570 [==============================] - 11s 20ms/step - loss: 278.7060 - synergy_loss: 36.6697 - class_loss: 0.3391\nEpoch 352/1000\n570/570 [==============================] - 12s 20ms/step - loss: 279.1515 - synergy_loss: 37.3519 - class_loss: 0.3405\nEpoch 353/1000\n570/570 [==============================] - 11s 20ms/step - loss: 279.6299 - synergy_loss: 38.0399 - class_loss: 0.3424\nEpoch 354/1000\n570/570 [==============================] - 11s 20ms/step - loss: 278.4988 - synergy_loss: 37.0949 - class_loss: 0.3398\nEpoch 355/1000\n570/570 [==============================] - 12s 20ms/step - loss: 278.0760 - synergy_loss: 36.9105 - class_loss: 0.3392\nEpoch 356/1000\n570/570 [==============================] - 11s 20ms/step - loss: 279.1876 - synergy_loss: 38.2078 - class_loss: 0.3370\nEpoch 357/1000\n570/570 [==============================] - 11s 20ms/step - loss: 277.8131 - synergy_loss: 37.0559 - class_loss: 0.3345\nEpoch 358/1000\n570/570 [==============================] - 12s 21ms/step - loss: 276.9569 - synergy_loss: 36.4253 - class_loss: 0.3342\nEpoch 359/1000\n570/570 [==============================] - 11s 20ms/step - loss: 276.9584 - synergy_loss: 36.6426 - class_loss: 0.3377\nEpoch 360/1000\n570/570 [==============================] - 12s 21ms/step - loss: 276.7267 - synergy_loss: 36.6243 - class_loss: 0.3379\nEpoch 361/1000\n570/570 [==============================] - 11s 20ms/step - loss: 276.3786 - synergy_loss: 36.4793 - class_loss: 0.3352\nEpoch 362/1000\n570/570 [==============================] - 11s 20ms/step - loss: 275.5966 - synergy_loss: 35.9355 - class_loss: 0.3318\nEpoch 363/1000\n570/570 [==============================] - 12s 21ms/step - loss: 275.3219 - synergy_loss: 35.8951 - class_loss: 0.3309\nEpoch 364/1000\n570/570 [==============================] - 12s 20ms/step - loss: 275.2336 - synergy_loss: 36.0154 - class_loss: 0.3324\nEpoch 365/1000\n570/570 [==============================] - 11s 20ms/step - loss: 274.7958 - synergy_loss: 35.7941 - class_loss: 0.3329\nEpoch 366/1000\n570/570 [==============================] - 12s 21ms/step - loss: 275.0310 - synergy_loss: 36.2536 - class_loss: 0.3332\nEpoch 367/1000\n570/570 [==============================] - 11s 19ms/step - loss: 273.9234 - synergy_loss: 35.3715 - class_loss: 0.3303\nEpoch 368/1000\n570/570 [==============================] - 11s 20ms/step - loss: 273.6635 - synergy_loss: 35.3127 - class_loss: 0.3276\nEpoch 369/1000\n570/570 [==============================] - 12s 21ms/step - loss: 272.8047 - synergy_loss: 34.6798 - class_loss: 0.3269\nEpoch 370/1000\n570/570 [==============================] - 11s 20ms/step - loss: 273.0523 - synergy_loss: 35.1606 - class_loss: 0.3287\nEpoch 371/1000\n570/570 [==============================] - 12s 21ms/step - loss: 272.7164 - synergy_loss: 35.0438 - class_loss: 0.3218\nEpoch 372/1000\n570/570 [==============================] - 11s 20ms/step - loss: 271.6304 - synergy_loss: 34.1802 - class_loss: 0.3224\nEpoch 373/1000\n570/570 [==============================] - 11s 19ms/step - loss: 271.1285 - synergy_loss: 33.9049 - class_loss: 0.3233\nEpoch 374/1000\n570/570 [==============================] - 12s 21ms/step - loss: 271.6930 - synergy_loss: 34.6939 - class_loss: 0.3232\nEpoch 375/1000\n570/570 [==============================] - 11s 20ms/step - loss: 271.0866 - synergy_loss: 34.3253 - class_loss: 0.3208\nEpoch 376/1000\n570/570 [==============================] - 11s 20ms/step - loss: 271.2137 - synergy_loss: 34.6820 - class_loss: 0.3246\nEpoch 377/1000\n570/570 [==============================] - 12s 21ms/step - loss: 271.2273 - synergy_loss: 34.9222 - class_loss: 0.3239\nEpoch 378/1000\n570/570 [==============================] - 11s 20ms/step - loss: 269.9050 - synergy_loss: 33.8205 - class_loss: 0.3194\nEpoch 379/1000\n570/570 [==============================] - 11s 20ms/step - loss: 269.8891 - synergy_loss: 34.0393 - class_loss: 0.3183\nEpoch 380/1000\n570/570 [==============================] - 12s 21ms/step - loss: 269.6562 - synergy_loss: 34.0139 - class_loss: 0.3200\nEpoch 381/1000\n570/570 [==============================] - 11s 20ms/step - loss: 269.9467 - synergy_loss: 34.5589 - class_loss: 0.3199\nEpoch 382/1000\n570/570 [==============================] - 12s 20ms/step - loss: 268.9146 - synergy_loss: 33.7261 - class_loss: 0.3104\nEpoch 383/1000\n570/570 [==============================] - 11s 20ms/step - loss: 268.3332 - synergy_loss: 33.3624 - class_loss: 0.3162\nEpoch 384/1000\n570/570 [==============================] - 11s 20ms/step - loss: 268.7839 - synergy_loss: 34.0650 - class_loss: 0.3139\nEpoch 385/1000\n570/570 [==============================] - 12s 21ms/step - loss: 267.8722 - synergy_loss: 33.3855 - class_loss: 0.3140\nEpoch 386/1000\n570/570 [==============================] - 11s 20ms/step - loss: 267.3067 - synergy_loss: 33.0425 - class_loss: 0.3127\nEpoch 387/1000\n570/570 [==============================] - 11s 20ms/step - loss: 266.9870 - synergy_loss: 32.9449 - class_loss: 0.3122\nEpoch 388/1000\n570/570 [==============================] - 12s 20ms/step - loss: 266.9890 - synergy_loss: 33.1717 - class_loss: 0.3106\nEpoch 389/1000\n570/570 [==============================] - 11s 19ms/step - loss: 266.8325 - synergy_loss: 33.2362 - class_loss: 0.3140\nEpoch 390/1000\n570/570 [==============================] - 11s 20ms/step - loss: 266.3845 - synergy_loss: 33.0150 - class_loss: 0.3093\nEpoch 391/1000\n570/570 [==============================] - 12s 21ms/step - loss: 265.7235 - synergy_loss: 32.5974 - class_loss: 0.3107\nEpoch 392/1000\n570/570 [==============================] - 11s 19ms/step - loss: 265.4136 - synergy_loss: 32.5269 - class_loss: 0.3080\nEpoch 393/1000\n570/570 [==============================] - 11s 19ms/step - loss: 265.3633 - synergy_loss: 32.7025 - class_loss: 0.3070\nEpoch 394/1000\n570/570 [==============================] - 12s 20ms/step - loss: 264.8694 - synergy_loss: 32.4399 - class_loss: 0.3045\nEpoch 395/1000\n570/570 [==============================] - 11s 19ms/step - loss: 264.7016 - synergy_loss: 32.4990 - class_loss: 0.3081\nEpoch 396/1000\n570/570 [==============================] - 11s 19ms/step - loss: 264.4015 - synergy_loss: 32.4413 - class_loss: 0.3057\nEpoch 397/1000\n570/570 [==============================] - 12s 20ms/step - loss: 263.4969 - synergy_loss: 31.7691 - class_loss: 0.3019\nEpoch 398/1000\n570/570 [==============================] - 11s 19ms/step - loss: 264.1095 - synergy_loss: 32.6081 - class_loss: 0.3041\nEpoch 399/1000\n570/570 [==============================] - 12s 20ms/step - loss: 263.4621 - synergy_loss: 32.1921 - class_loss: 0.3016\nEpoch 400/1000\n570/570 [==============================] - 11s 19ms/step - loss: 262.8981 - synergy_loss: 31.8380 - class_loss: 0.3016\nEpoch 401/1000\n570/570 [==============================] - 11s 19ms/step - loss: 261.9928 - synergy_loss: 31.1714 - class_loss: 0.3007\nEpoch 402/1000\n570/570 [==============================] - 11s 20ms/step - loss: 262.2529 - synergy_loss: 31.6726 - class_loss: 0.2981\nEpoch 403/1000\n570/570 [==============================] - 11s 19ms/step - loss: 261.8038 - synergy_loss: 31.4428 - class_loss: 0.3002\nEpoch 404/1000\n570/570 [==============================] - 11s 19ms/step - loss: 261.7682 - synergy_loss: 31.6282 - class_loss: 0.3006\nEpoch 405/1000\n570/570 [==============================] - 11s 20ms/step - loss: 261.0903 - synergy_loss: 31.1805 - class_loss: 0.2978\nEpoch 406/1000\n570/570 [==============================] - 11s 19ms/step - loss: 261.1878 - synergy_loss: 31.5145 - class_loss: 0.2973\nEpoch 407/1000\n570/570 [==============================] - 11s 19ms/step - loss: 260.9065 - synergy_loss: 31.4732 - class_loss: 0.3001\nEpoch 408/1000\n570/570 [==============================] - 12s 20ms/step - loss: 260.6285 - synergy_loss: 31.4305 - class_loss: 0.2935\nEpoch 409/1000\n570/570 [==============================] - 11s 19ms/step - loss: 259.6196 - synergy_loss: 30.6499 - class_loss: 0.2917\nEpoch 410/1000\n570/570 [==============================] - 11s 19ms/step - loss: 259.3884 - synergy_loss: 30.6410 - class_loss: 0.2925\nEpoch 411/1000\n570/570 [==============================] - 11s 20ms/step - loss: 259.4839 - synergy_loss: 30.9603 - class_loss: 0.2910\nEpoch 412/1000\n570/570 [==============================] - 11s 19ms/step - loss: 258.8981 - synergy_loss: 30.6186 - class_loss: 0.2923\nEpoch 413/1000\n570/570 [==============================] - 11s 19ms/step - loss: 258.9206 - synergy_loss: 30.9042 - class_loss: 0.2910\nEpoch 414/1000\n570/570 [==============================] - 12s 21ms/step - loss: 258.3877 - synergy_loss: 30.5957 - class_loss: 0.2865\nEpoch 415/1000\n570/570 [==============================] - 11s 19ms/step - loss: 258.1294 - synergy_loss: 30.5386 - class_loss: 0.2893\nEpoch 416/1000\n570/570 [==============================] - 11s 19ms/step - loss: 258.4612 - synergy_loss: 31.1058 - class_loss: 0.2887\nEpoch 417/1000\n570/570 [==============================] - 12s 20ms/step - loss: 257.1590 - synergy_loss: 30.0340 - class_loss: 0.2865\nEpoch 418/1000\n570/570 [==============================] - 11s 19ms/step - loss: 256.9174 - synergy_loss: 30.0190 - class_loss: 0.2884\nEpoch 419/1000\n570/570 [==============================] - 12s 20ms/step - loss: 256.9654 - synergy_loss: 30.2977 - class_loss: 0.2912\nEpoch 420/1000\n570/570 [==============================] - 11s 19ms/step - loss: 257.0321 - synergy_loss: 30.6042 - class_loss: 0.2867\nEpoch 421/1000\n570/570 [==============================] - 11s 19ms/step - loss: 256.4723 - synergy_loss: 30.2704 - class_loss: 0.2853\nEpoch 422/1000\n570/570 [==============================] - 12s 21ms/step - loss: 256.0137 - synergy_loss: 30.0577 - class_loss: 0.2852\nEpoch 423/1000\n570/570 [==============================] - 11s 19ms/step - loss: 255.7708 - synergy_loss: 30.0367 - class_loss: 0.2839\nEpoch 424/1000\n570/570 [==============================] - 11s 19ms/step - loss: 255.5164 - synergy_loss: 30.0171 - class_loss: 0.2805\nEpoch 425/1000\n570/570 [==============================] - 12s 20ms/step - loss: 255.1350 - synergy_loss: 29.8505 - class_loss: 0.2839\nEpoch 426/1000\n570/570 [==============================] - 11s 19ms/step - loss: 254.6085 - synergy_loss: 29.5538 - class_loss: 0.2812\nEpoch 427/1000\n570/570 [==============================] - 11s 19ms/step - loss: 254.1551 - synergy_loss: 29.3348 - class_loss: 0.2814\nEpoch 428/1000\n570/570 [==============================] - 11s 20ms/step - loss: 254.2004 - synergy_loss: 29.6209 - class_loss: 0.2787\nEpoch 429/1000\n570/570 [==============================] - 11s 19ms/step - loss: 254.8380 - synergy_loss: 30.4695 - class_loss: 0.2802\nEpoch 430/1000\n570/570 [==============================] - 11s 20ms/step - loss: 254.0258 - synergy_loss: 29.8964 - class_loss: 0.2752\nEpoch 431/1000\n570/570 [==============================] - 12s 20ms/step - loss: 253.5518 - synergy_loss: 29.6411 - class_loss: 0.2785\nEpoch 432/1000\n570/570 [==============================] - 11s 20ms/step - loss: 252.5698 - synergy_loss: 28.8971 - class_loss: 0.2743\nEpoch 433/1000\n570/570 [==============================] - 11s 19ms/step - loss: 252.6395 - synergy_loss: 29.1711 - class_loss: 0.2749\nEpoch 434/1000\n570/570 [==============================] - 12s 20ms/step - loss: 252.3436 - synergy_loss: 29.1208 - class_loss: 0.2731\nEpoch 435/1000\n570/570 [==============================] - 11s 19ms/step - loss: 251.3858 - synergy_loss: 28.3851 - class_loss: 0.2766\nEpoch 436/1000\n570/570 [==============================] - 12s 20ms/step - loss: 251.8132 - synergy_loss: 29.0311 - class_loss: 0.2757\nEpoch 437/1000\n570/570 [==============================] - 11s 20ms/step - loss: 250.7018 - synergy_loss: 28.1695 - class_loss: 0.2757\nEpoch 438/1000\n570/570 [==============================] - 11s 19ms/step - loss: 250.7305 - synergy_loss: 28.4366 - class_loss: 0.2693\nEpoch 439/1000\n570/570 [==============================] - 11s 20ms/step - loss: 250.5674 - synergy_loss: 28.5276 - class_loss: 0.2710\nEpoch 440/1000\n570/570 [==============================] - 11s 19ms/step - loss: 250.1476 - synergy_loss: 28.3498 - class_loss: 0.2719\nEpoch 441/1000\n570/570 [==============================] - 11s 19ms/step - loss: 249.7377 - synergy_loss: 28.1896 - class_loss: 0.2686\nEpoch 442/1000\n570/570 [==============================] - 11s 20ms/step - loss: 249.6511 - synergy_loss: 28.3434 - class_loss: 0.2688\nEpoch 443/1000\n570/570 [==============================] - 11s 19ms/step - loss: 249.4790 - synergy_loss: 28.4122 - class_loss: 0.2651\nEpoch 444/1000\n570/570 [==============================] - 11s 19ms/step - loss: 249.0036 - synergy_loss: 28.1776 - class_loss: 0.2686\nEpoch 445/1000\n570/570 [==============================] - 11s 20ms/step - loss: 249.1152 - synergy_loss: 28.5212 - class_loss: 0.2634\nEpoch 446/1000\n570/570 [==============================] - 11s 19ms/step - loss: 248.7373 - synergy_loss: 28.3585 - class_loss: 0.2699\nEpoch 447/1000\n570/570 [==============================] - 11s 19ms/step - loss: 248.4978 - synergy_loss: 28.3652 - class_loss: 0.2680\nEpoch 448/1000\n570/570 [==============================] - 11s 20ms/step - loss: 247.3938 - synergy_loss: 27.4840 - class_loss: 0.2630\nEpoch 449/1000\n570/570 [==============================] - 11s 19ms/step - loss: 247.5450 - synergy_loss: 27.8793 - class_loss: 0.2626\nEpoch 450/1000\n570/570 [==============================] - 11s 19ms/step - loss: 247.1344 - synergy_loss: 27.7072 - class_loss: 0.2640\nEpoch 451/1000\n570/570 [==============================] - 12s 20ms/step - loss: 247.2640 - synergy_loss: 28.0661 - class_loss: 0.2664\nEpoch 452/1000\n570/570 [==============================] - 11s 20ms/step - loss: 246.7233 - synergy_loss: 27.7495 - class_loss: 0.2624\nEpoch 453/1000\n570/570 [==============================] - 11s 19ms/step - loss: 246.2736 - synergy_loss: 27.5543 - class_loss: 0.2642\nEpoch 454/1000\n570/570 [==============================] - 11s 20ms/step - loss: 245.9711 - synergy_loss: 27.4918 - class_loss: 0.2607\nEpoch 455/1000\n570/570 [==============================] - 11s 19ms/step - loss: 245.7322 - synergy_loss: 27.4975 - class_loss: 0.2590\nEpoch 456/1000\n570/570 [==============================] - 11s 19ms/step - loss: 245.7022 - synergy_loss: 27.7212 - class_loss: 0.2596\nEpoch 457/1000\n570/570 [==============================] - 11s 20ms/step - loss: 245.2646 - synergy_loss: 27.5193 - class_loss: 0.2595\nEpoch 458/1000\n570/570 [==============================] - 11s 19ms/step - loss: 245.0590 - synergy_loss: 27.5584 - class_loss: 0.2571\nEpoch 459/1000\n570/570 [==============================] - 12s 21ms/step - loss: 244.4768 - synergy_loss: 27.2242 - class_loss: 0.2579\nEpoch 460/1000\n570/570 [==============================] - 11s 20ms/step - loss: 243.9648 - synergy_loss: 26.9509 - class_loss: 0.2552\nEpoch 461/1000\n570/570 [==============================] - 12s 21ms/step - loss: 243.7035 - synergy_loss: 26.9375 - class_loss: 0.2523\nEpoch 462/1000\n570/570 [==============================] - 12s 22ms/step - loss: 243.1370 - synergy_loss: 26.6095 - class_loss: 0.2565\nEpoch 463/1000\n570/570 [==============================] - 11s 19ms/step - loss: 244.2062 - synergy_loss: 27.9228 - class_loss: 0.2527\nEpoch 464/1000\n570/570 [==============================] - 12s 20ms/step - loss: 242.0968 - synergy_loss: 26.0573 - class_loss: 0.2510\nEpoch 465/1000\n570/570 [==============================] - 12s 21ms/step - loss: 242.3941 - synergy_loss: 26.5967 - class_loss: 0.2524\nEpoch 466/1000\n570/570 [==============================] - 11s 20ms/step - loss: 242.6262 - synergy_loss: 27.0937 - class_loss: 0.2542\nEpoch 467/1000\n570/570 [==============================] - 11s 20ms/step - loss: 242.3076 - synergy_loss: 27.0007 - class_loss: 0.2490\nEpoch 468/1000\n570/570 [==============================] - 12s 21ms/step - loss: 241.7058 - synergy_loss: 26.6204 - class_loss: 0.2504\nEpoch 469/1000\n570/570 [==============================] - 12s 20ms/step - loss: 240.9711 - synergy_loss: 26.1375 - class_loss: 0.2497\nEpoch 470/1000\n570/570 [==============================] - 12s 20ms/step - loss: 240.7220 - synergy_loss: 26.1441 - class_loss: 0.2482\nEpoch 471/1000\n570/570 [==============================] - 11s 19ms/step - loss: 242.1228 - synergy_loss: 27.7575 - class_loss: 0.2492\nEpoch 472/1000\n570/570 [==============================] - 11s 19ms/step - loss: 240.8479 - synergy_loss: 26.7221 - class_loss: 0.2489\nEpoch 473/1000\n570/570 [==============================] - 12s 20ms/step - loss: 240.4563 - synergy_loss: 26.5661 - class_loss: 0.2482\nEpoch 474/1000\n570/570 [==============================] - 11s 19ms/step - loss: 240.3239 - synergy_loss: 26.6744 - class_loss: 0.2478\nEpoch 475/1000\n570/570 [==============================] - 11s 19ms/step - loss: 239.6762 - synergy_loss: 26.2692 - class_loss: 0.2466\nEpoch 476/1000\n570/570 [==============================] - 12s 20ms/step - loss: 239.4229 - synergy_loss: 26.2480 - class_loss: 0.2427\nEpoch 477/1000\n570/570 [==============================] - 11s 18ms/step - loss: 238.7784 - synergy_loss: 25.8508 - class_loss: 0.2448\nEpoch 478/1000\n570/570 [==============================] - 11s 19ms/step - loss: 239.4416 - synergy_loss: 26.7263 - class_loss: 0.2422\nEpoch 479/1000\n570/570 [==============================] - 11s 20ms/step - loss: 238.0992 - synergy_loss: 25.6312 - class_loss: 0.2388\nEpoch 480/1000\n570/570 [==============================] - 11s 19ms/step - loss: 238.1886 - synergy_loss: 25.9512 - class_loss: 0.2413\nEpoch 481/1000\n570/570 [==============================] - 11s 19ms/step - loss: 237.8976 - synergy_loss: 25.9003 - class_loss: 0.2403\nEpoch 482/1000\n570/570 [==============================] - 12s 20ms/step - loss: 237.4529 - synergy_loss: 25.6942 - class_loss: 0.2408\nEpoch 483/1000\n570/570 [==============================] - 11s 19ms/step - loss: 236.8223 - synergy_loss: 25.3175 - class_loss: 0.2386\nEpoch 484/1000\n570/570 [==============================] - 11s 20ms/step - loss: 237.1307 - synergy_loss: 25.8749 - class_loss: 0.2422\nEpoch 485/1000\n570/570 [==============================] - 12s 20ms/step - loss: 236.7331 - synergy_loss: 25.7181 - class_loss: 0.2378\nEpoch 486/1000\n570/570 [==============================] - 11s 19ms/step - loss: 236.1957 - synergy_loss: 25.4322 - class_loss: 0.2400\nEpoch 487/1000\n570/570 [==============================] - 11s 19ms/step - loss: 235.6501 - synergy_loss: 25.1313 - class_loss: 0.2364\nEpoch 488/1000\n570/570 [==============================] - 12s 20ms/step - loss: 235.7085 - synergy_loss: 25.4444 - class_loss: 0.2327\nEpoch 489/1000\n570/570 [==============================] - 11s 20ms/step - loss: 235.7724 - synergy_loss: 25.7585 - class_loss: 0.2367\nEpoch 490/1000\n570/570 [==============================] - 12s 21ms/step - loss: 235.7936 - synergy_loss: 26.0064 - class_loss: 0.2371\nEpoch 491/1000\n570/570 [==============================] - 11s 19ms/step - loss: 234.7989 - synergy_loss: 25.2562 - class_loss: 0.2331\nEpoch 492/1000\n570/570 [==============================] - 11s 19ms/step - loss: 234.6592 - synergy_loss: 25.3591 - class_loss: 0.2338\nEpoch 493/1000\n570/570 [==============================] - 11s 20ms/step - loss: 234.1871 - synergy_loss: 25.1432 - class_loss: 0.2319\nEpoch 494/1000\n570/570 [==============================] - 11s 19ms/step - loss: 234.2937 - synergy_loss: 25.4805 - class_loss: 0.2354\nEpoch 495/1000\n570/570 [==============================] - 11s 19ms/step - loss: 233.5365 - synergy_loss: 24.9782 - class_loss: 0.2304\nEpoch 496/1000\n570/570 [==============================] - 11s 20ms/step - loss: 233.1711 - synergy_loss: 24.8563 - class_loss: 0.2309\nEpoch 497/1000\n570/570 [==============================] - 11s 19ms/step - loss: 233.0363 - synergy_loss: 24.9746 - class_loss: 0.2298\nEpoch 498/1000\n570/570 [==============================] - 11s 19ms/step - loss: 233.1736 - synergy_loss: 25.3474 - class_loss: 0.2324\nEpoch 499/1000\n570/570 [==============================] - 11s 20ms/step - loss: 232.5590 - synergy_loss: 24.9734 - class_loss: 0.2324\nEpoch 500/1000\n570/570 [==============================] - 11s 19ms/step - loss: 232.2173 - synergy_loss: 24.8780 - class_loss: 0.2297\nEpoch 501/1000\n570/570 [==============================] - 11s 19ms/step - loss: 232.1820 - synergy_loss: 25.0826 - class_loss: 0.2290\nEpoch 502/1000\n570/570 [==============================] - 11s 20ms/step - loss: 231.8650 - synergy_loss: 25.0090 - class_loss: 0.2324\nEpoch 503/1000\n570/570 [==============================] - 11s 19ms/step - loss: 231.3904 - synergy_loss: 24.7644 - class_loss: 0.2295\nEpoch 504/1000\n570/570 [==============================] - 11s 19ms/step - loss: 230.8034 - synergy_loss: 24.4281 - class_loss: 0.2255\nEpoch 505/1000\n570/570 [==============================] - 11s 20ms/step - loss: 230.5326 - synergy_loss: 24.3973 - class_loss: 0.2275\nEpoch 506/1000\n570/570 [==============================] - 11s 19ms/step - loss: 230.1490 - synergy_loss: 24.2690 - class_loss: 0.2223\nEpoch 507/1000\n570/570 [==============================] - 11s 19ms/step - loss: 230.2589 - synergy_loss: 24.6206 - class_loss: 0.2255\nEpoch 508/1000\n570/570 [==============================] - 11s 20ms/step - loss: 230.1135 - synergy_loss: 24.7131 - class_loss: 0.2257\nEpoch 509/1000\n570/570 [==============================] - 11s 19ms/step - loss: 229.7201 - synergy_loss: 24.5615 - class_loss: 0.2234\nEpoch 510/1000\n570/570 [==============================] - 11s 18ms/step - loss: 229.7219 - synergy_loss: 24.7921 - class_loss: 0.2254\nEpoch 511/1000\n570/570 [==============================] - 11s 20ms/step - loss: 228.8815 - synergy_loss: 24.2016 - class_loss: 0.2218\nEpoch 512/1000\n570/570 [==============================] - 11s 19ms/step - loss: 228.8064 - synergy_loss: 24.3605 - class_loss: 0.2248\nEpoch 513/1000\n570/570 [==============================] - 11s 19ms/step - loss: 228.2159 - synergy_loss: 24.0211 - class_loss: 0.2232\nEpoch 514/1000\n570/570 [==============================] - 11s 20ms/step - loss: 227.8758 - synergy_loss: 23.9312 - class_loss: 0.2152\nEpoch 515/1000\n570/570 [==============================] - 11s 19ms/step - loss: 227.7155 - synergy_loss: 24.0041 - class_loss: 0.2152\nEpoch 516/1000\n570/570 [==============================] - 10s 18ms/step - loss: 227.5965 - synergy_loss: 24.1265 - class_loss: 0.2198\nEpoch 517/1000\n570/570 [==============================] - 11s 19ms/step - loss: 227.1104 - synergy_loss: 23.8959 - class_loss: 0.2149\nEpoch 518/1000\n570/570 [==============================] - 11s 19ms/step - loss: 227.6303 - synergy_loss: 24.6507 - class_loss: 0.2196\nEpoch 519/1000\n570/570 [==============================] - 11s 19ms/step - loss: 226.6516 - synergy_loss: 23.9242 - class_loss: 0.2157\nEpoch 520/1000\n570/570 [==============================] - 11s 20ms/step - loss: 226.2433 - synergy_loss: 23.7610 - class_loss: 0.2179\nEpoch 521/1000\n570/570 [==============================] - 11s 19ms/step - loss: 226.1178 - synergy_loss: 23.8740 - class_loss: 0.2140\nEpoch 522/1000\n570/570 [==============================] - 11s 19ms/step - loss: 226.2149 - synergy_loss: 24.2280 - class_loss: 0.2136\nEpoch 523/1000\n570/570 [==============================] - 11s 19ms/step - loss: 225.4809 - synergy_loss: 23.7419 - class_loss: 0.2128\nEpoch 524/1000\n570/570 [==============================] - 11s 19ms/step - loss: 225.5222 - synergy_loss: 24.0385 - class_loss: 0.2125\nEpoch 525/1000\n570/570 [==============================] - 10s 18ms/step - loss: 225.1039 - synergy_loss: 23.8486 - class_loss: 0.2150\nEpoch 526/1000\n570/570 [==============================] - 11s 19ms/step - loss: 224.8588 - synergy_loss: 23.8469 - class_loss: 0.2121\nEpoch 527/1000\n570/570 [==============================] - 10s 18ms/step - loss: 224.3141 - synergy_loss: 23.5541 - class_loss: 0.2141\nEpoch 528/1000\n570/570 [==============================] - 10s 18ms/step - loss: 224.0209 - synergy_loss: 23.5082 - class_loss: 0.2098\nEpoch 529/1000\n570/570 [==============================] - 11s 19ms/step - loss: 223.9703 - synergy_loss: 23.6996 - class_loss: 0.2120\nEpoch 530/1000\n570/570 [==============================] - 11s 19ms/step - loss: 223.4212 - synergy_loss: 23.3997 - class_loss: 0.2127\nEpoch 531/1000\n570/570 [==============================] - 10s 18ms/step - loss: 223.4720 - synergy_loss: 23.6902 - class_loss: 0.2119\nEpoch 532/1000\n570/570 [==============================] - 11s 19ms/step - loss: 223.1763 - synergy_loss: 23.6443 - class_loss: 0.2092\nEpoch 533/1000\n570/570 [==============================] - 10s 18ms/step - loss: 222.0147 - synergy_loss: 22.7331 - class_loss: 0.2074\nEpoch 534/1000\n570/570 [==============================] - 11s 19ms/step - loss: 222.2348 - synergy_loss: 23.1964 - class_loss: 0.2126\nEpoch 535/1000\n570/570 [==============================] - 11s 19ms/step - loss: 221.9421 - synergy_loss: 23.1521 - class_loss: 0.2106\nEpoch 536/1000\n570/570 [==============================] - 10s 18ms/step - loss: 221.8930 - synergy_loss: 23.3584 - class_loss: 0.2091\nEpoch 537/1000\n570/570 [==============================] - 11s 19ms/step - loss: 221.6556 - synergy_loss: 23.3685 - class_loss: 0.2085\nEpoch 538/1000\n570/570 [==============================] - 11s 19ms/step - loss: 221.2130 - synergy_loss: 23.1780 - class_loss: 0.2039\nEpoch 539/1000\n570/570 [==============================] - 11s 20ms/step - loss: 221.2731 - synergy_loss: 23.4956 - class_loss: 0.2068\nEpoch 540/1000\n570/570 [==============================] - 12s 21ms/step - loss: 219.7806 - synergy_loss: 22.2567 - class_loss: 0.2062\nEpoch 541/1000\n570/570 [==============================] - 11s 20ms/step - loss: 220.2683 - synergy_loss: 22.9863 - class_loss: 0.2041\nEpoch 542/1000\n570/570 [==============================] - 10s 18ms/step - loss: 220.0711 - synergy_loss: 23.0252 - class_loss: 0.2060\nEpoch 543/1000\n570/570 [==============================] - 11s 19ms/step - loss: 220.2468 - synergy_loss: 23.4354 - class_loss: 0.2075\nEpoch 544/1000\n570/570 [==============================] - 11s 18ms/step - loss: 219.5587 - synergy_loss: 23.0024 - class_loss: 0.2025\nEpoch 545/1000\n570/570 [==============================] - 10s 18ms/step - loss: 219.3115 - synergy_loss: 23.0006 - class_loss: 0.2053\nEpoch 546/1000\n570/570 [==============================] - 11s 19ms/step - loss: 219.8628 - synergy_loss: 23.7809 - class_loss: 0.2042\nEpoch 547/1000\n570/570 [==============================] - 11s 18ms/step - loss: 218.5181 - synergy_loss: 22.6757 - class_loss: 0.2035\nEpoch 548/1000\n570/570 [==============================] - 10s 18ms/step - loss: 218.2467 - synergy_loss: 22.6562 - class_loss: 0.2034\nEpoch 549/1000\n570/570 [==============================] - 11s 19ms/step - loss: 218.7227 - synergy_loss: 23.3833 - class_loss: 0.2003\nEpoch 550/1000\n570/570 [==============================] - 11s 19ms/step - loss: 217.7621 - synergy_loss: 22.6572 - class_loss: 0.2007\nEpoch 551/1000\n570/570 [==============================] - 10s 18ms/step - loss: 217.4207 - synergy_loss: 22.5581 - class_loss: 0.2040\nEpoch 552/1000\n570/570 [==============================] - 11s 19ms/step - loss: 217.1708 - synergy_loss: 22.5557 - class_loss: 0.1995\nEpoch 553/1000\n570/570 [==============================] - 11s 19ms/step - loss: 216.8599 - synergy_loss: 22.4834 - class_loss: 0.2005\nEpoch 554/1000\n570/570 [==============================] - 11s 18ms/step - loss: 216.6337 - synergy_loss: 22.4988 - class_loss: 0.1974\nEpoch 555/1000\n570/570 [==============================] - 11s 19ms/step - loss: 216.3508 - synergy_loss: 22.4612 - class_loss: 0.1972\nEpoch 556/1000\n570/570 [==============================] - 10s 18ms/step - loss: 215.6695 - synergy_loss: 22.0453 - class_loss: 0.1927\nEpoch 557/1000\n570/570 [==============================] - 11s 18ms/step - loss: 215.5047 - synergy_loss: 22.1308 - class_loss: 0.1946\nEpoch 558/1000\n570/570 [==============================] - 11s 19ms/step - loss: 215.8062 - synergy_loss: 22.6771 - class_loss: 0.1972\nEpoch 559/1000\n570/570 [==============================] - 10s 18ms/step - loss: 215.7597 - synergy_loss: 22.8771 - class_loss: 0.1913\nEpoch 560/1000\n570/570 [==============================] - 10s 18ms/step - loss: 214.4003 - synergy_loss: 21.7627 - class_loss: 0.1920\nEpoch 561/1000\n570/570 [==============================] - 11s 19ms/step - loss: 214.5140 - synergy_loss: 22.1335 - class_loss: 0.1922\nEpoch 562/1000\n570/570 [==============================] - 10s 18ms/step - loss: 214.2865 - synergy_loss: 22.1480 - class_loss: 0.1907\nEpoch 563/1000\n570/570 [==============================] - 10s 18ms/step - loss: 214.0694 - synergy_loss: 22.1818 - class_loss: 0.1950\nEpoch 564/1000\n570/570 [==============================] - 11s 20ms/step - loss: 213.6114 - synergy_loss: 21.9667 - class_loss: 0.1942\nEpoch 565/1000\n570/570 [==============================] - 10s 18ms/step - loss: 213.1083 - synergy_loss: 21.7306 - class_loss: 0.1914\nEpoch 566/1000\n570/570 [==============================] - 10s 18ms/step - loss: 213.5314 - synergy_loss: 22.3858 - class_loss: 0.1945\nEpoch 567/1000\n570/570 [==============================] - 11s 19ms/step - loss: 212.6594 - synergy_loss: 21.7663 - class_loss: 0.1887\nEpoch 568/1000\n570/570 [==============================] - 11s 19ms/step - loss: 212.6250 - synergy_loss: 21.9791 - class_loss: 0.1928\nEpoch 569/1000\n570/570 [==============================] - 10s 18ms/step - loss: 212.5318 - synergy_loss: 22.1363 - class_loss: 0.1883\nEpoch 570/1000\n570/570 [==============================] - 11s 19ms/step - loss: 212.2011 - synergy_loss: 22.0574 - class_loss: 0.1878\nEpoch 571/1000\n570/570 [==============================] - 10s 18ms/step - loss: 211.9524 - synergy_loss: 22.0259 - class_loss: 0.1870\nEpoch 572/1000\n570/570 [==============================] - 10s 18ms/step - loss: 211.6919 - synergy_loss: 22.0112 - class_loss: 0.1866\nEpoch 573/1000\n570/570 [==============================] - 11s 19ms/step - loss: 211.8289 - synergy_loss: 22.3975 - class_loss: 0.1861\nEpoch 574/1000\n570/570 [==============================] - 10s 18ms/step - loss: 210.8631 - synergy_loss: 21.6660 - class_loss: 0.1831\nEpoch 575/1000\n570/570 [==============================] - 10s 18ms/step - loss: 211.1746 - synergy_loss: 22.2086 - class_loss: 0.1864\nEpoch 576/1000\n570/570 [==============================] - 11s 19ms/step - loss: 210.6051 - synergy_loss: 21.8736 - class_loss: 0.1888\nEpoch 577/1000\n570/570 [==============================] - 11s 19ms/step - loss: 210.4622 - synergy_loss: 21.9742 - class_loss: 0.1847\nEpoch 578/1000\n570/570 [==============================] - 11s 19ms/step - loss: 209.8228 - synergy_loss: 21.5812 - class_loss: 0.1862\nEpoch 579/1000\n570/570 [==============================] - 11s 19ms/step - loss: 209.7418 - synergy_loss: 21.7498 - class_loss: 0.1864\nEpoch 580/1000\n570/570 [==============================] - 10s 18ms/step - loss: 209.1933 - synergy_loss: 21.4343 - class_loss: 0.1827\nEpoch 581/1000\n570/570 [==============================] - 10s 18ms/step - loss: 209.0192 - synergy_loss: 21.5094 - class_loss: 0.1837\nEpoch 582/1000\n570/570 [==============================] - 11s 19ms/step - loss: 208.9586 - synergy_loss: 21.6895 - class_loss: 0.1859\nEpoch 583/1000\n570/570 [==============================] - 10s 18ms/step - loss: 208.8127 - synergy_loss: 21.7868 - class_loss: 0.1840\nEpoch 584/1000\n570/570 [==============================] - 11s 19ms/step - loss: 208.0779 - synergy_loss: 21.2959 - class_loss: 0.1836\nEpoch 585/1000\n570/570 [==============================] - 11s 19ms/step - loss: 208.4209 - synergy_loss: 21.8828 - class_loss: 0.1833\nEpoch 586/1000\n570/570 [==============================] - 10s 18ms/step - loss: 207.6792 - synergy_loss: 21.3851 - class_loss: 0.1832\nEpoch 587/1000\n570/570 [==============================] - 10s 18ms/step - loss: 207.3595 - synergy_loss: 21.3086 - class_loss: 0.1811\nEpoch 588/1000\n570/570 [==============================] - 11s 19ms/step - loss: 207.1770 - synergy_loss: 21.3833 - class_loss: 0.1769\nEpoch 589/1000\n570/570 [==============================] - 10s 18ms/step - loss: 206.7851 - synergy_loss: 21.2432 - class_loss: 0.1789\nEpoch 590/1000\n570/570 [==============================] - 11s 19ms/step - loss: 206.7789 - synergy_loss: 21.4732 - class_loss: 0.1827\nEpoch 591/1000\n570/570 [==============================] - 11s 19ms/step - loss: 205.9630 - synergy_loss: 20.8894 - class_loss: 0.1796\nEpoch 592/1000\n570/570 [==============================] - 11s 19ms/step - loss: 205.6343 - synergy_loss: 20.8231 - class_loss: 0.1790\nEpoch 593/1000\n570/570 [==============================] - 11s 18ms/step - loss: 205.6974 - synergy_loss: 21.1451 - class_loss: 0.1764\nEpoch 594/1000\n570/570 [==============================] - 11s 19ms/step - loss: 205.7604 - synergy_loss: 21.4417 - class_loss: 0.1778\nEpoch 595/1000\n570/570 [==============================] - 10s 18ms/step - loss: 205.4231 - synergy_loss: 21.3521 - class_loss: 0.1815\nEpoch 596/1000\n570/570 [==============================] - 10s 18ms/step - loss: 204.8212 - synergy_loss: 21.0037 - class_loss: 0.1774\nEpoch 597/1000\n570/570 [==============================] - 11s 19ms/step - loss: 204.8497 - synergy_loss: 21.2859 - class_loss: 0.1753\nEpoch 598/1000\n570/570 [==============================] - 10s 18ms/step - loss: 204.1957 - synergy_loss: 20.8755 - class_loss: 0.1757\nEpoch 599/1000\n570/570 [==============================] - 11s 18ms/step - loss: 203.6453 - synergy_loss: 20.5703 - class_loss: 0.1777\nEpoch 600/1000\n570/570 [==============================] - 11s 19ms/step - loss: 203.6264 - synergy_loss: 20.8097 - class_loss: 0.1721\nEpoch 601/1000\n570/570 [==============================] - 10s 18ms/step - loss: 203.7546 - synergy_loss: 21.1690 - class_loss: 0.1767\nEpoch 602/1000\n570/570 [==============================] - 10s 18ms/step - loss: 203.5506 - synergy_loss: 21.2090 - class_loss: 0.1747\nEpoch 603/1000\n570/570 [==============================] - 11s 19ms/step - loss: 203.1226 - synergy_loss: 21.0190 - class_loss: 0.1751\nEpoch 604/1000\n570/570 [==============================] - 10s 18ms/step - loss: 202.4158 - synergy_loss: 20.5654 - class_loss: 0.1742\nEpoch 605/1000\n570/570 [==============================] - 11s 19ms/step - loss: 202.2222 - synergy_loss: 20.6161 - class_loss: 0.1724\nEpoch 606/1000\n570/570 [==============================] - 11s 19ms/step - loss: 202.3807 - synergy_loss: 21.0092 - class_loss: 0.1749\nEpoch 607/1000\n570/570 [==============================] - 11s 19ms/step - loss: 201.7697 - synergy_loss: 20.6581 - class_loss: 0.1698\nEpoch 608/1000\n570/570 [==============================] - 10s 18ms/step - loss: 201.8179 - synergy_loss: 20.9481 - class_loss: 0.1743\nEpoch 609/1000\n570/570 [==============================] - 11s 19ms/step - loss: 201.0999 - synergy_loss: 20.4681 - class_loss: 0.1744\nEpoch 610/1000\n570/570 [==============================] - 10s 18ms/step - loss: 201.5449 - synergy_loss: 21.1620 - class_loss: 0.1719\nEpoch 611/1000\n570/570 [==============================] - 10s 18ms/step - loss: 201.1541 - synergy_loss: 21.0012 - class_loss: 0.1705\nEpoch 612/1000\n570/570 [==============================] - 11s 19ms/step - loss: 200.4339 - synergy_loss: 20.5095 - class_loss: 0.1738\nEpoch 613/1000\n570/570 [==============================] - 10s 18ms/step - loss: 199.9715 - synergy_loss: 20.3019 - class_loss: 0.1692\nEpoch 614/1000\n570/570 [==============================] - 10s 18ms/step - loss: 200.0936 - synergy_loss: 20.6629 - class_loss: 0.1693\nEpoch 615/1000\n570/570 [==============================] - 11s 19ms/step - loss: 199.5998 - synergy_loss: 20.4183 - class_loss: 0.1695\nEpoch 616/1000\n570/570 [==============================] - 10s 18ms/step - loss: 199.5685 - synergy_loss: 20.6213 - class_loss: 0.1660\nEpoch 617/1000\n570/570 [==============================] - 10s 18ms/step - loss: 199.3233 - synergy_loss: 20.6060 - class_loss: 0.1708\nEpoch 618/1000\n570/570 [==============================] - 11s 19ms/step - loss: 199.1146 - synergy_loss: 20.6509 - class_loss: 0.1718\nEpoch 619/1000\n570/570 [==============================] - 10s 18ms/step - loss: 198.5346 - synergy_loss: 20.3210 - class_loss: 0.1658\nEpoch 620/1000\n570/570 [==============================] - 10s 18ms/step - loss: 198.2468 - synergy_loss: 20.2782 - class_loss: 0.1636\nEpoch 621/1000\n570/570 [==============================] - 11s 19ms/step - loss: 198.5273 - synergy_loss: 20.7944 - class_loss: 0.1685\nEpoch 622/1000\n570/570 [==============================] - 10s 18ms/step - loss: 197.8224 - synergy_loss: 20.3257 - class_loss: 0.1652\nEpoch 623/1000\n570/570 [==============================] - 10s 18ms/step - loss: 197.5808 - synergy_loss: 20.3218 - class_loss: 0.1654\nEpoch 624/1000\n570/570 [==============================] - 11s 19ms/step - loss: 197.1475 - synergy_loss: 20.1272 - class_loss: 0.1647\nEpoch 625/1000\n570/570 [==============================] - 11s 19ms/step - loss: 196.7732 - synergy_loss: 20.0034 - class_loss: 0.1634\nEpoch 626/1000\n570/570 [==============================] - 10s 18ms/step - loss: 196.5641 - synergy_loss: 20.0425 - class_loss: 0.1641\nEpoch 627/1000\n570/570 [==============================] - 11s 19ms/step - loss: 196.7211 - synergy_loss: 20.4414 - class_loss: 0.1640\nEpoch 628/1000\n570/570 [==============================] - 10s 18ms/step - loss: 196.2879 - synergy_loss: 20.2509 - class_loss: 0.1637\nEpoch 629/1000\n570/570 [==============================] - 10s 18ms/step - loss: 195.9426 - synergy_loss: 20.1443 - class_loss: 0.1644\nEpoch 630/1000\n570/570 [==============================] - 11s 19ms/step - loss: 195.3440 - synergy_loss: 19.8160 - class_loss: 0.1624\nEpoch 631/1000\n570/570 [==============================] - 10s 18ms/step - loss: 195.3280 - synergy_loss: 20.0405 - class_loss: 0.1655\nEpoch 632/1000\n570/570 [==============================] - 10s 18ms/step - loss: 195.3037 - synergy_loss: 20.2565 - class_loss: 0.1634\nEpoch 633/1000\n570/570 [==============================] - 11s 19ms/step - loss: 194.7242 - synergy_loss: 19.9190 - class_loss: 0.1605\nEpoch 634/1000\n570/570 [==============================] - 10s 18ms/step - loss: 194.2380 - synergy_loss: 19.6995 - class_loss: 0.1631\nEpoch 635/1000\n570/570 [==============================] - 10s 18ms/step - loss: 194.8116 - synergy_loss: 20.5094 - class_loss: 0.1616\nEpoch 636/1000\n570/570 [==============================] - 11s 19ms/step - loss: 194.2327 - synergy_loss: 20.1699 - class_loss: 0.1594\nEpoch 637/1000\n570/570 [==============================] - 11s 18ms/step - loss: 193.8634 - synergy_loss: 20.0400 - class_loss: 0.1614\nEpoch 638/1000\n570/570 [==============================] - 10s 18ms/step - loss: 193.4348 - synergy_loss: 19.8540 - class_loss: 0.1617\nEpoch 639/1000\n570/570 [==============================] - 11s 19ms/step - loss: 193.0560 - synergy_loss: 19.7205 - class_loss: 0.1577\nEpoch 640/1000\n570/570 [==============================] - 10s 18ms/step - loss: 192.7606 - synergy_loss: 19.6774 - class_loss: 0.1573\nEpoch 641/1000\n570/570 [==============================] - 10s 18ms/step - loss: 192.6817 - synergy_loss: 19.8425 - class_loss: 0.1577\nEpoch 642/1000\n570/570 [==============================] - 11s 19ms/step - loss: 192.3756 - synergy_loss: 19.7783 - class_loss: 0.1568\nEpoch 643/1000\n570/570 [==============================] - 10s 18ms/step - loss: 192.1806 - synergy_loss: 19.8134 - class_loss: 0.1595\nEpoch 644/1000\n570/570 [==============================] - 10s 18ms/step - loss: 191.9053 - synergy_loss: 19.7764 - class_loss: 0.1574\nEpoch 645/1000\n570/570 [==============================] - 11s 19ms/step - loss: 191.5026 - synergy_loss: 19.6193 - class_loss: 0.1572\nEpoch 646/1000\n570/570 [==============================] - 11s 19ms/step - loss: 191.4381 - synergy_loss: 19.7945 - class_loss: 0.1582\nEpoch 647/1000\n570/570 [==============================] - 10s 18ms/step - loss: 191.1384 - synergy_loss: 19.7370 - class_loss: 0.1570\nEpoch 648/1000\n570/570 [==============================] - 11s 19ms/step - loss: 190.8550 - synergy_loss: 19.6911 - class_loss: 0.1517\nEpoch 649/1000\n570/570 [==============================] - 11s 18ms/step - loss: 190.7665 - synergy_loss: 19.8516 - class_loss: 0.1558\nEpoch 650/1000\n570/570 [==============================] - 10s 18ms/step - loss: 190.3846 - synergy_loss: 19.6930 - class_loss: 0.1547\nEpoch 651/1000\n570/570 [==============================] - 11s 19ms/step - loss: 189.8427 - synergy_loss: 19.3989 - class_loss: 0.1569\nEpoch 652/1000\n570/570 [==============================] - 10s 18ms/step - loss: 190.0148 - synergy_loss: 19.8103 - class_loss: 0.1567\nEpoch 653/1000\n570/570 [==============================] - 11s 19ms/step - loss: 189.7730 - synergy_loss: 19.8081 - class_loss: 0.1553\nEpoch 654/1000\n570/570 [==============================] - 11s 19ms/step - loss: 189.2106 - synergy_loss: 19.4856 - class_loss: 0.1553\nEpoch 655/1000\n570/570 [==============================] - 10s 18ms/step - loss: 188.9970 - synergy_loss: 19.5150 - class_loss: 0.1533\nEpoch 656/1000\n570/570 [==============================] - 10s 18ms/step - loss: 189.0086 - synergy_loss: 19.7717 - class_loss: 0.1558\nEpoch 657/1000\n570/570 [==============================] - 11s 19ms/step - loss: 188.4487 - synergy_loss: 19.4547 - class_loss: 0.1512\nEpoch 658/1000\n570/570 [==============================] - 10s 18ms/step - loss: 188.5576 - synergy_loss: 19.7984 - class_loss: 0.1553\nEpoch 659/1000\n570/570 [==============================] - 10s 18ms/step - loss: 187.9893 - synergy_loss: 19.4776 - class_loss: 0.1555\nEpoch 660/1000\n570/570 [==============================] - 11s 19ms/step - loss: 187.7111 - synergy_loss: 19.4290 - class_loss: 0.1527\nEpoch 661/1000\n570/570 [==============================] - 10s 18ms/step - loss: 187.5490 - synergy_loss: 19.5123 - class_loss: 0.1491\nEpoch 662/1000\n570/570 [==============================] - 10s 18ms/step - loss: 187.1905 - synergy_loss: 19.3946 - class_loss: 0.1529\nEpoch 663/1000\n570/570 [==============================] - 11s 19ms/step - loss: 186.7818 - synergy_loss: 19.2319 - class_loss: 0.1508\nEpoch 664/1000\n570/570 [==============================] - 10s 18ms/step - loss: 186.4646 - synergy_loss: 19.1604 - class_loss: 0.1532\nEpoch 665/1000\n570/570 [==============================] - 10s 18ms/step - loss: 185.7536 - synergy_loss: 18.6972 - class_loss: 0.1457\nEpoch 666/1000\n570/570 [==============================] - 11s 20ms/step - loss: 185.8699 - synergy_loss: 19.0695 - class_loss: 0.1537\nEpoch 667/1000\n570/570 [==============================] - 10s 18ms/step - loss: 185.9169 - synergy_loss: 19.3677 - class_loss: 0.1473\nEpoch 668/1000\n570/570 [==============================] - 10s 18ms/step - loss: 186.2036 - synergy_loss: 19.8746 - class_loss: 0.1476\nEpoch 669/1000\n570/570 [==============================] - 11s 19ms/step - loss: 185.6191 - synergy_loss: 19.5277 - class_loss: 0.1478\nEpoch 670/1000\n570/570 [==============================] - 10s 18ms/step - loss: 184.9047 - synergy_loss: 19.0511 - class_loss: 0.1492\nEpoch 671/1000\n570/570 [==============================] - 10s 18ms/step - loss: 185.4332 - synergy_loss: 19.8016 - class_loss: 0.1467\nEpoch 672/1000\n570/570 [==============================] - 11s 19ms/step - loss: 184.4032 - synergy_loss: 19.0148 - class_loss: 0.1462\nEpoch 673/1000\n570/570 [==============================] - 10s 18ms/step - loss: 184.3406 - synergy_loss: 19.1748 - class_loss: 0.1499\nEpoch 674/1000\n570/570 [==============================] - 11s 19ms/step - loss: 184.0827 - synergy_loss: 19.1687 - class_loss: 0.1463\nEpoch 675/1000\n570/570 [==============================] - 11s 19ms/step - loss: 184.0327 - synergy_loss: 19.3451 - class_loss: 0.1473\nEpoch 676/1000\n570/570 [==============================] - 10s 18ms/step - loss: 183.5581 - synergy_loss: 19.1091 - class_loss: 0.1453\nEpoch 677/1000\n570/570 [==============================] - 10s 18ms/step - loss: 183.1866 - synergy_loss: 18.9947 - class_loss: 0.1431\nEpoch 678/1000\n570/570 [==============================] - 11s 19ms/step - loss: 182.9065 - synergy_loss: 18.9565 - class_loss: 0.1478\nEpoch 679/1000\n570/570 [==============================] - 10s 18ms/step - loss: 182.5131 - synergy_loss: 18.8121 - class_loss: 0.1445\nEpoch 680/1000\n570/570 [==============================] - 10s 18ms/step - loss: 182.6275 - synergy_loss: 19.1568 - class_loss: 0.1394\nEpoch 681/1000\n570/570 [==============================] - 11s 19ms/step - loss: 181.7563 - synergy_loss: 18.5258 - class_loss: 0.1469\nEpoch 682/1000\n570/570 [==============================] - 11s 18ms/step - loss: 181.8139 - synergy_loss: 18.8286 - class_loss: 0.1454\nEpoch 683/1000\n570/570 [==============================] - 11s 18ms/step - loss: 181.8421 - synergy_loss: 19.1116 - class_loss: 0.1442\nEpoch 684/1000\n570/570 [==============================] - 11s 19ms/step - loss: 181.2992 - synergy_loss: 18.7988 - class_loss: 0.1455\nEpoch 685/1000\n570/570 [==============================] - 10s 18ms/step - loss: 180.7099 - synergy_loss: 18.4525 - class_loss: 0.1408\nEpoch 686/1000\n570/570 [==============================] - 10s 18ms/step - loss: 180.7220 - synergy_loss: 18.7130 - class_loss: 0.1428\nEpoch 687/1000\n570/570 [==============================] - 11s 19ms/step - loss: 180.8950 - synergy_loss: 19.1126 - class_loss: 0.1413\nEpoch 688/1000\n570/570 [==============================] - 10s 18ms/step - loss: 180.3996 - synergy_loss: 18.8539 - class_loss: 0.1437\nEpoch 689/1000\n570/570 [==============================] - 11s 18ms/step - loss: 179.8716 - synergy_loss: 18.5719 - class_loss: 0.1422\nEpoch 690/1000\n570/570 [==============================] - 11s 19ms/step - loss: 179.4919 - synergy_loss: 18.4321 - class_loss: 0.1407\nEpoch 691/1000\n570/570 [==============================] - 11s 19ms/step - loss: 179.4865 - synergy_loss: 18.6708 - class_loss: 0.1413\nEpoch 692/1000\n570/570 [==============================] - 11s 18ms/step - loss: 179.3872 - synergy_loss: 18.8104 - class_loss: 0.1408\nEpoch 693/1000\n570/570 [==============================] - 11s 19ms/step - loss: 179.5278 - synergy_loss: 19.1747 - class_loss: 0.1431\nEpoch 694/1000\n570/570 [==============================] - 10s 18ms/step - loss: 179.1387 - synergy_loss: 19.0047 - class_loss: 0.1439\nEpoch 695/1000\n570/570 [==============================] - 10s 18ms/step - loss: 178.3727 - synergy_loss: 18.4824 - class_loss: 0.1381\nEpoch 696/1000\n570/570 [==============================] - 11s 19ms/step - loss: 178.1947 - synergy_loss: 18.5373 - class_loss: 0.1400\nEpoch 697/1000\n570/570 [==============================] - 10s 18ms/step - loss: 178.3938 - synergy_loss: 18.9644 - class_loss: 0.1391\nEpoch 698/1000\n570/570 [==============================] - 11s 19ms/step - loss: 177.7690 - synergy_loss: 18.5687 - class_loss: 0.1368\nEpoch 699/1000\n570/570 [==============================] - 11s 19ms/step - loss: 177.5907 - synergy_loss: 18.6212 - class_loss: 0.1382\nEpoch 700/1000\n570/570 [==============================] - 10s 18ms/step - loss: 177.4173 - synergy_loss: 18.6824 - class_loss: 0.1392\nEpoch 701/1000\n570/570 [==============================] - 10s 18ms/step - loss: 177.3289 - synergy_loss: 18.8107 - class_loss: 0.1384\nEpoch 702/1000\n570/570 [==============================] - 11s 19ms/step - loss: 176.4245 - synergy_loss: 18.1441 - class_loss: 0.1365\nEpoch 703/1000\n570/570 [==============================] - 11s 19ms/step - loss: 176.8462 - synergy_loss: 18.7872 - class_loss: 0.1403\nEpoch 704/1000\n570/570 [==============================] - 10s 18ms/step - loss: 176.8086 - synergy_loss: 18.9656 - class_loss: 0.1361\nEpoch 705/1000\n570/570 [==============================] - 11s 19ms/step - loss: 176.7643 - synergy_loss: 19.1318 - class_loss: 0.1379\nEpoch 706/1000\n570/570 [==============================] - 10s 18ms/step - loss: 175.9130 - synergy_loss: 18.4947 - class_loss: 0.1412\nEpoch 707/1000\n570/570 [==============================] - 10s 18ms/step - loss: 175.7915 - synergy_loss: 18.5893 - class_loss: 0.1374\nEpoch 708/1000\n570/570 [==============================] - 11s 19ms/step - loss: 175.2246 - synergy_loss: 18.2466 - class_loss: 0.1363\nEpoch 709/1000\n570/570 [==============================] - 10s 18ms/step - loss: 175.3349 - synergy_loss: 18.5879 - class_loss: 0.1386\nEpoch 710/1000\n570/570 [==============================] - 10s 18ms/step - loss: 174.5336 - synergy_loss: 18.0291 - class_loss: 0.1357\nEpoch 711/1000\n570/570 [==============================] - 11s 19ms/step - loss: 174.3319 - synergy_loss: 18.0765 - class_loss: 0.1335\nEpoch 712/1000\n570/570 [==============================] - 10s 18ms/step - loss: 174.6985 - synergy_loss: 18.6536 - class_loss: 0.1354\nEpoch 713/1000\n570/570 [==============================] - 10s 18ms/step - loss: 174.2949 - synergy_loss: 18.4806 - class_loss: 0.1343\nEpoch 714/1000\n570/570 [==============================] - 11s 19ms/step - loss: 174.0237 - synergy_loss: 18.4335 - class_loss: 0.1323\nEpoch 715/1000\n570/570 [==============================] - 10s 18ms/step - loss: 173.4915 - synergy_loss: 18.1378 - class_loss: 0.1343\nEpoch 716/1000\n570/570 [==============================] - 10s 18ms/step - loss: 172.9096 - synergy_loss: 17.7964 - class_loss: 0.1353\nEpoch 717/1000\n570/570 [==============================] - 11s 19ms/step - loss: 173.4196 - synergy_loss: 18.5199 - class_loss: 0.1315\nEpoch 718/1000\n570/570 [==============================] - 10s 18ms/step - loss: 173.0341 - synergy_loss: 18.3535 - class_loss: 0.1329\nEpoch 719/1000\n570/570 [==============================] - 10s 18ms/step - loss: 172.2461 - synergy_loss: 17.8107 - class_loss: 0.1312\nEpoch 720/1000\n570/570 [==============================] - 11s 19ms/step - loss: 172.2039 - synergy_loss: 17.9873 - class_loss: 0.1327\nEpoch 721/1000\n570/570 [==============================] - 10s 18ms/step - loss: 172.7195 - synergy_loss: 18.7432 - class_loss: 0.1329\nEpoch 722/1000\n570/570 [==============================] - 10s 18ms/step - loss: 171.6224 - synergy_loss: 17.8604 - class_loss: 0.1325\nEpoch 723/1000\n570/570 [==============================] - 11s 19ms/step - loss: 172.8604 - synergy_loss: 19.3038 - class_loss: 0.1338\nEpoch 724/1000\n570/570 [==============================] - 11s 19ms/step - loss: 171.5306 - synergy_loss: 18.2008 - class_loss: 0.1298\nEpoch 725/1000\n570/570 [==============================] - 10s 18ms/step - loss: 171.1999 - synergy_loss: 18.0955 - class_loss: 0.1293\nEpoch 726/1000\n570/570 [==============================] - 11s 19ms/step - loss: 170.8729 - synergy_loss: 18.0004 - class_loss: 0.1286\nEpoch 727/1000\n570/570 [==============================] - 10s 18ms/step - loss: 170.2703 - synergy_loss: 17.6243 - class_loss: 0.1297\nEpoch 728/1000\n570/570 [==============================] - 10s 18ms/step - loss: 170.4189 - synergy_loss: 17.9906 - class_loss: 0.1282\nEpoch 729/1000\n570/570 [==============================] - 11s 19ms/step - loss: 170.5076 - synergy_loss: 18.3038 - class_loss: 0.1283\nEpoch 730/1000\n570/570 [==============================] - 11s 19ms/step - loss: 169.9543 - synergy_loss: 17.9615 - class_loss: 0.1321\nEpoch 731/1000\n570/570 [==============================] - 10s 18ms/step - loss: 169.5553 - synergy_loss: 17.7998 - class_loss: 0.1286\nEpoch 732/1000\n570/570 [==============================] - 11s 19ms/step - loss: 169.3121 - synergy_loss: 17.7897 - class_loss: 0.1302\nEpoch 733/1000\n570/570 [==============================] - 10s 18ms/step - loss: 169.0290 - synergy_loss: 17.7440 - class_loss: 0.1303\nEpoch 734/1000\n570/570 [==============================] - 10s 18ms/step - loss: 168.8942 - synergy_loss: 17.8416 - class_loss: 0.1291\nEpoch 735/1000\n570/570 [==============================] - 11s 19ms/step - loss: 168.6672 - synergy_loss: 17.8290 - class_loss: 0.1279\nEpoch 736/1000\n570/570 [==============================] - 10s 18ms/step - loss: 168.4888 - synergy_loss: 17.8833 - class_loss: 0.1291\nEpoch 737/1000\n570/570 [==============================] - 10s 18ms/step - loss: 168.0065 - synergy_loss: 17.6544 - class_loss: 0.1269\nEpoch 738/1000\n570/570 [==============================] - 11s 19ms/step - loss: 167.7735 - synergy_loss: 17.6482 - class_loss: 0.1297\nEpoch 739/1000\n570/570 [==============================] - 11s 19ms/step - loss: 167.6955 - synergy_loss: 17.7980 - class_loss: 0.1263\nEpoch 740/1000\n570/570 [==============================] - 10s 18ms/step - loss: 167.5108 - synergy_loss: 17.8429 - class_loss: 0.1293\nEpoch 741/1000\n570/570 [==============================] - 11s 19ms/step - loss: 167.2933 - synergy_loss: 17.8352 - class_loss: 0.1268\nEpoch 742/1000\n570/570 [==============================] - 10s 18ms/step - loss: 167.0715 - synergy_loss: 17.8438 - class_loss: 0.1310\nEpoch 743/1000\n570/570 [==============================] - 10s 18ms/step - loss: 166.8615 - synergy_loss: 17.8682 - class_loss: 0.1244\nEpoch 744/1000\n570/570 [==============================] - 11s 19ms/step - loss: 166.7606 - synergy_loss: 17.9839 - class_loss: 0.1251\nEpoch 745/1000\n570/570 [==============================] - 10s 18ms/step - loss: 166.0528 - synergy_loss: 17.4968 - class_loss: 0.1282\nEpoch 746/1000\n570/570 [==============================] - 10s 18ms/step - loss: 166.3304 - synergy_loss: 17.9806 - class_loss: 0.1268\nEpoch 747/1000\n570/570 [==============================] - 11s 20ms/step - loss: 165.4501 - synergy_loss: 17.3358 - class_loss: 0.1270\nEpoch 748/1000\n570/570 [==============================] - 10s 18ms/step - loss: 165.4366 - synergy_loss: 17.5625 - class_loss: 0.1226\nEpoch 749/1000\n570/570 [==============================] - 10s 18ms/step - loss: 165.1729 - synergy_loss: 17.5225 - class_loss: 0.1278\nEpoch 750/1000\n570/570 [==============================] - 11s 19ms/step - loss: 165.4435 - synergy_loss: 18.0213 - class_loss: 0.1257\nEpoch 751/1000\n570/570 [==============================] - 10s 18ms/step - loss: 164.5941 - synergy_loss: 17.4052 - class_loss: 0.1242\nEpoch 752/1000\n570/570 [==============================] - 11s 18ms/step - loss: 165.1738 - synergy_loss: 18.1873 - class_loss: 0.1259\nEpoch 753/1000\n570/570 [==============================] - 11s 19ms/step - loss: 164.1783 - synergy_loss: 17.4212 - class_loss: 0.1271\nEpoch 754/1000\n570/570 [==============================] - 11s 19ms/step - loss: 164.5468 - synergy_loss: 17.9835 - class_loss: 0.1203\nEpoch 755/1000\n570/570 [==============================] - 10s 18ms/step - loss: 163.5710 - synergy_loss: 17.2382 - class_loss: 0.1270\nEpoch 756/1000\n570/570 [==============================] - 11s 19ms/step - loss: 163.5659 - synergy_loss: 17.4473 - class_loss: 0.1246\nEpoch 757/1000\n570/570 [==============================] - 10s 18ms/step - loss: 163.1426 - synergy_loss: 17.2680 - class_loss: 0.1217\nEpoch 758/1000\n570/570 [==============================] - 10s 18ms/step - loss: 163.1626 - synergy_loss: 17.4943 - class_loss: 0.1204\nEpoch 759/1000\n570/570 [==============================] - 11s 20ms/step - loss: 163.3718 - synergy_loss: 17.9274 - class_loss: 0.1217\nEpoch 760/1000\n570/570 [==============================] - 10s 18ms/step - loss: 163.2065 - synergy_loss: 17.9584 - class_loss: 0.1236\nEpoch 761/1000\n570/570 [==============================] - 10s 18ms/step - loss: 162.6189 - synergy_loss: 17.5957 - class_loss: 0.1199\nEpoch 762/1000\n570/570 [==============================] - 11s 19ms/step - loss: 162.0748 - synergy_loss: 17.2565 - class_loss: 0.1235\nEpoch 763/1000\n570/570 [==============================] - 11s 18ms/step - loss: 162.0190 - synergy_loss: 17.4381 - class_loss: 0.1184\nEpoch 764/1000\n570/570 [==============================] - 10s 18ms/step - loss: 161.7404 - synergy_loss: 17.3835 - class_loss: 0.1227\nEpoch 765/1000\n570/570 [==============================] - 11s 19ms/step - loss: 161.2772 - synergy_loss: 17.1629 - class_loss: 0.1192\nEpoch 766/1000\n570/570 [==============================] - 10s 18ms/step - loss: 161.5901 - synergy_loss: 17.6960 - class_loss: 0.1200\nEpoch 767/1000\n570/570 [==============================] - 10s 18ms/step - loss: 161.2436 - synergy_loss: 17.5675 - class_loss: 0.1236\nEpoch 768/1000\n570/570 [==============================] - 11s 19ms/step - loss: 160.9579 - synergy_loss: 17.4922 - class_loss: 0.1200\nEpoch 769/1000\n570/570 [==============================] - 10s 18ms/step - loss: 160.8930 - synergy_loss: 17.6491 - class_loss: 0.1200\nEpoch 770/1000\n570/570 [==============================] - 10s 18ms/step - loss: 160.4985 - synergy_loss: 17.4800 - class_loss: 0.1194\nEpoch 771/1000\n570/570 [==============================] - 11s 19ms/step - loss: 159.8797 - synergy_loss: 17.0673 - class_loss: 0.1201\nEpoch 772/1000\n570/570 [==============================] - 10s 18ms/step - loss: 159.9562 - synergy_loss: 17.3650 - class_loss: 0.1203\nEpoch 773/1000\n570/570 [==============================] - 10s 18ms/step - loss: 159.5690 - synergy_loss: 17.2083 - class_loss: 0.1209\nEpoch 774/1000\n570/570 [==============================] - 11s 19ms/step - loss: 159.5385 - synergy_loss: 17.4008 - class_loss: 0.1177\nEpoch 775/1000\n570/570 [==============================] - 10s 18ms/step - loss: 159.3862 - synergy_loss: 17.4650 - class_loss: 0.1184\nEpoch 776/1000\n570/570 [==============================] - 10s 18ms/step - loss: 159.1559 - synergy_loss: 17.4510 - class_loss: 0.1172\nEpoch 777/1000\n570/570 [==============================] - 11s 19ms/step - loss: 158.7453 - synergy_loss: 17.2443 - class_loss: 0.1208\nEpoch 778/1000\n570/570 [==============================] - 10s 18ms/step - loss: 158.8682 - synergy_loss: 17.5868 - class_loss: 0.1201\nEpoch 779/1000\n570/570 [==============================] - 10s 18ms/step - loss: 158.1734 - synergy_loss: 17.1149 - class_loss: 0.1194\nEpoch 780/1000\n570/570 [==============================] - 11s 19ms/step - loss: 158.0508 - synergy_loss: 17.2050 - class_loss: 0.1187\nEpoch 781/1000\n570/570 [==============================] - 10s 18ms/step - loss: 157.8818 - synergy_loss: 17.2644 - class_loss: 0.1165\nEpoch 782/1000\n570/570 [==============================] - 10s 18ms/step - loss: 157.7728 - synergy_loss: 17.3365 - class_loss: 0.1197\nEpoch 783/1000\n570/570 [==============================] - 11s 19ms/step - loss: 157.0125 - synergy_loss: 16.8130 - class_loss: 0.1185\nEpoch 784/1000\n570/570 [==============================] - 10s 18ms/step - loss: 157.6982 - synergy_loss: 17.6940 - class_loss: 0.1171\nEpoch 785/1000\n570/570 [==============================] - 10s 18ms/step - loss: 156.8888 - synergy_loss: 17.0922 - class_loss: 0.1167\nEpoch 786/1000\n570/570 [==============================] - 11s 19ms/step - loss: 156.4330 - synergy_loss: 16.8616 - class_loss: 0.1164\nEpoch 787/1000\n570/570 [==============================] - 10s 18ms/step - loss: 156.0849 - synergy_loss: 16.7435 - class_loss: 0.1159\nEpoch 788/1000\n570/570 [==============================] - 10s 18ms/step - loss: 155.9099 - synergy_loss: 16.7888 - class_loss: 0.1115\nEpoch 789/1000\n570/570 [==============================] - 11s 19ms/step - loss: 156.0921 - synergy_loss: 17.1918 - class_loss: 0.1148\nEpoch 790/1000\n570/570 [==============================] - 10s 18ms/step - loss: 155.7073 - synergy_loss: 17.0331 - class_loss: 0.1179\nEpoch 791/1000\n570/570 [==============================] - 10s 18ms/step - loss: 156.0524 - synergy_loss: 17.5412 - class_loss: 0.1173\nEpoch 792/1000\n570/570 [==============================] - 11s 19ms/step - loss: 154.9505 - synergy_loss: 16.6770 - class_loss: 0.1135\nEpoch 793/1000\n570/570 [==============================] - 10s 18ms/step - loss: 155.1094 - synergy_loss: 17.0625 - class_loss: 0.1151\nEpoch 794/1000\n570/570 [==============================] - 10s 18ms/step - loss: 154.8982 - synergy_loss: 17.0708 - class_loss: 0.1169\nEpoch 795/1000\n570/570 [==============================] - 11s 19ms/step - loss: 154.6149 - synergy_loss: 17.0097 - class_loss: 0.1138\nEpoch 796/1000\n570/570 [==============================] - 10s 18ms/step - loss: 154.6548 - synergy_loss: 17.2427 - class_loss: 0.1159\nEpoch 797/1000\n570/570 [==============================] - 10s 18ms/step - loss: 153.9482 - synergy_loss: 16.7464 - class_loss: 0.1141\nEpoch 798/1000\n570/570 [==============================] - 10s 18ms/step - loss: 154.6616 - synergy_loss: 17.6536 - class_loss: 0.1168\nEpoch 799/1000\n570/570 [==============================] - 11s 19ms/step - loss: 153.6395 - synergy_loss: 16.8505 - class_loss: 0.1151\nEpoch 800/1000\n570/570 [==============================] - 10s 18ms/step - loss: 153.4545 - synergy_loss: 16.8805 - class_loss: 0.1140\nEpoch 801/1000\n570/570 [==============================] - 10s 18ms/step - loss: 152.9146 - synergy_loss: 16.5585 - class_loss: 0.1127\nEpoch 802/1000\n570/570 [==============================] - 11s 19ms/step - loss: 152.7906 - synergy_loss: 16.6607 - class_loss: 0.1103\nEpoch 803/1000\n570/570 [==============================] - 10s 18ms/step - loss: 152.7858 - synergy_loss: 16.8473 - class_loss: 0.1110\nEpoch 804/1000\n570/570 [==============================] - 10s 18ms/step - loss: 152.7519 - synergy_loss: 17.0223 - class_loss: 0.1149\nEpoch 805/1000\n570/570 [==============================] - 11s 19ms/step - loss: 152.3593 - synergy_loss: 16.8467 - class_loss: 0.1113\nEpoch 806/1000\n570/570 [==============================] - 10s 18ms/step - loss: 151.9944 - synergy_loss: 16.7191 - class_loss: 0.1112\nEpoch 807/1000\n570/570 [==============================] - 10s 18ms/step - loss: 152.2320 - synergy_loss: 17.1573 - class_loss: 0.1135\nEpoch 808/1000\n570/570 [==============================] - 11s 19ms/step - loss: 151.8347 - synergy_loss: 16.9659 - class_loss: 0.1133\nEpoch 809/1000\n570/570 [==============================] - 10s 18ms/step - loss: 151.4364 - synergy_loss: 16.7715 - class_loss: 0.1132\nEpoch 810/1000\n570/570 [==============================] - 10s 18ms/step - loss: 151.0865 - synergy_loss: 16.6466 - class_loss: 0.1102\nEpoch 811/1000\n570/570 [==============================] - 11s 19ms/step - loss: 151.1478 - synergy_loss: 16.9358 - class_loss: 0.1123\nEpoch 812/1000\n570/570 [==============================] - 10s 18ms/step - loss: 151.1506 - synergy_loss: 17.1085 - class_loss: 0.1106\nEpoch 813/1000\n570/570 [==============================] - 10s 18ms/step - loss: 150.1672 - synergy_loss: 16.3662 - class_loss: 0.1110\nEpoch 814/1000\n570/570 [==============================] - 11s 19ms/step - loss: 150.2990 - synergy_loss: 16.7111 - class_loss: 0.1098\nEpoch 815/1000\n570/570 [==============================] - 10s 18ms/step - loss: 149.6686 - synergy_loss: 16.2899 - class_loss: 0.1091\nEpoch 816/1000\n570/570 [==============================] - 10s 18ms/step - loss: 149.5825 - synergy_loss: 16.4285 - class_loss: 0.1117\nEpoch 817/1000\n570/570 [==============================] - 11s 19ms/step - loss: 149.7025 - synergy_loss: 16.7586 - class_loss: 0.1092\nEpoch 818/1000\n570/570 [==============================] - 10s 18ms/step - loss: 149.6580 - synergy_loss: 16.9257 - class_loss: 0.1095\nEpoch 819/1000\n570/570 [==============================] - 10s 18ms/step - loss: 149.0219 - synergy_loss: 16.5102 - class_loss: 0.1096\nEpoch 820/1000\n570/570 [==============================] - 11s 19ms/step - loss: 148.8785 - synergy_loss: 16.5721 - class_loss: 0.1089\nEpoch 821/1000\n570/570 [==============================] - 10s 18ms/step - loss: 148.9310 - synergy_loss: 16.8351 - class_loss: 0.1096\nEpoch 822/1000\n570/570 [==============================] - 10s 18ms/step - loss: 148.5086 - synergy_loss: 16.6302 - class_loss: 0.1089\nEpoch 823/1000\n570/570 [==============================] - 11s 19ms/step - loss: 148.7827 - synergy_loss: 17.0824 - class_loss: 0.1119\nEpoch 824/1000\n570/570 [==============================] - 10s 18ms/step - loss: 148.3538 - synergy_loss: 16.8477 - class_loss: 0.1124\nEpoch 825/1000\n570/570 [==============================] - 10s 18ms/step - loss: 147.7678 - synergy_loss: 16.4802 - class_loss: 0.1127\nEpoch 826/1000\n570/570 [==============================] - 11s 19ms/step - loss: 147.3064 - synergy_loss: 16.2370 - class_loss: 0.1083\nEpoch 827/1000\n570/570 [==============================] - 10s 18ms/step - loss: 147.2889 - synergy_loss: 16.4192 - class_loss: 0.1113\nEpoch 828/1000\n570/570 [==============================] - 10s 18ms/step - loss: 147.1299 - synergy_loss: 16.4783 - class_loss: 0.1068\nEpoch 829/1000\n570/570 [==============================] - 11s 19ms/step - loss: 146.8728 - synergy_loss: 16.4292 - class_loss: 0.1072\nEpoch 830/1000\n570/570 [==============================] - 10s 18ms/step - loss: 147.1464 - synergy_loss: 16.8941 - class_loss: 0.1091\nEpoch 831/1000\n570/570 [==============================] - 10s 18ms/step - loss: 146.5861 - synergy_loss: 16.5481 - class_loss: 0.1076\nEpoch 832/1000\n570/570 [==============================] - 11s 19ms/step - loss: 146.0641 - synergy_loss: 16.2404 - class_loss: 0.1045\nEpoch 833/1000\n570/570 [==============================] - 10s 18ms/step - loss: 145.7790 - synergy_loss: 16.1874 - class_loss: 0.1052\nEpoch 834/1000\n570/570 [==============================] - 10s 18ms/step - loss: 145.9724 - synergy_loss: 16.5752 - class_loss: 0.1087\nEpoch 835/1000\n570/570 [==============================] - 11s 19ms/step - loss: 145.3869 - synergy_loss: 16.2036 - class_loss: 0.1068\nEpoch 836/1000\n570/570 [==============================] - 10s 18ms/step - loss: 145.5263 - synergy_loss: 16.5485 - class_loss: 0.1054\nEpoch 837/1000\n570/570 [==============================] - 10s 18ms/step - loss: 145.3395 - synergy_loss: 16.5555 - class_loss: 0.1060\nEpoch 838/1000\n570/570 [==============================] - 11s 19ms/step - loss: 144.9321 - synergy_loss: 16.3631 - class_loss: 0.1080\nEpoch 839/1000\n570/570 [==============================] - 10s 18ms/step - loss: 144.7232 - synergy_loss: 16.3575 - class_loss: 0.1056\nEpoch 840/1000\n570/570 [==============================] - 10s 18ms/step - loss: 144.6723 - synergy_loss: 16.5203 - class_loss: 0.1064\nEpoch 841/1000\n570/570 [==============================] - 11s 19ms/step - loss: 144.0611 - synergy_loss: 16.1217 - class_loss: 0.1074\nEpoch 842/1000\n570/570 [==============================] - 10s 18ms/step - loss: 143.9565 - synergy_loss: 16.2475 - class_loss: 0.1045\nEpoch 843/1000\n570/570 [==============================] - 10s 18ms/step - loss: 144.2002 - synergy_loss: 16.6872 - class_loss: 0.1055\nEpoch 844/1000\n570/570 [==============================] - 11s 19ms/step - loss: 143.8736 - synergy_loss: 16.5268 - class_loss: 0.1039\nEpoch 845/1000\n570/570 [==============================] - 10s 18ms/step - loss: 143.7155 - synergy_loss: 16.5785 - class_loss: 0.1039\nEpoch 846/1000\n570/570 [==============================] - 11s 19ms/step - loss: 143.3473 - synergy_loss: 16.4146 - class_loss: 0.1035\nEpoch 847/1000\n570/570 [==============================] - 11s 19ms/step - loss: 142.8865 - synergy_loss: 16.1695 - class_loss: 0.1042\nEpoch 848/1000\n570/570 [==============================] - 10s 18ms/step - loss: 142.6366 - synergy_loss: 16.1270 - class_loss: 0.1066\nEpoch 849/1000\n570/570 [==============================] - 10s 18ms/step - loss: 142.3474 - synergy_loss: 16.0292 - class_loss: 0.1037\nEpoch 850/1000\n570/570 [==============================] - 11s 19ms/step - loss: 142.4480 - synergy_loss: 16.3513 - class_loss: 0.1036\nEpoch 851/1000\n570/570 [==============================] - 10s 18ms/step - loss: 142.4196 - synergy_loss: 16.5100 - class_loss: 0.1024\nEpoch 852/1000\n570/570 [==============================] - 10s 18ms/step - loss: 141.8277 - synergy_loss: 16.1379 - class_loss: 0.1020\nEpoch 853/1000\n570/570 [==============================] - 11s 20ms/step - loss: 141.9167 - synergy_loss: 16.4360 - class_loss: 0.1016\nEpoch 854/1000\n570/570 [==============================] - 10s 18ms/step - loss: 141.2584 - synergy_loss: 15.9906 - class_loss: 0.1012\nEpoch 855/1000\n570/570 [==============================] - 10s 18ms/step - loss: 141.5690 - synergy_loss: 16.4915 - class_loss: 0.1031\nEpoch 856/1000\n570/570 [==============================] - 11s 19ms/step - loss: 142.2763 - synergy_loss: 17.2788 - class_loss: 0.1032\nEpoch 857/1000\n570/570 [==============================] - 10s 18ms/step - loss: 140.5823 - synergy_loss: 15.8164 - class_loss: 0.1021\nEpoch 858/1000\n570/570 [==============================] - 10s 18ms/step - loss: 140.4518 - synergy_loss: 15.9005 - class_loss: 0.1011\nEpoch 859/1000\n570/570 [==============================] - 11s 19ms/step - loss: 140.6396 - synergy_loss: 16.2818 - class_loss: 0.1008\nEpoch 860/1000\n570/570 [==============================] - 10s 18ms/step - loss: 142.3192 - synergy_loss: 18.0073 - class_loss: 0.1043\nEpoch 861/1000\n570/570 [==============================] - 10s 18ms/step - loss: 140.1539 - synergy_loss: 16.0552 - class_loss: 0.1040\nEpoch 862/1000\n570/570 [==============================] - 11s 19ms/step - loss: 140.0062 - synergy_loss: 16.1071 - class_loss: 0.1051\nEpoch 863/1000\n570/570 [==============================] - 10s 18ms/step - loss: 139.7592 - synergy_loss: 16.0600 - class_loss: 0.1029\nEpoch 864/1000\n570/570 [==============================] - 10s 18ms/step - loss: 139.6400 - synergy_loss: 16.1456 - class_loss: 0.1053\nEpoch 865/1000\n570/570 [==============================] - 11s 18ms/step - loss: 139.6685 - synergy_loss: 16.3561 - class_loss: 0.0996\nEpoch 866/1000\n570/570 [==============================] - 11s 18ms/step - loss: 139.3160 - synergy_loss: 16.1839 - class_loss: 0.1027\nEpoch 867/1000\n570/570 [==============================] - 10s 18ms/step - loss: 139.0481 - synergy_loss: 16.1155 - class_loss: 0.1027\nEpoch 868/1000\n570/570 [==============================] - 10s 18ms/step - loss: 139.2705 - synergy_loss: 16.4999 - class_loss: 0.1050\nEpoch 869/1000\n570/570 [==============================] - 11s 19ms/step - loss: 139.0510 - synergy_loss: 16.4579 - class_loss: 0.1023\nEpoch 870/1000\n570/570 [==============================] - 10s 18ms/step - loss: 138.8459 - synergy_loss: 16.4446 - class_loss: 0.1044\nEpoch 871/1000\n570/570 [==============================] - 10s 18ms/step - loss: 138.4762 - synergy_loss: 16.2660 - class_loss: 0.1014\nEpoch 872/1000\n570/570 [==============================] - 11s 19ms/step - loss: 137.8994 - synergy_loss: 15.8953 - class_loss: 0.1017\nEpoch 873/1000\n570/570 [==============================] - 10s 18ms/step - loss: 137.9039 - synergy_loss: 16.0927 - class_loss: 0.1006\nEpoch 874/1000\n570/570 [==============================] - 10s 18ms/step - loss: 137.6200 - synergy_loss: 15.9996 - class_loss: 0.1008\nEpoch 875/1000\n570/570 [==============================] - 11s 19ms/step - loss: 137.5874 - synergy_loss: 16.1450 - class_loss: 0.1002\nEpoch 876/1000\n570/570 [==============================] - 10s 18ms/step - loss: 137.4281 - synergy_loss: 16.1888 - class_loss: 0.0999\nEpoch 877/1000\n570/570 [==============================] - 10s 18ms/step - loss: 137.2190 - synergy_loss: 16.1752 - class_loss: 0.0998\nEpoch 878/1000\n570/570 [==============================] - 11s 19ms/step - loss: 136.8545 - synergy_loss: 16.0051 - class_loss: 0.0980\nEpoch 879/1000\n570/570 [==============================] - 10s 18ms/step - loss: 136.4368 - synergy_loss: 15.7932 - class_loss: 0.1008\nEpoch 880/1000\n570/570 [==============================] - 10s 18ms/step - loss: 136.2148 - synergy_loss: 15.7701 - class_loss: 0.0977\nEpoch 881/1000\n570/570 [==============================] - 11s 19ms/step - loss: 136.0732 - synergy_loss: 15.8126 - class_loss: 0.0989\nEpoch 882/1000\n570/570 [==============================] - 10s 18ms/step - loss: 135.9081 - synergy_loss: 15.8605 - class_loss: 0.1006\nEpoch 883/1000\n570/570 [==============================] - 10s 18ms/step - loss: 135.4188 - synergy_loss: 15.5826 - class_loss: 0.1024\nEpoch 884/1000\n570/570 [==============================] - 11s 19ms/step - loss: 135.8096 - synergy_loss: 16.1548 - class_loss: 0.0986\nEpoch 885/1000\n570/570 [==============================] - 10s 18ms/step - loss: 135.3384 - synergy_loss: 15.8917 - class_loss: 0.0981\nEpoch 886/1000\n570/570 [==============================] - 10s 18ms/step - loss: 135.2870 - synergy_loss: 16.0260 - class_loss: 0.0967\nEpoch 887/1000\n570/570 [==============================] - 11s 19ms/step - loss: 134.8500 - synergy_loss: 15.7917 - class_loss: 0.0993\nEpoch 888/1000\n570/570 [==============================] - 10s 18ms/step - loss: 134.6022 - synergy_loss: 15.7540 - class_loss: 0.0988\nEpoch 889/1000\n570/570 [==============================] - 10s 18ms/step - loss: 134.4777 - synergy_loss: 15.8222 - class_loss: 0.0973\nEpoch 890/1000\n570/570 [==============================] - 11s 19ms/step - loss: 134.2440 - synergy_loss: 15.7844 - class_loss: 0.0972\nEpoch 891/1000\n570/570 [==============================] - 10s 18ms/step - loss: 134.1746 - synergy_loss: 15.9184 - class_loss: 0.0954\nEpoch 892/1000\n570/570 [==============================] - 10s 18ms/step - loss: 133.7811 - synergy_loss: 15.7084 - class_loss: 0.0995\nEpoch 893/1000\n570/570 [==============================] - 11s 19ms/step - loss: 133.3962 - synergy_loss: 15.5448 - class_loss: 0.0962\nEpoch 894/1000\n570/570 [==============================] - 10s 18ms/step - loss: 133.4514 - synergy_loss: 15.7960 - class_loss: 0.0971\nEpoch 895/1000\n570/570 [==============================] - 10s 18ms/step - loss: 133.2842 - synergy_loss: 15.8118 - class_loss: 0.0958\nEpoch 896/1000\n570/570 [==============================] - 11s 19ms/step - loss: 132.9449 - synergy_loss: 15.6636 - class_loss: 0.0962\nEpoch 897/1000\n570/570 [==============================] - 10s 18ms/step - loss: 132.5821 - synergy_loss: 15.5128 - class_loss: 0.0992\nEpoch 898/1000\n570/570 [==============================] - 10s 18ms/step - loss: 132.7075 - synergy_loss: 15.8282 - class_loss: 0.0931\nEpoch 899/1000\n570/570 [==============================] - 11s 19ms/step - loss: 132.4559 - synergy_loss: 15.7660 - class_loss: 0.0967\nEpoch 900/1000\n570/570 [==============================] - 10s 18ms/step - loss: 132.1534 - synergy_loss: 15.6613 - class_loss: 0.0955\nEpoch 901/1000\n570/570 [==============================] - 10s 18ms/step - loss: 131.9584 - synergy_loss: 15.6513 - class_loss: 0.0935\nEpoch 902/1000\n570/570 [==============================] - 11s 19ms/step - loss: 131.5565 - synergy_loss: 15.4580 - class_loss: 0.0955\nEpoch 903/1000\n570/570 [==============================] - 10s 18ms/step - loss: 131.4541 - synergy_loss: 15.5277 - class_loss: 0.0962\nEpoch 904/1000\n570/570 [==============================] - 10s 18ms/step - loss: 131.5088 - synergy_loss: 15.7658 - class_loss: 0.0965\nEpoch 905/1000\n570/570 [==============================] - 11s 19ms/step - loss: 131.2488 - synergy_loss: 15.7006 - class_loss: 0.0970\nEpoch 906/1000\n570/570 [==============================] - 10s 18ms/step - loss: 131.4513 - synergy_loss: 16.0952 - class_loss: 0.0953\nEpoch 907/1000\n570/570 [==============================] - 10s 18ms/step - loss: 131.1193 - synergy_loss: 15.9235 - class_loss: 0.0948\nEpoch 908/1000\n570/570 [==============================] - 11s 19ms/step - loss: 130.5744 - synergy_loss: 15.5810 - class_loss: 0.0937\nEpoch 909/1000\n570/570 [==============================] - 10s 18ms/step - loss: 130.1392 - synergy_loss: 15.3507 - class_loss: 0.0920\nEpoch 910/1000\n570/570 [==============================] - 10s 18ms/step - loss: 130.0779 - synergy_loss: 15.4741 - class_loss: 0.0949\nEpoch 911/1000\n570/570 [==============================] - 11s 19ms/step - loss: 129.9003 - synergy_loss: 15.4907 - class_loss: 0.0972\nEpoch 912/1000\n570/570 [==============================] - 10s 18ms/step - loss: 129.7785 - synergy_loss: 15.5692 - class_loss: 0.0964\nEpoch 913/1000\n570/570 [==============================] - 10s 18ms/step - loss: 129.5708 - synergy_loss: 15.5453 - class_loss: 0.0964\nEpoch 914/1000\n570/570 [==============================] - 11s 19ms/step - loss: 129.3289 - synergy_loss: 15.4913 - class_loss: 0.0952\nEpoch 915/1000\n570/570 [==============================] - 11s 18ms/step - loss: 129.2081 - synergy_loss: 15.5580 - class_loss: 0.0915\nEpoch 916/1000\n570/570 [==============================] - 10s 18ms/step - loss: 128.9002 - synergy_loss: 15.4373 - class_loss: 0.0913\nEpoch 917/1000\n570/570 [==============================] - 11s 19ms/step - loss: 129.4390 - synergy_loss: 16.1476 - class_loss: 0.0938\nEpoch 918/1000\n570/570 [==============================] - 10s 18ms/step - loss: 128.4530 - synergy_loss: 15.3430 - class_loss: 0.0946\nEpoch 919/1000\n570/570 [==============================] - 11s 18ms/step - loss: 128.6914 - synergy_loss: 15.7477 - class_loss: 0.0925\nEpoch 920/1000\n570/570 [==============================] - 11s 19ms/step - loss: 128.1824 - synergy_loss: 15.4377 - class_loss: 0.0930\nEpoch 921/1000\n570/570 [==============================] - 10s 18ms/step - loss: 128.1085 - synergy_loss: 15.5362 - class_loss: 0.0925\nEpoch 922/1000\n570/570 [==============================] - 10s 18ms/step - loss: 127.7987 - synergy_loss: 15.4194 - class_loss: 0.0910\nEpoch 923/1000\n570/570 [==============================] - 11s 19ms/step - loss: 127.6927 - synergy_loss: 15.4827 - class_loss: 0.0932\nEpoch 924/1000\n570/570 [==============================] - 10s 18ms/step - loss: 127.7396 - synergy_loss: 15.7065 - class_loss: 0.0931\nEpoch 925/1000\n570/570 [==============================] - 10s 18ms/step - loss: 127.0489 - synergy_loss: 15.2133 - class_loss: 0.0935\nEpoch 926/1000\n570/570 [==============================] - 11s 18ms/step - loss: 127.5076 - synergy_loss: 15.8422 - class_loss: 0.0930\nEpoch 927/1000\n570/570 [==============================] - 10s 18ms/step - loss: 126.7210 - synergy_loss: 15.2216 - class_loss: 0.0907\nEpoch 928/1000\n570/570 [==============================] - 10s 18ms/step - loss: 126.6650 - synergy_loss: 15.3647 - class_loss: 0.0909\nEpoch 929/1000\n570/570 [==============================] - 10s 18ms/step - loss: 126.6858 - synergy_loss: 15.5711 - class_loss: 0.0895\nEpoch 930/1000\n570/570 [==============================] - 11s 19ms/step - loss: 126.9538 - synergy_loss: 15.9764 - class_loss: 0.0941\nEpoch 931/1000\n570/570 [==============================] - 10s 18ms/step - loss: 126.5601 - synergy_loss: 15.7531 - class_loss: 0.0930\nEpoch 932/1000\n570/570 [==============================] - 10s 18ms/step - loss: 125.9575 - synergy_loss: 15.3374 - class_loss: 0.0918\nEpoch 933/1000\n570/570 [==============================] - 11s 19ms/step - loss: 125.9104 - synergy_loss: 15.4745 - class_loss: 0.0923\nEpoch 934/1000\n570/570 [==============================] - 10s 18ms/step - loss: 125.5980 - synergy_loss: 15.3431 - class_loss: 0.0902\nEpoch 935/1000\n570/570 [==============================] - 10s 18ms/step - loss: 126.8727 - synergy_loss: 16.6638 - class_loss: 0.0932\nEpoch 936/1000\n570/570 [==============================] - 11s 19ms/step - loss: 125.8273 - synergy_loss: 15.7907 - class_loss: 0.0936\nEpoch 937/1000\n570/570 [==============================] - 10s 18ms/step - loss: 124.7604 - synergy_loss: 14.9263 - class_loss: 0.0901\nEpoch 938/1000\n570/570 [==============================] - 10s 18ms/step - loss: 125.1225 - synergy_loss: 15.4659 - class_loss: 0.0917\nEpoch 939/1000\n570/570 [==============================] - 11s 19ms/step - loss: 124.7688 - synergy_loss: 15.2732 - class_loss: 0.0922\nEpoch 940/1000\n570/570 [==============================] - 10s 18ms/step - loss: 124.5682 - synergy_loss: 15.2433 - class_loss: 0.0923\nEpoch 941/1000\n570/570 [==============================] - 10s 18ms/step - loss: 124.5092 - synergy_loss: 15.3615 - class_loss: 0.0891\nEpoch 942/1000\n570/570 [==============================] - 11s 19ms/step - loss: 124.3967 - synergy_loss: 15.4146 - class_loss: 0.0929\nEpoch 943/1000\n570/570 [==============================] - 10s 18ms/step - loss: 124.1185 - synergy_loss: 15.3132 - class_loss: 0.0875\nEpoch 944/1000\n570/570 [==============================] - 10s 18ms/step - loss: 124.2566 - synergy_loss: 15.6206 - class_loss: 0.0894\nEpoch 945/1000\n570/570 [==============================] - 11s 19ms/step - loss: 123.6332 - synergy_loss: 15.1652 - class_loss: 0.0912\nEpoch 946/1000\n570/570 [==============================] - 10s 18ms/step - loss: 123.6623 - synergy_loss: 15.3669 - class_loss: 0.0913\nEpoch 947/1000\n570/570 [==============================] - 10s 18ms/step - loss: 123.5878 - synergy_loss: 15.4694 - class_loss: 0.0902\nEpoch 948/1000\n570/570 [==============================] - 11s 19ms/step - loss: 123.3386 - synergy_loss: 15.3954 - class_loss: 0.0901\nEpoch 949/1000\n570/570 [==============================] - 11s 18ms/step - loss: 123.2562 - synergy_loss: 15.4911 - class_loss: 0.0893\nEpoch 950/1000\n570/570 [==============================] - 10s 18ms/step - loss: 123.3505 - synergy_loss: 15.6820 - class_loss: 0.0900\nEpoch 951/1000\n570/570 [==============================] - 11s 20ms/step - loss: 122.3634 - synergy_loss: 14.8944 - class_loss: 0.0888\nEpoch 952/1000\n570/570 [==============================] - 10s 18ms/step - loss: 122.3913 - synergy_loss: 15.1000 - class_loss: 0.0915\nEpoch 953/1000\n570/570 [==============================] - 10s 18ms/step - loss: 122.2784 - synergy_loss: 15.1590 - class_loss: 0.0896\nEpoch 954/1000\n570/570 [==============================] - 11s 19ms/step - loss: 122.0835 - synergy_loss: 15.1329 - class_loss: 0.0877\nEpoch 955/1000\n570/570 [==============================] - 10s 18ms/step - loss: 121.7795 - synergy_loss: 14.9902 - class_loss: 0.0902\nEpoch 956/1000\n570/570 [==============================] - 10s 18ms/step - loss: 121.9562 - synergy_loss: 15.3355 - class_loss: 0.0881\nEpoch 957/1000\n570/570 [==============================] - 11s 19ms/step - loss: 121.3727 - synergy_loss: 14.9379 - class_loss: 0.0869\nEpoch 958/1000\n570/570 [==============================] - 10s 18ms/step - loss: 121.6058 - synergy_loss: 15.3272 - class_loss: 0.0881\nEpoch 959/1000\n570/570 [==============================] - 10s 18ms/step - loss: 121.1272 - synergy_loss: 15.0270 - class_loss: 0.0854\nEpoch 960/1000\n570/570 [==============================] - 11s 19ms/step - loss: 121.1786 - synergy_loss: 15.2435 - class_loss: 0.0893\nEpoch 961/1000\n570/570 [==============================] - 11s 18ms/step - loss: 121.1121 - synergy_loss: 15.3514 - class_loss: 0.0920\nEpoch 962/1000\n570/570 [==============================] - 10s 18ms/step - loss: 120.3544 - synergy_loss: 14.7724 - class_loss: 0.0892\nEpoch 963/1000\n570/570 [==============================] - 11s 19ms/step - loss: 121.1061 - synergy_loss: 15.6914 - class_loss: 0.0865\nEpoch 964/1000\n570/570 [==============================] - 10s 18ms/step - loss: 120.7578 - synergy_loss: 15.5024 - class_loss: 0.0868\nEpoch 965/1000\n570/570 [==============================] - 10s 18ms/step - loss: 120.2361 - synergy_loss: 15.1424 - class_loss: 0.0868\nEpoch 966/1000\n570/570 [==============================] - 11s 19ms/step - loss: 120.0562 - synergy_loss: 15.1287 - class_loss: 0.0903\nEpoch 967/1000\n570/570 [==============================] - 10s 18ms/step - loss: 119.7920 - synergy_loss: 15.0285 - class_loss: 0.0897\nEpoch 968/1000\n570/570 [==============================] - 10s 18ms/step - loss: 119.6397 - synergy_loss: 15.0706 - class_loss: 0.0848\nEpoch 969/1000\n570/570 [==============================] - 11s 19ms/step - loss: 119.3880 - synergy_loss: 14.9748 - class_loss: 0.0849\nEpoch 970/1000\n570/570 [==============================] - 10s 18ms/step - loss: 119.2051 - synergy_loss: 14.9594 - class_loss: 0.0866\nEpoch 971/1000\n570/570 [==============================] - 10s 18ms/step - loss: 119.2783 - synergy_loss: 15.2083 - class_loss: 0.0863\nEpoch 972/1000\n570/570 [==============================] - 11s 19ms/step - loss: 118.8888 - synergy_loss: 14.9851 - class_loss: 0.0858\nEpoch 973/1000\n570/570 [==============================] - 10s 18ms/step - loss: 118.8070 - synergy_loss: 15.0699 - class_loss: 0.0855\nEpoch 974/1000\n570/570 [==============================] - 10s 18ms/step - loss: 118.6583 - synergy_loss: 15.0788 - class_loss: 0.0877\nEpoch 975/1000\n570/570 [==============================] - 11s 19ms/step - loss: 118.3445 - synergy_loss: 14.9375 - class_loss: 0.0860\nEpoch 976/1000\n570/570 [==============================] - 10s 18ms/step - loss: 118.4171 - synergy_loss: 15.1716 - class_loss: 0.0867\nEpoch 977/1000\n570/570 [==============================] - 10s 18ms/step - loss: 118.1696 - synergy_loss: 15.0985 - class_loss: 0.0862\nEpoch 978/1000\n570/570 [==============================] - 11s 19ms/step - loss: 117.9893 - synergy_loss: 15.0907 - class_loss: 0.0861\nEpoch 979/1000\n570/570 [==============================] - 10s 18ms/step - loss: 117.7319 - synergy_loss: 14.9940 - class_loss: 0.0833\nEpoch 980/1000\n570/570 [==============================] - 10s 18ms/step - loss: 117.5988 - synergy_loss: 15.0217 - class_loss: 0.0885\nEpoch 981/1000\n570/570 [==============================] - 11s 19ms/step - loss: 117.4733 - synergy_loss: 15.0524 - class_loss: 0.0875\nEpoch 982/1000\n570/570 [==============================] - 10s 18ms/step - loss: 117.1703 - synergy_loss: 14.9205 - class_loss: 0.0841\nEpoch 983/1000\n570/570 [==============================] - 10s 18ms/step - loss: 116.9487 - synergy_loss: 14.8518 - class_loss: 0.0862\nEpoch 984/1000\n570/570 [==============================] - 11s 19ms/step - loss: 116.9901 - synergy_loss: 15.0567 - class_loss: 0.0870\nEpoch 985/1000\n570/570 [==============================] - 10s 18ms/step - loss: 116.9149 - synergy_loss: 15.1368 - class_loss: 0.0861\nEpoch 986/1000\n570/570 [==============================] - 10s 18ms/step - loss: 116.5856 - synergy_loss: 14.9725 - class_loss: 0.0878\nEpoch 987/1000\n570/570 [==============================] - 11s 19ms/step - loss: 116.2296 - synergy_loss: 14.7799 - class_loss: 0.0841\nEpoch 988/1000\n570/570 [==============================] - 10s 18ms/step - loss: 116.2598 - synergy_loss: 14.9788 - class_loss: 0.0859\nEpoch 989/1000\n570/570 [==============================] - 10s 18ms/step - loss: 116.3069 - synergy_loss: 15.1605 - class_loss: 0.0856\nEpoch 990/1000\n570/570 [==============================] - 11s 19ms/step - loss: 115.8421 - synergy_loss: 14.8698 - class_loss: 0.0830\nEpoch 991/1000\n570/570 [==============================] - 11s 19ms/step - loss: 115.9489 - synergy_loss: 15.1291 - class_loss: 0.0861\nEpoch 992/1000\n570/570 [==============================] - 10s 18ms/step - loss: 115.8169 - synergy_loss: 15.1313 - class_loss: 0.0863\nEpoch 993/1000\n570/570 [==============================] - 11s 19ms/step - loss: 115.3923 - synergy_loss: 14.8752 - class_loss: 0.0820\nEpoch 994/1000\n570/570 [==============================] - 10s 18ms/step - loss: 115.0675 - synergy_loss: 14.7177 - class_loss: 0.0843\nEpoch 995/1000\n570/570 [==============================] - 10s 18ms/step - loss: 115.2178 - synergy_loss: 15.0147 - class_loss: 0.0861\nEpoch 996/1000\n570/570 [==============================] - 11s 19ms/step - loss: 115.1301 - synergy_loss: 15.0515 - class_loss: 0.0859\nEpoch 997/1000\n570/570 [==============================] - 10s 18ms/step - loss: 114.6716 - synergy_loss: 14.7694 - class_loss: 0.0853\nEpoch 998/1000\n570/570 [==============================] - 10s 18ms/step - loss: 114.7394 - synergy_loss: 14.9923 - class_loss: 0.0845\nEpoch 999/1000\n570/570 [==============================] - 11s 19ms/step - loss: 114.5811 - synergy_loss: 14.9919 - class_loss: 0.0825\nEpoch 1000/1000\n570/570 [==============================] - 10s 18ms/step - loss: 115.5633 - synergy_loss: 16.0648 - class_loss: 0.0858\n283/283 [==============================] - 2s 5ms/step\nmsynergy_mean_squared_error 292.66528\nmclass_mean_squared_error 10.639238\nmsynergy_r2_score 0.5266119446538918\nmsynergy_pear (array([0.7302853582528049], dtype=object), 0.0)\nmsynergy_spear SpearmanrResult(correlation=0.7029148643651731, pvalue=0.0)\nmsclass_roc_curve 0.9159982444894871\nmclass_accuracy_scorer 0.8679337231968811\nmclass_cohen_kappa_score 0.49489896017265056\nmclass_precision_score 0.91005291005291\nmclass_average_precision_score 0.8022913144078025\n","output_type":"stream"}]},{"cell_type":"code","source":"   \n\nfrom IPython.display import FileLink    \nnp.savetxt('npred_syn3.csv', ap11 ,delimiter=',')\nFileLink(r'npred_syn3.csv')\n\nnp.savetxt('npred_cls3.csv', ap22 ,delimiter=',') \nFileLink(r'npred_cls3.csv')\n \nnp.savetxt('ntest_syn3.csv', test_synergy1 ,delimiter=',')\nFileLink(r'ntest_syn3.csv')\n\nnp.savetxt('ntest_cls3.csv', test_class1 ,delimiter=',')\nFileLink(r'ntest_cls3.csv')\n\n","metadata":{"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/ntest_cls3.csv","text/html":"<a href='ntest_cls3.csv' target='_blank'>ntest_cls3.csv</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"##### ","metadata":{}}]}