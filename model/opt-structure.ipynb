{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install deepchem\n!pip install pysmiles\n!pip install openpyxl\n# !pip install rdkit\n\nfrom keras.callbacks import LambdaCallback\n# !pip install MolGraphConvFeaturizer\n!pip install PubChemPy\n!pip install PyDrive\n!pip install xlrd==2.0.1\n!pip install Pandas==1.3.5\n# !pip install Pandas==1.1.5\n!pip install  gdown\nimport tensorflow as tf\nimport deepchem as dc\nimport random\nfrom rdkit import Chem\nprint(\"hjjbjh\")\nfrom pysmiles import read_smiles\nimport networkx as nx\nfrom deepchem.feat import MolGraphConvFeaturizer,PagtnMolGraphFeaturizer\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import spdiags\n!pip install pyyaml h5py \nimport os\nfrom sklearn.model_selection import train_test_split\n#   gauth = GoogleAuth()\n#   gauth.credentials = GoogleCredentials.get_application_default()\n#   drivea = GoogleDrive(gauth)\n#   drive.mount('/content/drive')\ndef normalize1(X, means1=None, std1=None, means2=None, std2=None, feat_filt=None, norm='tanh_norm'):\n    if std1 is None:\n        std1 = np.nanstd(X, axis=0)\n    if feat_filt is None:\n        feat_filt = std1!=0\n    X = X[:,feat_filt]\n    X = np.ascontiguousarray(X)\n    if means1 is None:\n        means1 = np.mean(X, axis=0)\n    X = (X-means1)/std1[feat_filt]\n    if norm == 'norm':\n        return(X, means1, std1, feat_filt)\n    elif norm == 'tanh':\n        return(np.tanh(X), means1, std1, feat_filt)\n    elif norm == 'tanh_norm':\n        X = np.tanh(X)\n        if means2 is None:\n            means2 = np.mean(X, axis=0)\n        if std2 is None:\n            std2 = np.std(X, axis=0)\n        X = (X-means2)/std2\n        X[:,std2==0]=0\n        return(X, means1, std1, means2, std2, feat_filt) \n\n\n    \ndef get_data():\n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  smiles=pd.read_excel('pubchem.xls', header=None)\n  smiles=np.array(smiles)\n\n\n\n#   !gdown --id 1cGZhmLqTWhgtvlhU2Zdi8htgIlErhJZU\n#   data_to_repeat=pd.read_excel('synergy.xlsx', header=None)\n#   data_to_repeat=np.array(data_to_repeat)\n\n\n  !gdown --id 10ztxKtGSVU7p9yPoCsTnapVLThWpjjYT\n  data_to_repeat=pd.read_excel('oneil.xlsx', header=None)\n  data_to_repeat=np.array(data_to_repeat)\n  \n\n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  unique_drugs=pd.read_excel('pubchem.xls', header=None)\n  unique_drugs=np.array(unique_drugs)         \n\n#   !gdown https://drive.google.com/uc?id=1IMr5zMLRAXC5iE2MAHJbKmHf2-0DCZNd\n#   !gdown --id 1bBJUFBA4Tm9YdE5OxA1mbUfBI8B7wqcr\n#   feature_cell=pd.read_excel('final_feature_cell.xlsx', header=None)\n#   feature_cell=np.array(feature_cell)\n\n#   !gdown --id 1NoUKCRZ0CkNVFP1-578x_L0otPUMkz6T\n#   feature_cell=pd.read_excel('unique934_cell.xlsx',header=None)\n#   feature_cell=np.array(feature_cell)\n#   a=np.zeros((1,934))\n#   feature_cell[22,1:]=a\n#   feature_cell[36,1:]=a\n\n#   !gdown --id 109nyFVOO_P9DdyrhWzbNg0FBB_Y8gD58\n#   feature_cell=pd.read_excel('final_deep_cell.xlsx',header=None)\n#   feature_cell=np.array(feature_cell)\n\n#   !gdown --id 14ZJ2gOn4jumVmEulo4Fky17rprSwxLot\n#   feature_cell=pd.read_excel('cell_377.xlsx',header=None)\n#   feature_cell=np.array(feature_cell)\n\n#   !gdown --id 13H-rtUbhrHzd2y2LlhMnAwv4rJHlYYux\n#   feature_cell=pd.read_excel('cell_line.xlsx',header=None)\n#   feature_cell=np.array(feature_cell)\n\n#   !gdown --id 1_Doucq5zqEuM6oZWBYIGPHmr_KagFH-T\n#   feature_cell=pd.read_excel('cell_874.xlsx')\n#   feature_cell=np.array(feature_cell)\n    \n  !gdown --id 1rVt2qEH-LMzk86ig6c8Qjll-08LhsxBv\n  feature_cell=pd.read_excel('cell_expression875.xlsx')\n  feature_cell=np.array(feature_cell)\n  feature_cell[:,1:]=(2**feature_cell[:,1:])-1\n    \n  !gdown --id 14upx46iIPcO80y_tcvbaS3uwMkZfKLVw\n  redkit_drug=pd.read_excel('redkit_drug.xlsx',header=None)\n  redkit_drug=np.array(redkit_drug)\n  \n    \n#   !gdown --id 1FMpKkkmWiTLE_tgk2rMlhwMbKvnvZjyC\n  !gdown --id 1Yf9YQRSq3Bf0bIwncpoANS5Ui1H3W_fH\n  graph=pd.read_excel('graph_emb1.xlsx',header=None)\n  graph=np.array(graph)\n  \n    \n  !gdown --id 1HQpJQ6yeZYnGK6Uy0AyIvzYsNkzBAeOD\n  model = tf.keras.models.load_model(\"model.h5\")\n\n  !gdown --id 1AZX2h806qMcMp63hnJB81JFYMjzEChpA\n  unique_drugs1=pd.read_excel('unique394_drugl.xlsx', header=None)\n  unique_drugs1=np.array(unique_drugs1)\n\n#   !gdown --id 1wZ654LoQlQQAbVegLbgMK326Pft5l-PT\n#   unique_drugs1=pd.read_excel('drug_feature614.xlsx', header=None)\n#   unique_drugs1=np.array(unique_drugs1)\n\n  return smiles[1:,1],data_to_repeat[1:,:],unique_drugs[1:,0],feature_cell,redkit_drug,model,graph,unique_drugs1\n\n\n\n\n\ndef graph_node(smiles):\n  node=[]\n  adj=[]\n  max=0\n  final_feature=[]\n  final_adjacency=[]\n  for i  in range(len(smiles)):\n     m=smiles[i]\n    #  aa=m#.item((0))\n#      featurizer = MolGraphConvFeaturizer(use_edges=True,use_chirality=True)\n     featurizer = PagtnMolGraphFeaturizer()\n     out = featurizer.featurize(m)\n     feature=out[0].node_features\n        \n#      featurizer = DMPNNFeaturizer()\n#      out = featurizer.featurize(m)\n#      feature=out[0].node_features\n    \n     ma=feature.shape[0]\n     n=feature.shape[1]\n     if(ma>max):\n       max=ma\n    # out[0].edge_featuwres.shape\n     molecules=read_smiles(m)\n     adjacency1=nx.to_numpy_array(molecules)\n     node.append(feature)\n     adj.append(adjacency1)\n#   f_n=np.zeros((max,n))\n#   f_a=np.zeros((max,max))\n  feature=[]\n  adjacency=[]\n  for i  in range(len(smiles)):\n    n1=node[i]\n    a=adj[i]\n    f_n=np.zeros((max,n))\n    f_a=np.zeros((max,max))\n    f_n[0:n1.shape[0],]=n1\n    f_a[0:n1.shape[0],0:n1.shape[0]]=a\n    final_feature.append(f_n)\n    final_adjacency.append(f_a)\n\n\n  return final_feature,final_adjacency \n\n\ndef atom_features(atom):\n    return np.array(one_of_k_encoding_unk(atom.GetSymbol(),\n                                          ['C', 'N', 'O', 'S', 'F', 'Si', 'P', 'Cl', 'Br', 'Mg', 'Na', 'Ca', 'Fe', 'As',\n                                           'Al', 'I', 'B', 'V', 'K', 'Tl', 'Yb', 'Sb', 'Sn', 'Ag', 'Pd', 'Co', 'Se',\n                                           'Ti', 'Zn', 'H', 'Li', 'Ge', 'Cu', 'Au', 'Ni', 'Cd', 'In', 'Mn', 'Zr', 'Cr',\n                                           'Pt', 'Hg', 'Pb', 'Unknown']) +\n                    one_of_k_encoding(atom.GetDegree(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    one_of_k_encoding_unk(atom.GetTotalNumHs(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    one_of_k_encoding_unk(atom.GetExplicitValence(), [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) +\n                    [atom.IsInRing()]+[atom.GetIsAromatic()])\n\n\ndef one_of_k_encoding(x, allowable_set):\n    if x not in allowable_set:\n        raise Exception(\"input {0} not in allowable set{1}:\".format(x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))\n\n\ndef one_of_k_encoding_unk(x, allowable_set):\n    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\n\n\ndef smile_to_graph(smile):\n    mol = Chem.MolFromSmiles(smile)\n\n    c_size = mol.GetNumAtoms()\n\n    features = []\n    for atom in mol.GetAtoms():\n        feature = atom_features(atom)\n        features.append(feature)# / sum(feature))\n\n    edges = []\n    for bond in mol.GetBonds():\n        edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n    g = nx.Graph(edges).to_directed()\n    edge_index = []\n    for e1, e2 in g.edges:\n        edge_index.append([e1, e2])\n\n    return c_size, features, edge_index\n\n\n\ndef graph_node_edge(smiles):\n  node=[]\n  adj=[]\n  max=0\n  max1=0\n  final_feature=[]\n  final_adjacency=[]\n  for i  in range(len(smiles)):\n     m=smiles[i]\n    #  aa=m#.item((0))\n    #  featurizer = MolGraphConvFeaturizer(use_edges=True,use_chirality=True)\n    #  out = featurizer.featurize(m)\n    #  feature=out[0].node_features\n     num_atom,feature,edge_index=smile_to_graph(m)  \n#      featurizer = DMPNNFeaturizer()\n#      out = featurizer.featurize(m)\n#      feature=out[0].node_features\n     feature=np.stack( feature, axis=0)\n     edge_index=np.stack( edge_index, axis=0)\n    \n     ma=num_atom\n     n=feature.shape[1]\n     if(ma>max):\n       max=ma\n    # out[0].edge_featuwres.shape\n     node.append(feature)\n\n     \n     adj.append(edge_index)\n#   f_n=np.zeros((max,n))\n#   f_a=np.zeros((max,max))\n  feature=[]\n  adjacency=[]\n  for i  in range(len(smiles)):\n    n1=node[i]\n    a=adj[i]\n    f_n=np.zeros((max,n))\n    f_n[0:n1.shape[0],]=n1\n    f_a=np.zeros((max,max))\n    for j in range(len(a)):\n        a1=a[j]\n        f_a[a1[0],a1[1]]=1\n        f_a[a1[1],a1[0]]=1\n#     f_a[0:a.shape[0],]=a\n    final_feature.append(f_n)\n    final_adjacency.append(f_a)\n\n  return final_feature,final_adjacency\n\n\ndef fil_walk(a):\n \n # Continue walking until you reach the required distance. \n#  xvalues=[]\n#  yvalues=[]\n#  xvalues.append(34)\n#  yvalues.append(34)\n for i in range(a.shape[0]):\n    for j in range(a.shape[1]):\n    \n         r=random.choice([0, 0.01,0.02,0.03])\n # Choose a route and how far you want to go in that way.\n     \n\n     # discard walks that go nowhere.\n#      if x_move == 0 and y_move == 0:\n#        continue\n\n     # determine the succeeding x and y co-ordinates.\n     \n         \n         if(a[i,j] != 1):\n             a[i,j]=a[i,j]+r\n         else:\n            a[i,j]=a[i,j]\n return a\n\ndef random_walkall(a):\n    x=[]\n    for i in range (len(a)):\n        b=a[i]\n        x.append(fil_walk(b))\n    return x\n\n\n\ndef row_normalize(A):\n    '''\n    Perform row-normalization of the given matrix\n    inputs\n        A : crs_matrix\n            (n x n) input matrix where n is # of nodes\n    outputs\n        nA : crs_matrix\n             (n x n) row-normalized matrix\n    '''\n    n = A.shape[0]\n\n    # do row-wise sum where d is out-degree for each node\n    d = A.sum(axis=1)\n    d = np.asarray(d).flatten()\n\n    # handle 0 entries in d\n    d = np.maximum(d, np.ones(n))\n    invd = 1.0 / d\n\n    invD = spdiags(invd, 0, n, n)\n\n    # compute row normalized adjacency matrix by nA = invD * A\n    nA = invD.dot(A)\n\n    return nA\ndef normalize_adj(a):\n    b=[]\n    for i in range(a.shape[0]):\n        h=row_normalize(a[0])\n        b.append(h)\n        \n    return b\n\n\ndef repeat_smiles1(data_to_repeat,unique_drugs,feature,adjacency,unique_cell,redkit_drug,graph):\n  unique_redkit_feature=redkit_drug[:,1:]\n  unique_redkit_name=redkit_drug[:,0]\n  unique_feature=unique_cell[:,1:]\n  unique_name=unique_cell[:,0]\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  c=data_to_repeat[:,2]\n  unique_drugs=feature[:,0]\n  feature=feature[:,1:]  \n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  feature_cell=[]\n  redkit_d1=[]\n  redkit_d2=[]\n  graph1=[]\n  graph2=[]\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    cc=c[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    cc1=[m for m, v in enumerate(unique_name) if cc in v]\n    r1= [m for m, v in enumerate(unique_redkit_name) if n1 in v]\n    r2=[m for m, v in enumerate(unique_redkit_name) if n2 in v]\n    f_drug1.append(feature[k1[0]])\n    f_drug2.append(feature[k2[0]])\n    a_drug1.append(adjacency[k1[0]])\n    a_drug2.append(adjacency[k2[0]])\n    graph1.append(graph[k1[0]])\n    graph2.append(graph[k2[0]])\n    feature_cell.append(unique_feature[cc1[0]])\n    redkit_d1.append(unique_redkit_feature[r1[0]])\n    redkit_d2.append(unique_redkit_feature[r2[0]])\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,feature_cell,redkit_d1,redkit_d2,graph1,graph2\n\ndef repeat_smiles_deep(data_to_repeat,unique_drugs,feature,adjacency,unique_cell):\n  unique_feature=unique_cell[:,1:]\n  unique_name=unique_cell[:,0]\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  c=data_to_repeat[:,2]\n  f_drug1=[]\n  f_drug2=[]\n  a_drug1=[]\n  a_drug2=[]\n  feature_cell=[]\n  f=np.zeros((feature[0].shape[0],feature[0].shape[1]))\n  a=np.zeros((adjacency[0].shape[0],adjacency[0].shape[1]))\n  cell=np.zeros((unique_cell[0].shape[0]))\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    cc=c[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    cc1=[m for m, v in enumerate(unique_name) if cc in v]\n    if(cc1):\n        f_drug1.append(feature[k1[0]])\n        f_drug2.append(feature[k2[0]])\n        a_drug1.append(adjacency[k1[0]])\n        a_drug2.append(adjacency[k2[0]])\n        feature_cell.append(unique_feature[cc1[0]])\n    else:\n        f_drug1.append(f)\n        f_drug2.append(f)\n        a_drug1.append(a)\n        a_drug2.append(a)\n        feature_cell.append(cell)\n\n  return f_drug1,f_drug2,a_drug1,a_drug2,feature_cell\n\n\ndef train_test_input(f_drug1,a_drug1,f_drug2,a_drug2,cell_line,index_train,index_test,synery,class1,redkit_d1,redkit_d2,graph1,graph2):\n  train_f_drug1=[]\n  train_a_drug1=[]\n  train_f_drug2=[]\n  train_a_drug2=[]\n  train_cell_line=[]\n  train_synergy=[]\n  train_class=[]\n  train_redkit_d1=[]\n  train_redkit_d2=[]\n  test_f_drug1=[]\n  test_a_drug1=[]\n  test_f_drug2=[]\n  test_a_drug2=[]\n  test_cell_line=[]\n  test_synergy=[]\n  test_class=[]\n  test_redkit_d1=[]\n  test_redkit_d2=[]\n  train_graph1=[]\n  test_graph1=[]\n  train_graph2=[]\n  test_graph2=[]\n  for i in range(len(index_train)):\n      train_a_drug1.append(a_drug1[index_train[i]])\n      train_a_drug2.append(a_drug2[index_train[i]])\n      train_f_drug1.append(f_drug1[index_train[i]])\n      train_f_drug2.append(f_drug2[index_train[i]])\n      train_cell_line.append(cell_line[index_train[i]])\n      train_synergy.append(synergy[index_train[i]])\n      train_class.append(class1[index_train[i]])\n      train_redkit_d1.append(redkit_d1[index_train[i]])\n      train_redkit_d2.append(redkit_d2[index_train[i]])\n      train_graph1.append(graph1[index_train[i]])\n      train_graph2.append(graph2[index_train[i]])\n\n  for ii in range(len(index_test)):\n      test_a_drug1.append(a_drug1[index_test[ii]])\n      test_a_drug2.append(a_drug2[index_test[ii]])\n      test_f_drug1.append(f_drug1[index_test[ii]])\n      test_f_drug2.append(f_drug2[index_test[ii]])\n      test_cell_line.append(cell_line[index_test[ii]])\n      test_synergy.append(synergy[index_test[ii]])\n      test_class.append(class1[index_test[ii]])\n      test_redkit_d1.append(redkit_d1[index_test[ii]])\n      test_redkit_d2.append(redkit_d2[index_test[ii]])\n      test_graph1.append(graph1[index_test[ii]])\n      test_graph2.append(graph2[index_test[ii]])\n\n  return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_class,test_synergy,test_class,train_redkit_d1,train_redkit_d2,test_redkit_d1,test_redkit_d2,train_graph1,test_graph1,train_graph2,test_graph2\n\n\n\ndef preprocess(index_train,index_test):\n    index_train1=[]\n    index_test1=[]\n    index_train2=(index_train)[0]\n    index_test2=(index_test)[0]\n    for i in range(len((index_train2))):\n        index_train1.append((index_train2[i]))\n        \n    for ii in range(len(index_test2)):\n        index_test1.append((index_test2[ii]))\n        \n    return index_train1,index_test1\n\ndef get_data_me2(s):\n    \n    \n\n    !gdown 1C7Z2ziPdQVzH3omIdIfyJa7VUmog4IIk\n    labels = pd.read_csv('oneil.csv', index_col=0) \n    \n    h=len(np.array(labels))\n    #labels are duplicated for the two different ways of ordering in the data\n    labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['fold']!=test_fold)\n    \n\n    idx_test = np.where(labels['fold']==test_fold)\n#     \n#    \n    return idx_train,idx_test\n\n\ndef get_data_me1(s):\n    \n\n    !gdown --id 1lLKI0xPEbTBlkpu_6F3yoSYqf7pazg8d\n    dele=pd.read_excel('deleted_index.xls',header=None)\n    dele=np.array(dele)\n    dele=dele-1\n    \n    \n#     !gdown http://www.bioinf.jku.at/software/DeepSynergy/data_test_fold0_tanh.p.gz\n#     file = gzip.open(data_file, 'rb')\n#     X_tr, X_val, X_train, X_test, y_tr, y_val, y_train, y_test = pickle.load(file)\n#     file.close()\n    \n\n    !gdown 1SvLGjU---dvwsuCTqRMehtV-Gmf2Hk52\n    labels = pd.read_csv('pcbi.1006752.s004.csv', index_col=0) \n    \n    h=len(np.array(labels))\n    #labels are duplicated for the two different ways of ordering in the data\n#     labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['Fold']!=test_fold)\n    \n#     h=len(idx_train[0])\n    xx=np.where(idx_train == dele)[1]\n    idx_train=np.delete(idx_train,xx)\n    xx1=np.where(idx_train == (dele+h))[1]\n    idx_train=np.delete(idx_train,xx1)\n#     idx_train=idx_train[!dele]\n    #indices of test data for model testing: fold 0\n    idx_test = np.where(labels['Fold']==test_fold)\n#     h1=len(idx_test[0])\n    yy=np.where(idx_test == dele)[1]\n    idx_test=np.delete(idx_test,yy)\n    yy1=np.where(idx_test == (dele+h))[1]\n    idx_test=np.delete(idx_test,yy1)\n    \n#     X_train = X[idx_train]\n#     X_test = X[idx_test]\n    idx_train1=idx_train + (idx_train.shape)\n    idx_test1=idx_test + (idx_test.shape)\n    \n    idx_train1=idx_train + (h)\n    idx_test1=idx_test + (h)\n    idx_train1=np.r_[idx_train,idx_train1]\n    idx_test1=np.r_[idx_test,idx_test1]\n    #choose idx_train1 and idx_test1 if you want to duplicate data\n    return idx_train1,idx_test1\n\ndef get_data_val(t,v):\n   \n    !gdown 1C7Z2ziPdQVzH3omIdIfyJa7VUmog4IIk\n    labels = pd.read_csv('oneil.csv', index_col=0) \n\n    #labels are duplicated for the two different ways of ordering in the data\n    labels = pd.concat([labels, labels]) \n    \n    test_fold =t\n    vali_fold =v\n    idx_train = np.where((labels['fold']!=test_fold) & (labels['fold']!=vali_fold))\n    \n#     h=len(idx_train[0])\n    \n#     idx_train=idx_train[!dele]\n    #indices of test data for model testing: fold 0\n    idx_val = np.where(labels['fold']==vali_fold)\n#     h1=len(idx_test[0])\n    \n#     X_train = X[idx_train]\n#     X_test = X[idx_test]\n\n    y_train = labels.iloc[idx_train]['synergy'].values\n    y_test = labels.iloc[idx_val]['synergy'].values\n    c_train=labels.iloc[idx_train]['class'].values\n    c_test=labels.iloc[idx_val]['class'].values\n \n    return y_train,y_test,idx_train,idx_val,c_train,c_test\n\n\ndef get_data_me(s):\n   \n    !gdown 1mu7dislo1zEd9trKw_mhm46Zaa3XGA7x\n    labels = pd.read_csv('synergy.csv', index_col=0)\n\n    #labels are duplicated for the two different ways of ordering in the data\n    labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['fold']!=test_fold)\n    \n#     h=len(idx_train[0])\n    \n#     idx_train=idx_train[!dele]\n    #indices of test data for model testing: fold 0\n    idx_test = np.where(labels['fold']==test_fold)\n#     h1=len(idx_test[0])\n    \n#     X_train = X[idx_train]\n#     X_test = X[idx_test]\n\n    y_train = labels.iloc[idx_train]['score'].values\n    y_test = labels.iloc[idx_test]['score'].values\n    c_train=labels.iloc[idx_train]['class'].values\n    c_test=labels.iloc[idx_test]['class'].values\n \n    return y_train,y_test,idx_train,idx_test,c_train,c_test\n\n\ndef convert_tobin(cc):\n    cb=[]\n    for i in range(len(cc)):\n        if(cc[i]>=0.5):\n            cb.append(1)\n        else:\n            cb.append(0)\n    return cb\n\n\ndef norm1(train_cell_line,test_cell_line,norm=\"tanh_norm\"):\n# norm = \"norm\"\n    if norm == \"tanh_norm\":\n        train_cell_line, mean, std, mean2, std2, feat_filt = normalize1(train_cell_line, norm=norm)\n        test_cell_line, mean, std, mean2, std2, feat_filt = normalize1(test_cell_line, mean, std, mean2, std2, \n                                                              feat_filt=feat_filt, norm=norm)\n    else:\n        train_cell_line, mean, std, feat_filt = normalize1(train_cell_line, norm=norm)\n        test_cell_line, mean, std, feat_filt = normalize1(test_cell_line, mean, std, feat_filt=feat_filt, norm=norm)\n    \n    return train_cell_line,test_cell_line\n\n\ndef call_validate(test,val):\n    y_train,y_test,index_train,index_test,c_train,c_test=get_data_val(test,val)\n\n    index_train,index_test=preprocess(index_train,index_test) \n    train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_class,test_synergy,test_class,train_redkit_d1,train_redkit_d2,test_redkit_d1,test_redkit_d2=train_test_input(f_drug1,a_drug1,f_drug2,a_drug2,feature_cell,index_train,index_test,synergy,class1,redkit_d1,redkit_d2)\n    train_f_drug1=np.array(train_f_drug1).astype(float)\n    test_f_drug1=np.array(test_f_drug1).astype(float) \n    train_f_drug2=np.array(train_f_drug2).astype(float)\n    test_f_drug2=np.array(test_f_drug2).astype(float)\n    train_cell_line=np.array(train_cell_line).astype(float)\n    test_cell_line=np.array(test_cell_line).astype(float)  \n    train_a_drug1=np.array(train_a_drug1).astype(float)\n    test_a_drug1=np.array(test_a_drug1).astype(float) \n    train_a_drug2=np.array(train_a_drug2).astype(float)\n    test_a_drug2=np.array(test_a_drug2).astype(float) \n    train_synergy=np.array(train_synergy)\n    train_class=np.array(train_class)\n    test_synergy=np.array(test_synergy)\n    test_class=np.array(test_class)\n    test_redkit_d1=np.array(test_redkit_d1).astype(float)\n    test_redkit_d2=np.array(test_redkit_d2).astype(float)\n    \n    \n    \n    return train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_class,test_synergy,test_class,train_redkit_d1,train_redkit_d2,test_redkit_d1,test_redkit_d2\n    \n\nsmiles,data_to_repeat,unique_drugs,unique_cell,redkit_drug,ddi_model,graph,feature=get_data()\n# smiles,data_to_repeat,unique_drugs,unique_cell,deleted_index=get_data()\n# unique_cell[:,1:]=2**unique_cell[:,1:]-1\n\n# feature,adjacency=graph_node(smiles)\nfeature1,adjacency=graph_node_edge(smiles)\n# adjacency=normalize_adj(np.array(adjacency))\nadjacency=random_walkall(adjacency)\n# feature,edge_index=graph_node_edge(smiles)\ndata_to_repeat=np.r_[data_to_repeat,data_to_repeat]\nl=int((data_to_repeat.shape[0])/2)\ndata_to_repeat[l:,0]=data_to_repeat[0:l,1]\ndata_to_repeat[l:,1]=data_to_repeat[0:l,0]\n# np.random.shuffle(data_to_repeat)\nsynergy=data_to_repeat[:,3]\n# class1=data_to_repeat[:,4]\nclass1=[]\nfor i in range(len(synergy)):\n if(synergy[i]>=30):\n    class1.append(1)\n elif(synergy[i]<0): \n    class1.append(0)\n else:\n    class1.append(2)    \n\n# synergy=data_to_repeat[:,3]\n# class=data_to_repeat[:,5]\n\n# f_drug1,f_drug2,a_drug1,a_drug2,synergy,class=repeat_smiles(data_to_repeat,unique_drugs,feature,adjacency,deleted_index,synergy,class)\n\nf_drug1,f_drug2,a_drug1,a_drug2,feature_cell,redkit_d1,redkit_d2,graph1,graph2=repeat_smiles1(data_to_repeat,unique_drugs,feature,adjacency,unique_cell,redkit_drug,graph)\n# f_drug1,f_drug2,a_drug1,a_drug2,feature_cell=repeat_smiles_deep(data_to_repeat,unique_drugs,feature,adjacency,unique_cell)\n\n# a_drug2=normalize_adj(np.array(a_drug2))\n# a_drug1=normalize_adj(np.array(a_drug1))\n\ntest_fold=0\n# y_train,y_test,index_train,index_test,c_train,c_test=get_data_me(test_fold)\nindex_train,index_test=get_data_me2(test_fold)\n\nindex_train,index_test=preprocess(index_train,index_test)\ntrain_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,train_synergy,train_class,test_synergy,test_class,train_redkit_d1,train_redkit_d2,test_redkit_d1,test_redkit_d2,train_graph1,test_graph1,train_graph2,test_graph2=train_test_input(f_drug1,a_drug1,f_drug2,a_drug2,feature_cell,index_train,index_test,synergy,class1,redkit_d1,redkit_d2,graph1,graph2)\ntrain_cell_line=np.array(train_cell_line).astype(float)\ntest_cell_line=np.array(test_cell_line).astype(float)  \ntrain_f_drug1=np.array(train_f_drug1).astype(float)\ntest_f_drug1=np.array(test_f_drug1).astype(float)\ntrain_f_drug2=np.array(train_f_drug2).astype(float)\ntest_f_drug2=np.array(test_f_drug2).astype(float)\n# train_synergy=np.array(train_synergy)\n# train_class=np.array(train_class)\n# test_synergy=np.array(test_synergy)\n# test_class=np.array(test_class)\n# test_redkit_d1=np.array(test_redkit_d1).astype(float)\n# test_redkit_d2=np.array(test_redkit_d2).astype(float)\ntrain_graph1=np.array(train_graph1).astype(float)\ntest_graph1=np.array(test_graph1).astype(float) \ntrain_graph2=np.array(train_graph2).astype(float)\ntest_graph2=np.array(test_graph2).astype(float)\n\ntrain_cell_line,test_cell_line=norm1(train_cell_line,test_cell_line)\n\ntrain_f_drug1,test_f_drug1=norm1(train_f_drug1,test_f_drug1)\ntrain_f_drug2,test_f_drug2=norm1(train_f_drug2,test_f_drug2)\n\ntrain_graph1,test_graph1=norm1(train_graph1,test_graph1)\ntrain_graph2,test_graph2=norm1(train_graph2,test_graph2)\n\n\n# train_cell_line.shape\n# test_cell_line.shape\n# feature_cell.shape\n# test_a_drug1.shape\n# train_f_drug1.shape\n# train_a_drug2.shape\nprint(train_cell_line.shape)  \nprint(test_cell_line.shape)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T18:39:17.689568Z","iopub.execute_input":"2023-07-10T18:39:17.689946Z","iopub.status.idle":"2023-07-10T18:42:45.836144Z","shell.execute_reply.started":"2023-07-10T18:39:17.689917Z","shell.execute_reply":"2023-07-10T18:42:45.834754Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting deepchem\n  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.2.0)\nRequirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.23.5)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.5.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.2.2)\nCollecting scipy<1.9 (from deepchem)\n  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting rdkit (from deepchem)\n  Downloading rdkit-2023.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2023.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rdkit->deepchem) (9.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->deepchem) (3.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\nInstalling collected packages: scipy, rdkit, deepchem\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.10.1\n    Uninstalling scipy-1.10.1:\n      Successfully uninstalled scipy-1.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 23.4.1 requires cupy-cuda11x<12.0.0a0,>=9.5.0, which is not installed.\ncuml 23.4.1 requires dask==2023.3.2, but you have dask 2023.5.0 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed deepchem-2.7.1 rdkit-2023.3.2 scipy-1.8.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting pysmiles\n  Downloading pysmiles-1.0.2-py2.py3-none-any.whl (22 kB)\nCollecting pbr (from pysmiles)\n  Downloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting networkx~=2.0 (from pysmiles)\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pbr, networkx, pysmiles\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.1\n    Uninstalling networkx-3.1:\n      Successfully uninstalled networkx-3.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed networkx-2.8.8 pbr-5.11.1 pysmiles-1.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (3.1.2)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"Collecting PubChemPy\n  Downloading PubChemPy-1.0.4.tar.gz (29 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: PubChemPy\n  Building wheel for PubChemPy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PubChemPy: filename=PubChemPy-1.0.4-py3-none-any.whl size=13835 sha256=06d41722c893df8d4f48075f61cc353907bfdecadc28aa4d92a867ba1f422db7\n  Stored in directory: /root/.cache/pip/wheels/90/7c/45/18a0671e3c3316966ef7ed9ad2b3f3300a7e41d3421a44e799\nSuccessfully built PubChemPy\nInstalling collected packages: PubChemPy\nSuccessfully installed PubChemPy-1.0.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting PyDrive\n  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /opt/conda/lib/python3.10/site-packages (from PyDrive) (2.86.0)\nRequirement already satisfied: oauth2client>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from PyDrive) (4.1.3)\nRequirement already satisfied: PyYAML>=3.0 in /opt/conda/lib/python3.10/site-packages (from PyDrive) (5.4.1)\nRequirement already satisfied: httplib2<1dev,>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (0.21.0)\nRequirement already satisfied: google-auth<3.0.0dev,>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (2.17.3)\nRequirement already satisfied: google-auth-httplib2>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (0.1.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (1.33.2)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=4.0.0->PyDrive) (0.2.7)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=4.0.0->PyDrive) (4.9)\nRequirement already satisfied: six>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=4.0.0->PyDrive) (1.16.0)\nRequirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.57.1)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.20.3)\nRequirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.28.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=1.19.0->google-api-python-client>=1.2->PyDrive) (4.2.4)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client>=1.2->PyDrive) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2023.5.7)\nBuilding wheels for collected packages: PyDrive\n  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PyDrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27453 sha256=fec30674bd6a058a4941a609b6b8b7c428d56193e6086defecb918a6284754e7\n  Stored in directory: /root/.cache/pip/wheels/63/79/df/924c22c080c9dac1a57f611baa837fe0bc3daec1500b27f23b\nSuccessfully built PyDrive\nInstalling collected packages: PyDrive\nSuccessfully installed PyDrive-1.3.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting xlrd==2.0.1\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: xlrd\nSuccessfully installed xlrd-2.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting Pandas==1.3.5\n  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from Pandas==1.3.5) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from Pandas==1.3.5) (2023.3)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from Pandas==1.3.5) (1.23.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.3->Pandas==1.3.5) (1.16.0)\nInstalling collected packages: Pandas\n  Attempting uninstall: Pandas\n    Found existing installation: pandas 1.5.3\n    Uninstalling pandas-1.5.3:\n      Successfully uninstalled pandas-1.5.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.4.1 requires cupy-cuda11x<12.0.0a0,>=9.5.0, which is not installed.\ncuml 23.4.1 requires cupy-cuda11x<12.0.0a0,>=9.5.0, which is not installed.\ndask-cudf 23.4.1 requires cupy-cuda11x<12.0.0a0,>=9.5.0, which is not installed.\nbeatrix-jupyterlab 2023.58.190319 requires jupyter-server~=1.16, but you have jupyter-server 2.5.0 which is incompatible.\ncudf 23.4.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.4.1 requires dask==2023.3.2, but you have dask 2023.5.0 which is incompatible.\ndask-cuda 23.4.0 requires dask==2023.3.2, but you have dask 2023.5.0 which is incompatible.\ndask-cudf 23.4.1 requires dask==2023.3.2, but you have dask 2023.5.0 which is incompatible.\nfeaturetools 1.26.0 requires pandas<2.0.0,>=1.5.0, but you have pandas 1.3.5 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.8.1 which is incompatible.\nraft-dask 23.4.1 requires dask==2023.3.2, but you have dask 2023.5.0 which is incompatible.\nwoodwork 0.23.0 requires pandas<2.0.0,>=1.4.3, but you have pandas 1.3.5 which is incompatible.\nxarray 2023.5.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Pandas-1.3.5\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting gdown\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.0)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.28.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.64.1)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.7.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","output_type":"stream"},{"name":"stdout","text":"hjjbjh\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (5.4.1)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (3.8.0)\nRequirement already satisfied: numpy>=1.14.5 in /opt/conda/lib/python3.10/site-packages (from h5py) (1.23.5)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 59.2MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=10ztxKtGSVU7p9yPoCsTnapVLThWpjjYT\nTo: /kaggle/working/oneil.xlsx\n100%|█████████████████████████████████████████| 850k/850k [00:00<00:00, 120MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 62.8MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1rVt2qEH-LMzk86ig6c8Qjll-08LhsxBv\nTo: /kaggle/working/cell_expression875.xlsx\n100%|█████████████████████████████████████████| 303k/303k [00:00<00:00, 111MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=14upx46iIPcO80y_tcvbaS3uwMkZfKLVw\nTo: /kaggle/working/redkit_drug.xlsx\n100%|██████████████████████████████████████| 68.6k/68.6k [00:00<00:00, 67.4MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1Yf9YQRSq3Bf0bIwncpoANS5Ui1H3W_fH\nTo: /kaggle/working/graph_emb1.xlsx\n100%|██████████████████████████████████████| 64.8k/64.8k [00:00<00:00, 82.0MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (uriginal): https://drive.google.com/uc?id=1HQpJQ6yeZYnGK6Uy0AyIvzYsNkzBAeOD\nFrom (redirected): https://drive.google.com/uc?id=1HQpJQ6yeZYnGK6Uy0AyIvzYsNkzBAeOD&confirm=t&uuid=c65af9d5-cc7d-4c2b-a126-6283dc60ff69\nTo: /kaggle/working/model.h5\n100%|████████████████████████████████████████| 567M/567M [00:08<00:00, 66.5MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1AZX2h806qMcMp63hnJB81JFYMjzEChpA\nTo: /kaggle/working/unique394_drugl.xlsx\n100%|████████████████████████████████████████| 162k/162k [00:00<00:00, 99.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1C7Z2ziPdQVzH3omIdIfyJa7VUmog4IIk\nTo: /kaggle/working/oneil.csv\n100%|█████████████████████████████████████████| 988k/988k [00:00<00:00, 116MB/s]\n(36426, 875)\n(9048, 875)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow\nfrom networkx.readwrite.graph6 import data_to_n\nfrom tensorflow.python.training.tracking import data_structures\n!pip install spektral\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Minimum,Maximum,Add,Maximum,PReLU, Flatten,Reshape,Dropout, Input,Dense,Add,concatenate,BatchNormalization, Activation#,MultiHeadAttention,AdditiveAttention\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\n# from tensorflow.random import set_seed\nfrom spektral.data.loaders import SingleLoader\nfrom spektral.datasets.citation import Citation\nfrom spektral.layers import GATConv, GlobalAvgPool,GlobalMaxPool, GCNConv,GlobalAttentionPool\nfrom spektral.transforms import LayerPreprocess\n# import tensorflow.compat.v1.keras.backend as K\nfrom sklearn.metrics import confusion_matrix, mean_squared_error,mean_absolute_error,r2_score#,AUC\nfrom scipy.stats import pearsonr,spearmanr\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.metrics import roc_curve,auc,accuracy_score,precision_score,cohen_kappa_score,precision_recall_curve,average_precision_score,roc_auc_score\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras import regularizers\nfrom keras.constraints import max_norm\nnp.random.seed(10)\n\ntrain_cell_line=tf.convert_to_tensor(train_cell_line)\ntest_cell_line=tf.convert_to_tensor(test_cell_line)\ntrain_a_drug1=tf.convert_to_tensor(train_a_drug1)\ntest_a_drug1=tf.convert_to_tensor(test_a_drug1) \ntrain_a_drug2=tf.convert_to_tensor(train_a_drug2)\ntest_a_drug2=tf.convert_to_tensor(test_a_drug2)\ntrain_synergy=tf.convert_to_tensor(train_synergy)\ntrain_class=tf.convert_to_tensor(train_class)\ntest_synergy=tf.convert_to_tensor(test_synergy)\ntest_class=tf.convert_to_tensor(test_class)\ntest_redkit_d1=tf.convert_to_tensor(test_redkit_d1)\ntest_redkit_d2=tf.convert_to_tensor(test_redkit_d2)\ntrain_f_drug1=tf.convert_to_tensor(train_f_drug1)\ntest_f_drug1=tf.convert_to_tensor(test_f_drug1)\ntrain_f_drug2=tf.convert_to_tensor(train_f_drug2)\ntest_f_drug2=tf.convert_to_tensor(test_f_drug2)\ntrain_graph1=tf.convert_to_tensor(train_graph1)\ntest_graph1=tf.convert_to_tensor(test_graph1)\ntrain_graph2=tf.convert_to_tensor(train_graph2)\ntest_graph2=tf.convert_to_tensor(test_graph2)\n\n# train_cell_line=np.array(train_cell_line)\n# test_cell_line=np.array(test_cell_line)\n# train_a_drug1=np.array(train_a_drug1)\n# test_a_drug1=np.array(test_a_drug1) \n# train_a_drug2=np.array(train_a_drug2)\n# test_a_drug2=np.array(test_a_drug2)\n# train_synergy=np.array(train_synergy)\n# train_class=np.array(train_class)\n# test_synergy=np.array(test_synergy)\n# test_class=np.array(test_class)\n# test_redkit_d1=np.array(test_redkit_d1)\n# test_redkit_d2=np.array(test_redkit_d2)\n# train_f_drug1=np.array(train_f_drug1)\n# test_f_drug1=np.array(test_f_drug1)\n# train_f_drug2=np.array(train_f_drug2)\n# test_f_drug2=np.array(test_f_drug2)\n\n\ndef ddi_fun(train_redkit_d1,train_redkit_d2):\n    train_redkit_d1=np.array(train_redkit_d1).astype(float)\n    train_redkit_d2=np.array(train_redkit_d2).astype(float)\n    ddi_extractor = keras.Model(\n    inputs=ddi_model.inputs,\n    outputs=ddi_model.get_layer(name=\"features\").output,\n    )\n\n    train_ddi=ddi_extractor.predict([train_redkit_d1,train_redkit_d2])\n# test_ddi=ddi_extractor.predict([test_redkit_d1,test_redkit_d2])\n    return train_ddi\n\ntest_ddi=ddi_fun(test_redkit_d1,test_redkit_d2)\ntrain_ddi=ddi_fun(train_redkit_d1,train_redkit_d2)\ntrain_ddi=np.array(train_ddi).astype(float)\ntest_ddi=np.array(test_ddi).astype(float)\n# # train_synergy\n# test_synergy\n# train_synergy.shape\n# test_synergy.shape\n# train_class\n# train_ddi.shape[1]\n# train_ddi=tf.convert_to_tensor(train_ddi)\n# test_ddi=tf.convert_to_tensor(test_ddi)\n\ntrain_ddi,test_ddi=norm1(train_ddi,test_ddi)\n# norm = \"tanh_norm\"\n# if norm == \"tanh_norm\":\n#     train_ddi, mean, std, mean2, std2, feat_filt = normalize1(train_ddi, norm=norm)\n#     test_ddi, mean, std, mean2, std2, feat_filt = normalize1(test_ddi, mean, std, mean2, std2, \n#                                                           feat_filt=feat_filt, norm=norm)\n# else:\n#     train_ddi, mean, std, feat_filt = normalize1(train_ddi, norm=norm)\n#     test_ddi, mean, std, feat_filt = normalize1(test_ddi, mean, std, feat_filt=feat_filt, norm=norm)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T18:42:45.839725Z","iopub.execute_input":"2023-07-10T18:42:45.840095Z","iopub.status.idle":"2023-07-10T18:44:32.350737Z","shell.execute_reply.started":"2023-07-10T18:42:45.840068Z","shell.execute_reply":"2023-07-10T18:44:32.349531Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.3.3)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.8.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.10)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.0)\nRequirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.5.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.31.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (0.1.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.8.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.28.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting spektral\n  Downloading spektral-1.3.0-py3-none-any.whl (140 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from spektral) (1.2.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from spektral) (4.9.2)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from spektral) (2.8.8)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from spektral) (1.23.5)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from spektral) (1.3.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from spektral) (2.28.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from spektral) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from spektral) (1.8.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from spektral) (4.64.1)\nRequirement already satisfied: tensorflow>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from spektral) (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (23.3.3)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (3.8.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.4.10)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (16.0.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (4.5.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.31.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from pandas->spektral) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas->spektral) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (2023.5.7)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->spektral) (3.1.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow>=2.2.0->spektral) (0.1.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (2.17.3)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (3.4.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (0.7.0)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (2.3.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow>=2.2.0->spektral) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (2.1.2)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (3.2.2)\nInstalling collected packages: spektral\nSuccessfully installed spektral-1.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n283/283 [==============================] - 3s 2ms/step\n1139/1139 [==============================] - 2s 2ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# import tensorflow_datasets as tfds\nimport tensorflow as tf\ndef scaled_dot_product_attention(q, k, v, mask=None):\n  \"\"\"Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead)\n  but it must be broadcastable for addition.\n\n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable\n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n  Returns:\n    output, attention_weights\n  \"\"\"\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)\n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits)#, axis=-1)#tf.nn.linear()#, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output,attention_weights \n\n#multi-head\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self,*, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n\n    assert d_model % self.num_heads == 0\n\n    self.depth = d_model // self.num_heads\n\n    self.wq = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wk = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wv = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n\n    self.dense = tf.keras.layers.Dense(d_model)#,activation='relu',use_bias='true')\n\n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n\n  def call(self, v, k, q, mask=None):\n    batch_size = tf.shape(q)[0]\n\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n#     q=PReLU()(q)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n#     k=PReLU()(k)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n#     v=PReLU()(v)\n\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention,\n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n    return output#, attention_weights\n\n\n\nclass CrossStitch(tf.keras.layers.Layer):\n\n    \"\"\"Cross-Stitch implementation according to arXiv:1604.03539\n    Implementation adapted from https://github.com/helloyide/Cross-stitch-Networks-for-Multi-task-Learning\"\"\"\n\n    def __init__(self, num_tasks, *args, **kwargs):\n        \"\"\"initialize class variables\"\"\"\n        self.num_tasks = num_tasks\n        super(CrossStitch, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        \"\"\"initialize the kernel and set the instance to 'built'\"\"\"\n        self.kernel = self.add_weight(name=\"kernel\",\n                                      shape=(self.num_tasks,\n                                             self.num_tasks),\n                                      initializer='identity',\n                                      trainable=True)\n        super(CrossStitch, self).build(input_shape)\n\n    def call(self, xl):\n        \"\"\"\n        called by TensorFlow when the model gets build. \n        Returns a stacked tensor with num_tasks channels in the 0 dimension, \n        which need to be unstacked.\n        \"\"\"\n        if (len(xl) != self.num_tasks):\n            # should not happen\n            raise ValueError()\n\n        out_values = []\n        for this_task in range(self.num_tasks):\n            this_weight = self.kernel[this_task, this_task]\n            out = tf.math.scalar_mul(this_weight, xl[this_task])\n            for other_task in range(self.num_tasks):\n                if this_task == other_task:\n                    continue  # already weighted!\n                other_weight = self.kernel[this_task, other_task]\n#                 out += tf.math.scalar_mul(other_weight, xl[other_task])\n            out_values.append(out)\n        # HACK!\n        # unless we stack, and then unstack the tensors, TF (2.0.0) can't follow\n        # the graph, so it aborts during model initialization.\n        # return tf.stack(out_values, axis=0)\n        return out_values[0],out_values[1]\n\n    def compute_output_shape(self, input_shape):\n        return [self.num_tasks] + input_shape\n\n    def get_config(self):\n        \"\"\"implemented so keras can save the model to json/yml\"\"\"\n        config = {\n            \"num_tasks\": self.num_tasks\n        }\n        base_config = super(CrossStitch, self).get_config()\n        return dict(list(config.items()) + list(base_config.items()))\n\n    \n \ndef calculate(x1, x2, v):\n\n \n\n#   matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n#   # scale matmul_qk\n#   dk = tf.cast(tf.shape(k)[-1], tf.float32)\n#   scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if v =='min':\n    out=tf.math.minimum(x1,x2)\n    \n  if v =='max':\n    out=tf.math.maximum(x1,x2)\n    \n  if v =='avg':\n    out=tf.math.reduce_mean(x1,x2)\n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  \n  return output \n\nclass rough_neuron(tf.keras.layers.Layer):\n  def __init__(self,input_shape,*args, **kwargs):\n#     super(rough_neuron, self).__init__()\n\n\n#   def build(self, input_shape):  \n\n    self.w1 = tf.keras.layers.Dense(input_shape,activation='relu',use_bias='true',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))\n    self.w2 = tf.keras.layers.Dense(input_shape,activation='relu',use_bias='true',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))\n    self.min=tf.keras.layers.Minimum()\n    self.max=tf.keras.layers.Maximum()\n    self.avg = tf.keras.layers.Average()\n    \n#     super(rough_neuron, self).build(input_shape)\n\n    super(rough_neuron, self).__init__()\n    \n  def build(self, input_shape):\n        self.a = self.add_weight(name=\"a\",trainable=True)\n        self.b = self.add_weight(name=\"b\",trainable=True)#initializer='zero',shape=(1)\n        super(rough_neuron, self).build(input_shape)\n    \n#     self.wv = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n\n#     self.dense = tf.keras.layers.Dense(d_model)#,activation='relu',use_bias='true')\n\n\n  def call(self, x):\n#     batch_size = tf.shape(q)[0]\n\n    x1 = self.w1(x)  # (batch_size, seq_len, d_model)\n#     q=PReLU()(q)\n    x2 = self.w2(x)  # (batch_size, seq_len, d_model)\n    \n    y1= self.min([x1,x2])\n    y2= self.min([x1,x2])\n    \n#     y=self.avg([y1,y2])\n#     y=self.sum([y1,y2])\n    y= tf.math.scalar_mul(self.a,y1)+tf.math.scalar_mul((1-self.a),y2)\n    \n#     y1= calculate(x1,x2,'min')\n#     y2= calculate(x1,x2,'max')\n    \n#     y=calculate(y1,y2,'avg')\n\n   \n\n    \n    \n    \n\n    return y#, attention_weights\n\nimport keras.backend as K\nfrom keras.optimizers import Adam\n\nclass AdamW(Adam):\n    def __init__(self, learning_rate=0.001, weight_decay=0.025, **kwargs):\n        super(AdamW, self).__init__(learning_rate, **kwargs)\n        self.weight_decay = K.variable(weight_decay, name='weight_decay')\n\n    def get_updates(self, loss, params):\n        # Apply weight decay to the parameters\n        decay_updates = [\n            K.update_add(param, -self.weight_decay * param)\n            for param in params\n            if param.name.endswith('kernel:0')  # Apply weight decay only to kernel weights\n        ]\n\n        # Call the parent get_updates() method to get the remaining updates\n        updates = super(AdamW, self).get_updates(loss, params)\n\n        # Combine the weight decay updates and other updates\n        updates.extend(decay_updates)\n\n        return updates\n\n    \nimport keras.backend as K\nfrom keras.optimizers import Optimizer\n\nclass AMSGrad(Optimizer):\n    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, **kwargs):\n        super(AMSGrad, self).__init__(**kwargs)\n        self.learning_rate = learning_rate\n        self.beta_1 = beta_1\n        self.beta_2 = beta_2\n        self.epsilon = epsilon\n        self.iterations = K.variable(0)\n        self.m = None\n        self.v = None\n\ndef get_updates(self, loss, params):\n    grads = self.get_gradients(loss, params)\n    self.updates = [K.update_add(self.iterations, 1)]\n\n    t = self.iterations + 1\n    lr_t = self.learning_rate * (K.sqrt(1.0 - K.pow(self.beta_2, t)) / (1.0 - K.pow(self.beta_1, t)))\n\n    shapes = [K.int_shape(p) for p in params]\n    self.m = [K.zeros(shape) for shape in shapes]\n    self.v = [K.zeros(shape) for shape in shapes]\n\n    for p, g, m, v in zip(params, grads, self.m, self.v):\n        m_t = (self.beta_1 * m) + (1.0 - self.beta_1) * g\n        v_t = (self.beta_2 * v) + (1.0 - self.beta_2) * K.square(g)\n        v_hat = K.maximum(v, v_t)  # AMSGrad modification\n\n        p_t = p - lr_t * m_t / (K.sqrt(v_hat) + self.epsilon)\n\n        self.updates.append(K.update(m, m_t))\n        self.updates.append(K.update(v, v_t))\n        self.updates.append(K.update(p, p_t))\n\n    return self.updates\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T18:44:32.352842Z","iopub.execute_input":"2023-07-10T18:44:32.353237Z","iopub.status.idle":"2023-07-10T18:44:32.397196Z","shell.execute_reply.started":"2023-07-10T18:44:32.353201Z","shell.execute_reply":"2023-07-10T18:44:32.396018Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\ndef generate_network_att1(x_in1,a_in1,x_in2,a_in2,cell, inDrop, drop,ddi,graph1,graph2):\n    # fill the architecture params from dict\n#     gcn_layers = [1024,512,156]\n#     gan_layers = [1024,512]\n#     cell_layers = [2048,512]#,2048]\n#     snp_layers = [1024,512,128]#,2048]\n#     dsn1_layers = [1024,2048,1024]\n#     dsn2_layers = [1024,2048,1024]\n    # fill the architecture params from dict\n#     gcn_layers = [78,156,312]\n    gcn_layers = [94,94*2,94*4]\n#     gcn_layers = [128,256,512]\n#     gan_layers = [128,128]\n#     gcn_layers = [32,64,128]\n    cell_layers = [512,265,128]#64for gcn\n#     cell_layers = [2048,512,128] #for gan\n    snp_layers = [512,128]#for gcn\n#     snp_layers = [2048,512,256]# for gan\n    ddi_layers=[1024,512,256]\n    layers=[256,512,256]\n    g_layers=[258,128]\n\n    dsn1_layers = [1024,512,256]\n    dsn2_layers = [1024,512,256]\n   \n    \n    N = x_in1[0].shape[0]\n#     F = x_in1[0].shape[1]\n#     N = 69\n#     F = 32\n    \n   \n    \n    channels = 8  # Number of channels in each head of the first GAT layer\n    n_attn_heads = 8  # Number of attention heads in first GAT layer\n    dropout = 0.2 # Dropout rate for the features and adjacency matrix\n    drop=0.2\n    l2_reg = 1e-3  # L2 regularization rate\n    \n    \n    # contruct two parallel networks\n#     x_in1 = Input(shape=(x_in1.shape[1],),name='x_in1')\n#     a_in1 = Input((N,N,),name='a_in1')\n    \n\n    snp_layers = [2048,1024,2048]\n    # contruct two parallel networks\n    for l in range(len(dsn1_layers)):\n        if l == 0:\n            x_in1    = Input(shape=(x_in1.shape[1],),name=\"input1\")\n            middle_layer = Dense(int(dsn1_layers[l]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(x_in1)\n            middle_layer = Dropout(float(inDrop))(middle_layer) \n        elif l == (len(dsn1_layers)-1):\n            dsn1_output = Dense(int(dsn1_layers[l]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(middle_layer)\n        else:\n            middle_layer = Dense(int(dsn1_layers[l]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(middle_layer)\n            middle_layer = Dropout(float(drop))(middle_layer)\n\n    \n    \n#     gan_output1 = Dense(128, activation='relu')(gan_output1)\n    \n    concatModel1 =  dsn1_output\n    # # addtModel = Add([gcn_output, gan_output])\n#     x_in2 = Input(shape=(N,F,),name='x_in2')\n#     a_in2 = Input((N,N,),name='a_in2')\n    \n    for l in range(len(dsn2_layers)):\n        if l == 0:\n            x_in2    = Input(shape=(x_in2.shape[1],),name=\"input2\")\n            middle_layer = Dense(int(dsn2_layers[l]), activation='relu', use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(x_in2)\n            middle_layer = Dropout(float(inDrop))(middle_layer)\n        elif l == (len(dsn2_layers)-1):\n            dsn2_output = Dense(int(dsn2_layers[l]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(middle_layer)\n        else:\n            middle_layer = Dense(int(dsn2_layers[l]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(middle_layer)\n            middle_layer = Dropout(float(drop))(middle_layer)\n    \n    concatModel2 = dsn2_output\n    \n    # concatModel2 = Add()([gcn_output2, gan_output2])\n    for cell_layer in range(len(cell_layers)):\n      if cell_layer == 0:\n        input_cell    = Input(shape=(cell[0].shape[0],))  #\n#         input_cell1 = BatchNormalization()(input_cell)\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(input_cell)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=rough_neuron(int(cell_layers[cell_layer]))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      elif cell_layer == (len(cell_layers)-1):\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n#         cellFC=rough_neuron(int(cell_layers[cell_layer]))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n      #  snp_output = Dense(1, activation='linear')(snpFC)\n      else:\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n        cellFC = Dropout(float(drop))(cellFC)\n#         cellFC=rough_neuron(int(cell_layers[cell_layer]))(cellFC)\n#         cellFC=BatchNormalization()(cellFC)\n    # print(concatModel1.shape)\n#     print(cellFC.shape)\n#     concatModeld=concatenate([concatModel1,concatModel2])\n#     mlayer =MultiHeadAttention(d_model=cellFC.shape[1], num_heads=4)\n#     cellFC= mlayer(cellFC,concatModeld,concatModeld)\n#     cellFC = Reshape([cellFC.shape[2]])(cellFC)\n\n\n    graph_in1=Input(shape=(graph1.shape[1],),name='graph1')\n\n    for g_layer in range(len(g_layers)):\n      if g_layer == 0:\n\n        graph_out1 = Dense(int(g_layers[g_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_in1)\n        graph_out1 = Dropout(float(drop))(graph_out1)\n#         graph_out1=rough_neuron(int(g_layers[g_layer]))(graph_out1)\n        \n      elif g_layer == (len(g_layers)-1):\n        graph_out1 = Dense(int(g_layers[g_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_out1)\n#         graph_out1=rough_neuron(int(g_layers[g_layer]))(graph_out1)\n\n      else:\n        graph_out1 = Dense(int(g_layers[g_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_out1)\n        graph_out1 = Dropout(float(drop))(graph_out1)\n#         graph_out1=rough_neuron(int(g_layers[g_layer]))(graph_out1)\n    \n    \n    graph_in2=Input(shape=(graph2.shape[1],),name='graph2')\n\n    for g_layer in range(len(g_layers)):\n      if g_layer == 0:\n\n        graph_out2 = Dense(int(g_layers[g_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_in2)\n        graph_out2 = Dropout(float(drop))(graph_out2)\n#         graph_out2=rough_neuron(int(g_layers[g_layer]))(graph_out2)\n        \n      elif g_layer == (len(g_layers)-1):\n        graph_out2 = Dense(int(g_layers[g_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_out2)\n                                \n#         graph_out2=rough_neuron(int(g_layers[g_layer]))(graph_out2)\n\n      else:\n        graph_out2 = Dense(int(g_layers[g_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_out2)\n        graph_out2 = Dropout(float(drop))(graph_out2)\n#         graph_out2=rough_neuron(int(g_layers[g_layer]))(graph_out2)\n        \n    \n    concatModel1=concatenate([concatModel1,graph_out1])\n    concatModel2=concatenate([concatModel2,graph_out2])\n    \n    \n    for layer in range(len(layers)):\n      if layer == 0:\n\n        concatModel1 = Dense(int(layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel1)\n        concatModel1 = Dropout(float(drop))(concatModel1)\n#         concatModel1=rough_neuron(int(layers[layer]))(concatModel1)\n        \n      elif layer == (len(layers)-1):\n        concatModel1 = Dense(int(layers[layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel1)\n                                  \n#         concatModel1=rough_neuron(int(layers[layer]))(concatModel1)\n\n      else:\n        concatModel1 = Dense(int(layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel1)\n        concatModel1 = Dropout(float(drop))(concatModel1)\n#         concatModel1=rough_neuron(int(layers[layer]))(concatModel1)\n    \n    \n    for layer in range(len(layers)):\n      if layer == 0:\n\n        concatModel2 = Dense(int(layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel2)\n        concatModel2 = Dropout(float(drop))(concatModel2)\n#         concatModel2=rough_neuron(int(layers[layer]))(concatModel2)\n        \n      elif layer == (len(layers)-1):\n        concatModel2 = Dense(int(layers[layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel2)\n#         concatModel2=rough_neuron(int(layers[layer]))(concatModel2)\n\n      else:\n        concatModel2 = Dense(int(layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel2)\n        concatModel2 = Dropout(float(drop))(concatModel2)\n#         concatModel2=rough_neuron(int(layers[layer]))(concatModel2)\n    \n    \n    \n    ddi_out1=Input(shape=(ddi.shape[1],),name='ddi')\n# #     ddi_out=BatchNormalization()(ddi_out1)\n       \n\n#     ddi_out1 = Dense(1024, activation='linear')(ddi_out)\n    for ddi_layer in range(len(ddi_layers)):\n      if ddi_layer == 0:\n\n        ddi_out = Dense(int(ddi_layers[ddi_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(ddi_out1)\n        ddi_out = Dropout(float(drop))(ddi_out)\n#         ddi_out=rough_neuron(int(ddi_layers[ddi_layer]))(ddi_out)\n        \n      elif ddi_layer == (len(ddi_layers)-1):\n        ddi_out = Dense(int(ddi_layers[ddi_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(ddi_out)\n#         ddi_out=rough_neuron(int(ddi_layers[ddi_layer]))(ddi_out)\n\n      else:\n        ddi_out = Dense(int(ddi_layers[ddi_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(ddi_out)\n        ddi_out = Dropout(float(drop))(ddi_out)\n#         ddi_out=rough_neuron(int(ddi_layers[ddi_layer]))(ddi_out)\n    \n#     concatModel1=concatenate([concatModeld,cellFC])\n#     concatModel1 = Dense(int(256), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n#                      activity_regularizer=regularizers.L2(0.001))(concatModel1)\n    \n#     concatModel2=concatenate([ddi_out,cellFC])\n#     concatModel2 = Dense(int(256), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n#                      activity_regularizer=regularizers.L2(0.001))(concatModel2)\n    \n#     concatModel=concatenate([concatModel2,concatModel1])#,cellFC])\n    \n    mlayer =MultiHeadAttention(d_model=cellFC.shape[1], num_heads=4)\n    cellFC1= mlayer(cellFC,ddi_out,ddi_out)\n    cellFC1 = Reshape([cellFC1.shape[2]])(cellFC1)\n    cellFC=concatenate([cellFC1,cellFC])\n#     cellFC = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n#                      activity_regularizer=regularizers.L2(0.001))(cellFC)\n    \n    concatModel=concatenate([concatModel2,concatModel1,cellFC])\n    concatModel=BatchNormalization()(concatModel)\n    \n        \n   \n#    \n    layer1 =MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task1= layer1(concatModel,concatModel,concatModel)\n    layer2 = MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task2= layer2(concatModel,concatModel,concatModel)\n    task11 = Reshape([a_task1.shape[2]])(a_task1)\n    task22 = Reshape([a_task2.shape[2]])(a_task2)\n    task1=concatenate([task11,concatModel])\n    task2=concatenate([task22,concatModel])\n#     task2=PReLU()(task2)\n#     task1=BatchNormalization()(task1)\n#     task2=BatchNormalization()(task2)\n    r_task1,r_task2 = CrossStitch(2)([task1,task2])\n\n    \n    \n    r_task1=Dense(2048,activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    \n    r_task2=Dense(2048, activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n                             \n#     r_task1=rough_neuron(2048)(r_task1)\n#     r_task2=rough_neuron(2048)(r_task2)\n                             \n    r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n    \n#     layer1 =MultiHeadAttention(d_model=r_task1.shape[1], num_heads=4)\n#     a_task1= layer1(r_task1,r_task1,r_task1)\n#     layer2 = MultiHeadAttention(d_model=r_task2.shape[1], num_heads=4)\n#     a_task2= layer2(r_task2,r_task2,r_task2)\n#     task11 = Reshape([a_task1.shape[2]])(a_task1)\n#     task22 = Reshape([a_task2.shape[2]])(a_task2)\n#     r_task1=concatenate([task11,r_task1])\n#     r_task2=concatenate([task22,r_task2])\n    \n    r_task1=concatenate([r_task1,task1])\n    r_task2=concatenate([r_task2,task2])\n    \n#     r_task1=BatchNormalization()(r_task1)\n#     r_task2=BatchNormalization()(r_task2)\n\n    \n    \n    \n    \n    r_task1 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fsynergy1')(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(128, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fclass1')(r_task2)\n#     r_task2=PReLU()(r_task2)\n#     r_task1=rough_neuron(128)(r_task1)\n#     r_task2=rough_neuron(128)(r_task2)\n    \n   \n    r_task1 = Dense(64, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fsynergy2')(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(64, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fclass2')(r_task2)\n#   \n\n#     r_task1=rough_neuron(64)(r_task1)\n#     r_task2=rough_neuron(64)(r_task2)\n    \n    snp_output1 = Dense(1, activation='linear',name='synergy')(r_task1)\n#     snp_output1=PReLU()(snp_output1)\n    snp_output2 = Dense(3, activation='sigmoid',name='class')(r_task2)\n    \n#     snp_output1 = Dense(1, activation='linear',name='synergy')(snpFC1)\n# #     snp_output1=PReLU()(snp_output1)\n#     snp_output2 = Dense(1, activation='sigmoid',name='class')(snpFC2)\n#     model = Model(inputs=[x_in1,a_in1,x_in2,a_in2,input_cell],outputs= [f_task1,f_task2])\n    model = Model(inputs=[x_in1,x_in2,input_cell,ddi_out1,graph_in1,graph_in2],outputs= [snp_output1,snp_output2])\n    # tf.keras.utils.plot_model(model.build_graph(), to_file=\"model.png\",\n    #        expand_nested=True, show_shapes=True)\n    print(model.summary())\n    return model\n\n# def trainer_att1(model, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_class,train_ddi, epo, batch_size, earlyStop,val_f_drug1,val_a_drug1,val_f_drug2,val_a_drug2,val_cell_line,val_ddi,val_synergy,val_class):\n#     model.compile(loss={'synergy':'mse','class':'BinaryCrossentropy'}, optimizer=keras.optimizers.Adam(lr=float(l_rate)))#, beta_1=0.9, beta_2=0.999, amsgrad=True))\n# #     model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=float(l_rate), momentum=0.9, nesterov=True, clipnorm=1.))\n#     checkpoint_path = \"training_1\" #    checkpoint_path = \"training_1\" #\n\n#     checkpoint_dir = os.path.dirname(checkpoint_path)\n# #     filepath = 'my_best_model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5'\n#     checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n#                              monitor='val_loss',\n#                              verbose=1, \n#                              save_best_only=True,\n#                              save_weights_only=True,\n#                              mode='min')\n#     es = EarlyStopping(monitor='val_loss', patience=100)\n#     callbacks = [checkpoint ,es]\n#     history = model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_ddi],\n#                   [train_synergy,train_class],\n#                   shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 ,\n#                   validation_data=([val_f_drug1,val_a_drug1,val_f_drug2,val_a_drug2,val_cell_line,val_ddi],[val_synergy,val_class]),\n#                   callbacks=callbacks)\n        \n                    \n#     model.load_weights(checkpoint_path)\n#     loss=history.history['val_loss']\n# #     model.load_model(checkpoint_path)\n# #     model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_ddi], [train_synergy,train_class], shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 )#,restore_best_weights=True,validation_split=0.1) \n#                   #  validation_data=([val_input1,val_input2], [val_synergy,val_class]))\n#     # model.fit({\"input1\":train_drug1, \"input2\":train_drug2}, {\"synergy\":train_synergy,\"class\":train_class}, epochs=epo, shuffle=True, batch_size=batch_size,verbose=1) \n#     #                validation_data=({\"input1\":val_input1,\"input2\":val_input2}, {\"synergy\":val_synergy,\"class\":val_class}))\n\n#     return model,loss\n\n# def trainer_att1(model, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_class,train_ddi, epo, batch_size, earlyStop):\n#     model.compile(loss={'synergy':'mse','class':'BinaryCrossentropy'}, optimizer=keras.optimizers.Adam(lr=float(l_rate)))#, beta_1=0.9, beta_2=0.999, amsgrad=True))\n# #     model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=float(l_rate), momentum=0.9, nesterov=True, clipnorm=1.))\n#     checkpoint_path = \"training_1\" #    checkpoint_path = \"training_1\" #\n\n#     checkpoint_dir = os.path.dirname(checkpoint_path)\n# #     filepath = 'my_best_model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5'\n#     checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n#                              monitor='val_loss',\n#                              verbose=1, \n#                              save_best_only=True,\n#                              save_weights_only=True,\n#                              mode='min')\n#     es = EarlyStopping(monitor='val_loss', patience=50)\n#     callbacks = [checkpoint ,es]\n#     history = model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_ddi],\n#                   [train_synergy,train_class],\n#                   shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 ,\n#                   validation_split=0.1,\n#                   callbacks=callbacks)\n        \n                    \n#     model.load_weights(checkpoint_path)\n#     loss=history.history['val_loss']\n# #     model.load_model(checkpoint_path)\n# #     model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_ddi], [train_synergy,train_class], shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 )#,restore_best_weights=True,validation_split=0.1) \n#                   #  validation_data=([val_input1,val_input2], [val_synergy,val_class]))\n#     # model.fit({\"input1\":train_drug1, \"input2\":train_drug2}, {\"synergy\":train_synergy,\"class\":train_class}, epochs=epo, shuffle=True, batch_size=batch_size,verbose=1) \n#     #                validation_data=({\"input1\":val_input1,\"input2\":val_input2}, {\"synergy\":val_synergy,\"class\":val_class}))\n\n#     return model,loss\ndef trainer_att1(model, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_class,train_ddi,train_graph1,train_grpah2, epo, batch_size, earlyStop):\n#     model.compile(loss={'synergy':'mse','class':'BinaryCrossentropy'}, optimizer=keras.optimizers.Adam(lr=float(l_rate)))#, beta_1=0.9, beta_2=0.999, amsgrad=True))\n#     model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=float(l_rate), momentum=0.9, nesterov=True, clipnorm=1.))\n    optimizer = AdamW(learning_rate=0.00005, weight_decay=0.025)\n#     optimizer1 = AMSGrad(learning_rate=0.001)\n#     model.compile(optimizer=optimizer,loss={'synergy':'mse','class':'BinaryCrossentropy'})\n    model.compile(optimizer=optimizer,loss={'synergy':'mse','class':'categorical_crossentropy'})\n\n    model.fit([train_f_drug1,train_f_drug2,train_cell_line,train_ddi,train_graph1,train_graph2],[train_synergy,train_class],shuffle=True, epochs=epo, batch_size=batch_size,verbose=1) \n                  \n\n    return model\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-10T18:44:32.398877Z","iopub.execute_input":"2023-07-10T18:44:32.399250Z","iopub.status.idle":"2023-07-10T18:44:32.463498Z","shell.execute_reply.started":"2023-07-10T18:44:32.399219Z","shell.execute_reply":"2023-07-10T18:44:32.462601Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\n# def trainer_att1(model, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_class,train_ddi, epo, batch_size, earlyStop,val_f_drug1,val_a_drug1,val_f_drug2,val_a_drug2,val_cell_line,val_ddi,val_synergy,val_class):\n#     model.compile(loss={'synergy':'mse','class':'BinaryCrossentropy'}, optimizer=keras.optimizers.Adam(lr=float(l_rate)))#, beta_1=0.9, beta_2=0.999, amsgrad=True))\n# #     model.compile(loss='mse', optimizer=keras.optimizers.SGD(lr=float(l_rate), momentum=0.9, nesterov=True, clipnorm=1.))\n#     checkpoint_path = \"training_1\" #    checkpoint_path = \"training_1\" #\n\n#     checkpoint_dir = os.path.dirname(checkpoint_path)\n# #     filepath = 'my_best_model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5'\n#     checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n#                              monitor='val_loss',\n#                              verbose=1, \n#                              save_best_only=True,\n#                              save_weights_only=True,\n#                              mode='min')\n#     es = EarlyStopping(monitor='val_synergy_loss'+'val_class_loss', patience=100)\n#     callbacks = [checkpoint ,es,logs: print(model.layers[0].get_weights())]\n#     history = model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_ddi],\n#                   [train_synergy,train_class],\n#                   shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 ,\n#                   validation_data=([val_f_drug1,val_a_drug1,val_f_drug2,val_a_drug2,val_cell_line,val_ddi],[val_synergy,val_class]),\n#                   callbacks=callbacks)#\n        \n                    \n#     model.load_weights(checkpoint_path)\n#     loss=history.history['val_loss']\n# #     model.load_model(checkpoint_path)\n# #     model.fit([train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_ddi], [train_synergy,train_class], shuffle=True, epochs=epo, batch_size=batch_size,verbose=1 )#,restore_best_weights=True,validation_split=0.1) \n#                   #  validation_data=([val_input1,val_input2], [val_synergy,val_class]))\n#     # model.fit({\"input1\":train_drug1, \"input2\":train_drug2}, {\"synergy\":train_synergy,\"class\":train_class}, epochs=epo, shuffle=True, batch_size=batch_size,verbose=1) \n#     #                validation_data=({\"input1\":val_input1,\"input2\":val_input2}, {\"synergy\":val_synergy,\"class\":val_class}))\n\n#     return model,loss","metadata":{"execution":{"iopub.status.busy":"2023-07-10T18:44:32.467442Z","iopub.execute_input":"2023-07-10T18:44:32.467740Z","iopub.status.idle":"2023-07-10T18:44:32.473807Z","shell.execute_reply.started":"2023-07-10T18:44:32.467693Z","shell.execute_reply":"2023-07-10T18:44:32.472844Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# l_rate = 0.0001\n# inDrop = 0.2\n# drop = 0.2\n# max_epoch = 1000\n# # batch_size = 128 #gcn\n# batch_size = 64 #gan\n# earlyStop_patience = 20\n\n# # losses=[483.10247802734375]\n\n# # !gdown --id 15oJ8oD6ClS7_VZm6wUGQM-5xNaZ7grSb\n# # model_att1.load_weights(\"modc.h5\")\n\n\n\n# # model_at=[]\n\n# i=1   \n# if(i!=test_fold):\n#     print(i)\n#     train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,val_f_drug1,val_a_drug1,val_f_drug2,val_a_drug2,val_cell_line,train_synergy,train_class,val_synergy,val_class,train_redkit_d1,train_redkit_d2,val_redkit_d1,val_redkit_d2=call_validate(test_fold,i)\n#     train_ddi=ddi_fun(train_redkit_d1,train_redkit_d2)\n#     val_ddi=ddi_fun(val_redkit_d1,val_redkit_d2)\n#     train_ddi,val_ddi=norm1(train_ddi,val_ddi)\n#     train_cell_line,val_cell_line=norm1(train_cell_line,val_cell_line)\n    \n#     model_att1= generate_network_att1(train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line, inDrop, drop,train_ddi)\n#     !gdown  14I1OvQmH7l7yxDQsyQzOUvDRfGVZQ6uq\n#     model_att1.load_weights(\"model30.h5\")\n#     model_att1,loss=trainer_att1(model_att1, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_class,train_ddi,max_epoch, batch_size,\n#                                 earlyStop_patience,val_f_drug1,val_a_drug1,val_f_drug2,val_a_drug2,val_cell_line,val_ddi,val_synergy,val_class)\n# #         losses.append(loss)\n# #         model_at.append(model_att1)\n        \n#     model_att1.save_weights(\"model31.h5\")\n# #         print(loss)\n\n\n        \n# # m=losses.index(min(losses))\n# # model_at.append(mod1)\n# # model_att=model_at[4]","metadata":{"execution":{"iopub.status.busy":"2023-07-10T18:44:32.475241Z","iopub.execute_input":"2023-07-10T18:44:32.475892Z","iopub.status.idle":"2023-07-10T18:44:32.489223Z","shell.execute_reply.started":"2023-07-10T18:44:32.475859Z","shell.execute_reply":"2023-07-10T18:44:32.488288Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#n\n\nl_rate = 0.0001\ninDrop = 0.2\ndrop = 0.2   \nmax_epoch =1000\n# batch_size = 128 #gcn\nbatch_size = 64 #gan\nearlyStop_patience = 20\n\n\n\nmodel_att1= generate_network_att1(train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line, inDrop, drop,train_ddi,train_graph1,train_graph2)\n# model_att1=trainer_att1(model_att1, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_class,train_ddi,train_graph1,train_graph2,max_epoch, batch_size,\n#                                 earlyStop_patience)\ntrain_class1 = keras.utils.to_categorical(train_class, num_classes=3)\nmodel_att1=trainer_att1(model_att1, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_class1,train_ddi,train_graph1,train_graph2,max_epoch, batch_size,\n                                earlyStop_patience)\n\n# model_att1,loss=trainer_att1(model_att1, l_rate, train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line,train_synergy,train_class,train_ddi,max_epoch, batch_size,\n#                                 earlyStop_patience,test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,test_ddi,test_synergy,test_class)\n# p1,p2= predict(model, [test_input1,test_input2])\nap111,ap221= model_att1.predict( [test_f_drug1,test_f_drug2,test_cell_line,test_ddi,test_graph1,test_graph2])\n\npositive_negative_indices = np.where(test_class != 2)\nap221=ap221[positive_negative_indices]\ntest_class12=np.array(test_class)\ntest_class12=test_class12[positive_negative_indices]\nap221=ap221[:,1]\n\n\n# train_class1 = train_class1.ravel()\n# ap221 = ap221.ravel()\n# ap221=np.argmax(ap221, axis=1)\n\nap11=[]\ntest_synergy1=[]\nl=len(ap111)\nl1=int(l/2)\nfor i in range(l1) :\n    ap=(ap111[i]+ap111[i+l1])/2\n    ap11.append(ap)\n    aap=(test_synergy[i]+test_synergy[i+l1])/2\n    test_synergy1.append(aap)\n    \n    \nap22=[]\ntest_class1=[]\nl=len(ap221)\nl1=int(l/2)\nfor i in range(l1) :\n    ap=(ap221[i]+ap221[i+l1])/2\n    ap22.append(ap)\n    aap=(test_class12[i]+test_class12[i+l1])/2\n    test_class1.append(aap)\n    \n    \n    \nasynergy_error1=mean_squared_error(test_synergy1, ap11)\nasynergy_error11=mean_absolute_error(test_synergy1, ap11)\nasynergy_error21=r2_score(test_synergy1, ap11)\n\nprint(\"msynergy_mean_squared_error\",asynergy_error1)\nprint(\"mclass_mean_squared_error\",asynergy_error11)\nprint(\"msynergy_r2_score\",asynergy_error21)\n\nasynergy_pear1= pearsonr(test_synergy1, ap11)\nasynergy_spear1= spearmanr(test_synergy1, ap11)\nprint(\"msynergy_pear\",asynergy_pear1)\nprint(\"msynergy_spear\",asynergy_spear1)\n\naap2=convert_tobin(ap22)\n# aap2 = np.argmax(ap22, axis=1)\n# test_class11 = keras.utils.to_categorical(test_class1, num_classes=3)\n# aclass_error1=roc_curve(auc(test_class1, ap22))\naclass_error1=roc_auc_score(test_class1, ap22)\naclass_error11=accuracy_score(test_class1, aap2)\naclass_error21=cohen_kappa_score(test_class1, aap2)\n\nprint(\"msclass_roc_curve\",aclass_error1)\nprint(\"mclass_accuracy_scorer\",aclass_error11)\nprint(\"mclass_cohen_kappa_score\",aclass_error21)\n\n# cross_att1 = model_att1.evaluate([test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,test_ddi],[test_synergy,test_class])\n# print(cross_att1)\n\naclass_pear1= precision_score(test_class1, aap2)\naclass_spear1= average_precision_score(test_class1, ap22)\nprint(\"mclass_precision_score\",aclass_pear1)\nprint(\"mclass_average_precision_score\",aclass_spear1)\n# ap111,ap221.shape\n# yourTerminal:prompt> jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10 \n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T21:35:59.731636Z","iopub.execute_input":"2023-07-10T21:35:59.732245Z","iopub.status.idle":"2023-07-11T00:26:25.018144Z","shell.execute_reply.started":"2023-07-10T21:35:59.732201Z","shell.execute_reply":"2023-07-11T00:26:25.017150Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Model: \"model_3\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input2 (InputLayer)            [(None, 394)]        0           []                               \n                                                                                                  \n input1 (InputLayer)            [(None, 394)]        0           []                               \n                                                                                                  \n dense_39 (Dense)               (None, 1024)         404480      ['input2[0][0]']                 \n                                                                                                  \n dense_36 (Dense)               (None, 1024)         404480      ['input1[0][0]']                 \n                                                                                                  \n dropout_16 (Dropout)           (None, 1024)         0           ['dense_39[0][0]']               \n                                                                                                  \n graph2 (InputLayer)            [(None, 455)]        0           []                               \n                                                                                                  \n dropout_14 (Dropout)           (None, 1024)         0           ['dense_36[0][0]']               \n                                                                                                  \n graph1 (InputLayer)            [(None, 455)]        0           []                               \n                                                                                                  \n dense_40 (Dense)               (None, 512)          524800      ['dropout_16[0][0]']             \n                                                                                                  \n dense_47 (Dense)               (None, 258)          117648      ['graph2[0][0]']                 \n                                                                                                  \n dense_37 (Dense)               (None, 512)          524800      ['dropout_14[0][0]']             \n                                                                                                  \n dense_45 (Dense)               (None, 258)          117648      ['graph1[0][0]']                 \n                                                                                                  \n input_2 (InputLayer)           [(None, 875)]        0           []                               \n                                                                                                  \n ddi (InputLayer)               [(None, 2048)]       0           []                               \n                                                                                                  \n dropout_17 (Dropout)           (None, 512)          0           ['dense_40[0][0]']               \n                                                                                                  \n dropout_21 (Dropout)           (None, 258)          0           ['dense_47[0][0]']               \n                                                                                                  \n dropout_15 (Dropout)           (None, 512)          0           ['dense_37[0][0]']               \n                                                                                                  \n dropout_20 (Dropout)           (None, 258)          0           ['dense_45[0][0]']               \n                                                                                                  \n dense_42 (Dense)               (None, 512)          448512      ['input_2[0][0]']                \n                                                                                                  \n dense_55 (Dense)               (None, 1024)         2098176     ['ddi[0][0]']                    \n                                                                                                  \n dense_41 (Dense)               (None, 256)          131328      ['dropout_17[0][0]']             \n                                                                                                  \n dense_48 (Dense)               (None, 128)          33152       ['dropout_21[0][0]']             \n                                                                                                  \n dense_38 (Dense)               (None, 256)          131328      ['dropout_15[0][0]']             \n                                                                                                  \n dense_46 (Dense)               (None, 128)          33152       ['dropout_20[0][0]']             \n                                                                                                  \n dropout_18 (Dropout)           (None, 512)          0           ['dense_42[0][0]']               \n                                                                                                  \n dropout_26 (Dropout)           (None, 1024)         0           ['dense_55[0][0]']               \n                                                                                                  \n concatenate_9 (Concatenate)    (None, 384)          0           ['dense_41[0][0]',               \n                                                                  'dense_48[0][0]']               \n                                                                                                  \n concatenate_8 (Concatenate)    (None, 384)          0           ['dense_38[0][0]',               \n                                                                  'dense_46[0][0]']               \n                                                                                                  \n dense_43 (Dense)               (None, 265)          135945      ['dropout_18[0][0]']             \n                                                                                                  \n dense_56 (Dense)               (None, 512)          524800      ['dropout_26[0][0]']             \n                                                                                                  \n dense_52 (Dense)               (None, 256)          98560       ['concatenate_9[0][0]']          \n                                                                                                  \n dense_49 (Dense)               (None, 256)          98560       ['concatenate_8[0][0]']          \n                                                                                                  \n dropout_19 (Dropout)           (None, 265)          0           ['dense_43[0][0]']               \n                                                                                                  \n dropout_27 (Dropout)           (None, 512)          0           ['dense_56[0][0]']               \n                                                                                                  \n dropout_24 (Dropout)           (None, 256)          0           ['dense_52[0][0]']               \n                                                                                                  \n dropout_22 (Dropout)           (None, 256)          0           ['dense_49[0][0]']               \n                                                                                                  \n dense_44 (Dense)               (None, 128)          34048       ['dropout_19[0][0]']             \n                                                                                                  \n dense_57 (Dense)               (None, 256)          131328      ['dropout_27[0][0]']             \n                                                                                                  \n dense_53 (Dense)               (None, 512)          131584      ['dropout_24[0][0]']             \n                                                                                                  \n dense_50 (Dense)               (None, 512)          131584      ['dropout_22[0][0]']             \n                                                                                                  \n multi_head_attention_3 (MultiH  (None, None, 128)   98816       ['dense_44[0][0]',               \n eadAttention)                                                    'dense_57[0][0]',               \n                                                                  'dense_57[0][0]']               \n                                                                                                  \n dropout_25 (Dropout)           (None, 512)          0           ['dense_53[0][0]']               \n                                                                                                  \n dropout_23 (Dropout)           (None, 512)          0           ['dense_50[0][0]']               \n                                                                                                  \n reshape_3 (Reshape)            (None, 128)          0           ['multi_head_attention_3[0][0]'] \n                                                                                                  \n dense_54 (Dense)               (None, 256)          131328      ['dropout_25[0][0]']             \n                                                                                                  \n dense_51 (Dense)               (None, 256)          131328      ['dropout_23[0][0]']             \n                                                                                                  \n concatenate_10 (Concatenate)   (None, 256)          0           ['reshape_3[0][0]',              \n                                                                  'dense_44[0][0]']               \n                                                                                                  \n concatenate_11 (Concatenate)   (None, 768)          0           ['dense_54[0][0]',               \n                                                                  'dense_51[0][0]',               \n                                                                  'concatenate_10[0][0]']         \n                                                                                                  \n batch_normalization_1 (BatchNo  (None, 768)         3072        ['concatenate_11[0][0]']         \n rmalization)                                                                                     \n                                                                                                  \n multi_head_attention_4 (MultiH  (None, None, 768)   2362368     ['batch_normalization_1[0][0]',  \n eadAttention)                                                    'batch_normalization_1[0][0]',  \n                                                                  'batch_normalization_1[0][0]']  \n                                                                                                  \n multi_head_attention_5 (MultiH  (None, None, 768)   2362368     ['batch_normalization_1[0][0]',  \n eadAttention)                                                    'batch_normalization_1[0][0]',  \n                                                                  'batch_normalization_1[0][0]']  \n                                                                                                  \n reshape_4 (Reshape)            (None, 768)          0           ['multi_head_attention_4[0][0]'] \n                                                                                                  \n reshape_5 (Reshape)            (None, 768)          0           ['multi_head_attention_5[0][0]'] \n                                                                                                  \n concatenate_12 (Concatenate)   (None, 1536)         0           ['reshape_4[0][0]',              \n                                                                  'batch_normalization_1[0][0]']  \n                                                                                                  \n concatenate_13 (Concatenate)   (None, 1536)         0           ['reshape_5[0][0]',              \n                                                                  'batch_normalization_1[0][0]']  \n                                                                                                  \n cross_stitch_2 (CrossStitch)   ((None, 1536),       4           ['concatenate_12[0][0]',         \n                                 (None, 1536))                    'concatenate_13[0][0]']         \n                                                                                                  \n dense_70 (Dense)               (None, 2048)         3147776     ['cross_stitch_2[0][0]']         \n                                                                                                  \n dense_71 (Dense)               (None, 2048)         3147776     ['cross_stitch_2[0][1]']         \n                                                                                                  \n cross_stitch_3 (CrossStitch)   ((None, 2048),       4           ['dense_70[0][0]',               \n                                 (None, 2048))                    'dense_71[0][0]']               \n                                                                                                  \n concatenate_14 (Concatenate)   (None, 3584)         0           ['cross_stitch_3[0][0]',         \n                                                                  'concatenate_12[0][0]']         \n                                                                                                  \n fsynergy1 (Dense)              (None, 128)          458880      ['concatenate_14[0][0]']         \n                                                                                                  \n p_re_lu_2 (PReLU)              (None, 128)          128         ['fsynergy1[0][0]']              \n                                                                                                  \n concatenate_15 (Concatenate)   (None, 3584)         0           ['cross_stitch_3[0][1]',         \n                                                                  'concatenate_13[0][0]']         \n                                                                                                  \n fsynergy2 (Dense)              (None, 64)           8256        ['p_re_lu_2[0][0]']              \n                                                                                                  \n fclass1 (Dense)                (None, 128)          458880      ['concatenate_15[0][0]']         \n                                                                                                  \n p_re_lu_3 (PReLU)              (None, 64)           64          ['fsynergy2[0][0]']              \n                                                                                                  \n fclass2 (Dense)                (None, 64)           8256        ['fclass1[0][0]']                \n                                                                                                  \n synergy (Dense)                (None, 1)            65          ['p_re_lu_3[0][0]']              \n                                                                                                  \n class (Dense)                  (None, 3)            195         ['fclass2[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 18,575,477\nTrainable params: 18,573,941\nNon-trainable params: 1,536\n__________________________________________________________________________________________________\nNone\nEpoch 1/1000\n570/570 [==============================] - 16s 18ms/step - loss: 874.9484 - synergy_loss: 444.3890 - class_loss: 0.8558\nEpoch 2/1000\n570/570 [==============================] - 10s 18ms/step - loss: 752.5055 - synergy_loss: 409.2575 - class_loss: 0.7923\nEpoch 3/1000\n570/570 [==============================] - 10s 18ms/step - loss: 700.2509 - synergy_loss: 391.4514 - class_loss: 0.7782\nEpoch 4/1000\n570/570 [==============================] - 10s 17ms/step - loss: 672.6411 - synergy_loss: 372.1086 - class_loss: 0.7678\nEpoch 5/1000\n570/570 [==============================] - 10s 18ms/step - loss: 656.0903 - synergy_loss: 357.8153 - class_loss: 0.7596\nEpoch 6/1000\n570/570 [==============================] - 10s 18ms/step - loss: 641.2070 - synergy_loss: 344.6163 - class_loss: 0.7476\nEpoch 7/1000\n570/570 [==============================] - 10s 17ms/step - loss: 628.4790 - synergy_loss: 333.4167 - class_loss: 0.7357\nEpoch 8/1000\n570/570 [==============================] - 10s 18ms/step - loss: 618.6216 - synergy_loss: 324.7938 - class_loss: 0.7256\nEpoch 9/1000\n570/570 [==============================] - 10s 18ms/step - loss: 605.1933 - synergy_loss: 312.6682 - class_loss: 0.7162\nEpoch 10/1000\n570/570 [==============================] - 10s 17ms/step - loss: 597.4499 - synergy_loss: 306.0066 - class_loss: 0.7068\nEpoch 11/1000\n570/570 [==============================] - 10s 18ms/step - loss: 589.2093 - synergy_loss: 298.8980 - class_loss: 0.6969\nEpoch 12/1000\n570/570 [==============================] - 10s 17ms/step - loss: 576.3356 - synergy_loss: 286.8103 - class_loss: 0.6896\nEpoch 13/1000\n570/570 [==============================] - 10s 18ms/step - loss: 568.4270 - synergy_loss: 279.9228 - class_loss: 0.6795\nEpoch 14/1000\n570/570 [==============================] - 10s 18ms/step - loss: 557.9662 - synergy_loss: 270.3315 - class_loss: 0.6719\nEpoch 15/1000\n570/570 [==============================] - 10s 18ms/step - loss: 548.1717 - synergy_loss: 261.4960 - class_loss: 0.6625\nEpoch 16/1000\n570/570 [==============================] - 10s 18ms/step - loss: 541.1977 - synergy_loss: 255.3827 - class_loss: 0.6568\nEpoch 17/1000\n570/570 [==============================] - 10s 17ms/step - loss: 529.9630 - synergy_loss: 245.0323 - class_loss: 0.6470\nEpoch 18/1000\n570/570 [==============================] - 11s 19ms/step - loss: 523.5654 - synergy_loss: 239.5092 - class_loss: 0.6431\nEpoch 19/1000\n570/570 [==============================] - 10s 17ms/step - loss: 516.5993 - synergy_loss: 233.3510 - class_loss: 0.6373\nEpoch 20/1000\n570/570 [==============================] - 10s 17ms/step - loss: 508.2187 - synergy_loss: 225.8178 - class_loss: 0.6266\nEpoch 21/1000\n570/570 [==============================] - 10s 18ms/step - loss: 496.8620 - synergy_loss: 215.2269 - class_loss: 0.6258\nEpoch 22/1000\n570/570 [==============================] - 10s 17ms/step - loss: 489.2506 - synergy_loss: 208.4057 - class_loss: 0.6215\nEpoch 23/1000\n570/570 [==============================] - 10s 17ms/step - loss: 480.8659 - synergy_loss: 200.8838 - class_loss: 0.6165\nEpoch 24/1000\n570/570 [==============================] - 10s 18ms/step - loss: 474.7090 - synergy_loss: 195.6617 - class_loss: 0.6067\nEpoch 25/1000\n570/570 [==============================] - 10s 18ms/step - loss: 470.0887 - synergy_loss: 191.9270 - class_loss: 0.6017\nEpoch 26/1000\n570/570 [==============================] - 10s 18ms/step - loss: 462.5235 - synergy_loss: 185.0872 - class_loss: 0.5974\nEpoch 27/1000\n570/570 [==============================] - 11s 18ms/step - loss: 459.4813 - synergy_loss: 182.9019 - class_loss: 0.5945\nEpoch 28/1000\n570/570 [==============================] - 10s 18ms/step - loss: 452.3965 - synergy_loss: 176.7085 - class_loss: 0.5875\nEpoch 29/1000\n570/570 [==============================] - 10s 17ms/step - loss: 445.7412 - synergy_loss: 170.9578 - class_loss: 0.5828\nEpoch 30/1000\n570/570 [==============================] - 11s 19ms/step - loss: 442.0724 - synergy_loss: 168.2392 - class_loss: 0.5793\nEpoch 31/1000\n570/570 [==============================] - 11s 18ms/step - loss: 445.3734 - synergy_loss: 172.2415 - class_loss: 0.5751\nEpoch 32/1000\n570/570 [==============================] - 10s 18ms/step - loss: 430.6904 - synergy_loss: 158.4134 - class_loss: 0.5691\nEpoch 33/1000\n570/570 [==============================] - 11s 18ms/step - loss: 430.2875 - synergy_loss: 158.9593 - class_loss: 0.5662\nEpoch 34/1000\n570/570 [==============================] - 10s 17ms/step - loss: 426.1572 - synergy_loss: 155.6305 - class_loss: 0.5647\nEpoch 35/1000\n570/570 [==============================] - 10s 18ms/step - loss: 422.1289 - synergy_loss: 152.4126 - class_loss: 0.5566\nEpoch 36/1000\n570/570 [==============================] - 10s 18ms/step - loss: 416.8194 - synergy_loss: 147.9272 - class_loss: 0.5522\nEpoch 37/1000\n570/570 [==============================] - 10s 17ms/step - loss: 407.7158 - synergy_loss: 139.8082 - class_loss: 0.5493\nEpoch 38/1000\n570/570 [==============================] - 10s 18ms/step - loss: 414.7156 - synergy_loss: 147.5633 - class_loss: 0.5453\nEpoch 39/1000\n570/570 [==============================] - 11s 18ms/step - loss: 404.0159 - synergy_loss: 137.6915 - class_loss: 0.5427\nEpoch 40/1000\n570/570 [==============================] - 10s 17ms/step - loss: 401.8303 - synergy_loss: 136.4090 - class_loss: 0.5375\nEpoch 41/1000\n570/570 [==============================] - 10s 18ms/step - loss: 397.2326 - synergy_loss: 132.6928 - class_loss: 0.5345\nEpoch 42/1000\n570/570 [==============================] - 11s 19ms/step - loss: 393.7188 - synergy_loss: 130.0965 - class_loss: 0.5304\nEpoch 43/1000\n570/570 [==============================] - 10s 18ms/step - loss: 388.7849 - synergy_loss: 125.9250 - class_loss: 0.5240\nEpoch 44/1000\n570/570 [==============================] - 10s 18ms/step - loss: 388.9307 - synergy_loss: 127.0663 - class_loss: 0.5247\nEpoch 45/1000\n570/570 [==============================] - 11s 19ms/step - loss: 388.8003 - synergy_loss: 127.5886 - class_loss: 0.5210\nEpoch 46/1000\n570/570 [==============================] - 10s 18ms/step - loss: 377.8058 - synergy_loss: 117.4395 - class_loss: 0.5157\nEpoch 47/1000\n570/570 [==============================] - 10s 17ms/step - loss: 376.1342 - synergy_loss: 116.8015 - class_loss: 0.5122\nEpoch 48/1000\n570/570 [==============================] - 10s 18ms/step - loss: 373.8011 - synergy_loss: 115.3002 - class_loss: 0.5080\nEpoch 49/1000\n570/570 [==============================] - 10s 18ms/step - loss: 366.6606 - synergy_loss: 109.1114 - class_loss: 0.5042\nEpoch 50/1000\n570/570 [==============================] - 10s 18ms/step - loss: 363.8772 - synergy_loss: 107.3186 - class_loss: 0.5023\nEpoch 51/1000\n570/570 [==============================] - 10s 18ms/step - loss: 361.0860 - synergy_loss: 105.5135 - class_loss: 0.5014\nEpoch 52/1000\n570/570 [==============================] - 10s 18ms/step - loss: 358.7594 - synergy_loss: 104.0676 - class_loss: 0.4928\nEpoch 53/1000\n570/570 [==============================] - 10s 17ms/step - loss: 362.3495 - synergy_loss: 108.6545 - class_loss: 0.4966\nEpoch 54/1000\n570/570 [==============================] - 10s 18ms/step - loss: 353.6909 - synergy_loss: 100.8039 - class_loss: 0.4845\nEpoch 55/1000\n570/570 [==============================] - 11s 18ms/step - loss: 348.9742 - synergy_loss: 97.1413 - class_loss: 0.4886\nEpoch 56/1000\n570/570 [==============================] - 10s 18ms/step - loss: 346.1581 - synergy_loss: 95.2990 - class_loss: 0.4824\nEpoch 57/1000\n570/570 [==============================] - 10s 17ms/step - loss: 346.3914 - synergy_loss: 96.5074 - class_loss: 0.4786\nEpoch 58/1000\n570/570 [==============================] - 11s 19ms/step - loss: 342.6370 - synergy_loss: 93.5250 - class_loss: 0.4781\nEpoch 59/1000\n570/570 [==============================] - 10s 18ms/step - loss: 338.3941 - synergy_loss: 90.2860 - class_loss: 0.4742\nEpoch 60/1000\n570/570 [==============================] - 10s 18ms/step - loss: 335.1890 - synergy_loss: 88.0011 - class_loss: 0.4693\nEpoch 61/1000\n570/570 [==============================] - 10s 18ms/step - loss: 331.8465 - synergy_loss: 85.6759 - class_loss: 0.4665\nEpoch 62/1000\n570/570 [==============================] - 10s 18ms/step - loss: 332.4398 - synergy_loss: 87.0912 - class_loss: 0.4654\nEpoch 63/1000\n570/570 [==============================] - 10s 18ms/step - loss: 328.0723 - synergy_loss: 83.7383 - class_loss: 0.4595\nEpoch 64/1000\n570/570 [==============================] - 10s 18ms/step - loss: 324.0818 - synergy_loss: 80.7528 - class_loss: 0.4604\nEpoch 65/1000\n570/570 [==============================] - 10s 18ms/step - loss: 323.6784 - synergy_loss: 81.3367 - class_loss: 0.4562\nEpoch 66/1000\n570/570 [==============================] - 10s 18ms/step - loss: 322.7013 - synergy_loss: 81.3377 - class_loss: 0.4516\nEpoch 67/1000\n570/570 [==============================] - 10s 18ms/step - loss: 315.9100 - synergy_loss: 75.6338 - class_loss: 0.4477\nEpoch 68/1000\n570/570 [==============================] - 10s 17ms/step - loss: 315.0464 - synergy_loss: 75.7724 - class_loss: 0.4461\nEpoch 69/1000\n570/570 [==============================] - 10s 18ms/step - loss: 315.0156 - synergy_loss: 76.7343 - class_loss: 0.4412\nEpoch 70/1000\n570/570 [==============================] - 10s 18ms/step - loss: 310.6330 - synergy_loss: 73.2704 - class_loss: 0.4414\nEpoch 71/1000\n570/570 [==============================] - 10s 17ms/step - loss: 306.6111 - synergy_loss: 70.2685 - class_loss: 0.4405\nEpoch 72/1000\n570/570 [==============================] - 10s 18ms/step - loss: 305.1274 - synergy_loss: 69.7992 - class_loss: 0.4327\nEpoch 73/1000\n570/570 [==============================] - 10s 18ms/step - loss: 301.4132 - synergy_loss: 67.1556 - class_loss: 0.4297\nEpoch 74/1000\n570/570 [==============================] - 11s 19ms/step - loss: 302.1337 - synergy_loss: 68.8487 - class_loss: 0.4288\nEpoch 75/1000\n570/570 [==============================] - 11s 20ms/step - loss: 299.1424 - synergy_loss: 66.9658 - class_loss: 0.4248\nEpoch 76/1000\n570/570 [==============================] - 11s 19ms/step - loss: 298.8351 - synergy_loss: 67.6093 - class_loss: 0.4239\nEpoch 77/1000\n570/570 [==============================] - 10s 18ms/step - loss: 292.7202 - synergy_loss: 62.5465 - class_loss: 0.4172\nEpoch 78/1000\n570/570 [==============================] - 10s 18ms/step - loss: 290.0859 - synergy_loss: 61.0565 - class_loss: 0.4140\nEpoch 79/1000\n570/570 [==============================] - 10s 17ms/step - loss: 289.7911 - synergy_loss: 61.8335 - class_loss: 0.4146\nEpoch 80/1000\n570/570 [==============================] - 10s 18ms/step - loss: 287.5966 - synergy_loss: 60.6883 - class_loss: 0.4122\nEpoch 81/1000\n570/570 [==============================] - 10s 17ms/step - loss: 284.5551 - synergy_loss: 58.7390 - class_loss: 0.4059\nEpoch 82/1000\n570/570 [==============================] - 10s 17ms/step - loss: 286.8361 - synergy_loss: 62.0228 - class_loss: 0.4044\nEpoch 83/1000\n570/570 [==============================] - 10s 18ms/step - loss: 282.1169 - synergy_loss: 58.3336 - class_loss: 0.4015\nEpoch 84/1000\n570/570 [==============================] - 10s 18ms/step - loss: 279.4425 - synergy_loss: 56.7141 - class_loss: 0.3968\nEpoch 85/1000\n570/570 [==============================] - 10s 17ms/step - loss: 277.1029 - synergy_loss: 55.4004 - class_loss: 0.3978\nEpoch 86/1000\n570/570 [==============================] - 10s 18ms/step - loss: 274.9993 - synergy_loss: 54.4094 - class_loss: 0.3945\nEpoch 87/1000\n570/570 [==============================] - 10s 18ms/step - loss: 275.3183 - synergy_loss: 55.8440 - class_loss: 0.3931\nEpoch 88/1000\n570/570 [==============================] - 10s 18ms/step - loss: 270.2177 - synergy_loss: 51.7797 - class_loss: 0.3871\nEpoch 89/1000\n570/570 [==============================] - 11s 19ms/step - loss: 270.4661 - synergy_loss: 53.1651 - class_loss: 0.3794\nEpoch 90/1000\n570/570 [==============================] - 10s 18ms/step - loss: 268.4183 - synergy_loss: 52.1696 - class_loss: 0.3799\nEpoch 91/1000\n570/570 [==============================] - 10s 17ms/step - loss: 266.5857 - synergy_loss: 51.3003 - class_loss: 0.3797\nEpoch 92/1000\n570/570 [==============================] - 10s 18ms/step - loss: 265.6546 - synergy_loss: 51.5146 - class_loss: 0.3785\nEpoch 93/1000\n570/570 [==============================] - 10s 18ms/step - loss: 263.7559 - synergy_loss: 50.6088 - class_loss: 0.3734\nEpoch 94/1000\n570/570 [==============================] - 10s 18ms/step - loss: 261.1295 - synergy_loss: 49.0833 - class_loss: 0.3747\nEpoch 95/1000\n570/570 [==============================] - 11s 19ms/step - loss: 259.9189 - synergy_loss: 48.9495 - class_loss: 0.3690\nEpoch 96/1000\n570/570 [==============================] - 10s 18ms/step - loss: 256.0170 - synergy_loss: 46.2408 - class_loss: 0.3651\nEpoch 97/1000\n570/570 [==============================] - 10s 17ms/step - loss: 256.4221 - synergy_loss: 47.7370 - class_loss: 0.3641\nEpoch 98/1000\n570/570 [==============================] - 10s 18ms/step - loss: 253.4689 - synergy_loss: 45.8294 - class_loss: 0.3607\nEpoch 99/1000\n570/570 [==============================] - 10s 18ms/step - loss: 252.8891 - synergy_loss: 46.3709 - class_loss: 0.3582\nEpoch 100/1000\n570/570 [==============================] - 10s 17ms/step - loss: 250.1608 - synergy_loss: 44.7699 - class_loss: 0.3547\nEpoch 101/1000\n570/570 [==============================] - 11s 19ms/step - loss: 247.8943 - synergy_loss: 43.7015 - class_loss: 0.3515\nEpoch 102/1000\n570/570 [==============================] - 10s 18ms/step - loss: 246.6606 - synergy_loss: 43.6575 - class_loss: 0.3516\nEpoch 103/1000\n570/570 [==============================] - 10s 17ms/step - loss: 244.7666 - synergy_loss: 42.9280 - class_loss: 0.3486\nEpoch 104/1000\n570/570 [==============================] - 10s 18ms/step - loss: 244.6211 - synergy_loss: 43.9972 - class_loss: 0.3474\nEpoch 105/1000\n570/570 [==============================] - 10s 18ms/step - loss: 242.3519 - synergy_loss: 42.8003 - class_loss: 0.3449\nEpoch 106/1000\n570/570 [==============================] - 10s 17ms/step - loss: 244.6385 - synergy_loss: 45.9993 - class_loss: 0.3435\nEpoch 107/1000\n570/570 [==============================] - 10s 17ms/step - loss: 238.6649 - synergy_loss: 41.0377 - class_loss: 0.3344\nEpoch 108/1000\n570/570 [==============================] - 10s 18ms/step - loss: 238.7627 - synergy_loss: 42.2170 - class_loss: 0.3369\nEpoch 109/1000\n570/570 [==============================] - 10s 17ms/step - loss: 235.8279 - synergy_loss: 40.3906 - class_loss: 0.3338\nEpoch 110/1000\n570/570 [==============================] - 10s 17ms/step - loss: 234.6844 - synergy_loss: 40.3759 - class_loss: 0.3294\nEpoch 111/1000\n570/570 [==============================] - 10s 18ms/step - loss: 231.0330 - synergy_loss: 37.8642 - class_loss: 0.3280\nEpoch 112/1000\n570/570 [==============================] - 10s 17ms/step - loss: 231.8398 - synergy_loss: 39.7974 - class_loss: 0.3248\nEpoch 113/1000\n570/570 [==============================] - 10s 17ms/step - loss: 229.4424 - synergy_loss: 38.4783 - class_loss: 0.3226\nEpoch 114/1000\n570/570 [==============================] - 10s 18ms/step - loss: 227.3361 - synergy_loss: 37.6162 - class_loss: 0.3205\nEpoch 115/1000\n570/570 [==============================] - 10s 17ms/step - loss: 227.0970 - synergy_loss: 38.5475 - class_loss: 0.3181\nEpoch 116/1000\n570/570 [==============================] - 10s 17ms/step - loss: 226.5974 - synergy_loss: 39.1302 - class_loss: 0.3178\nEpoch 117/1000\n570/570 [==============================] - 10s 18ms/step - loss: 223.3390 - synergy_loss: 37.0487 - class_loss: 0.3106\nEpoch 118/1000\n570/570 [==============================] - 10s 18ms/step - loss: 221.6202 - synergy_loss: 36.4276 - class_loss: 0.3148\nEpoch 119/1000\n570/570 [==============================] - 10s 17ms/step - loss: 220.3991 - synergy_loss: 36.4772 - class_loss: 0.3107\nEpoch 120/1000\n570/570 [==============================] - 11s 19ms/step - loss: 219.0868 - synergy_loss: 36.3367 - class_loss: 0.3079\nEpoch 121/1000\n570/570 [==============================] - 10s 18ms/step - loss: 218.6395 - synergy_loss: 36.9123 - class_loss: 0.3071\nEpoch 122/1000\n570/570 [==============================] - 10s 17ms/step - loss: 223.7295 - synergy_loss: 42.7668 - class_loss: 0.3082\nEpoch 123/1000\n570/570 [==============================] - 10s 18ms/step - loss: 216.3723 - synergy_loss: 36.4074 - class_loss: 0.2980\nEpoch 124/1000\n570/570 [==============================] - 10s 18ms/step - loss: 213.7306 - synergy_loss: 34.8957 - class_loss: 0.2933\nEpoch 125/1000\n570/570 [==============================] - 10s 17ms/step - loss: 212.2950 - synergy_loss: 34.6511 - class_loss: 0.2951\nEpoch 126/1000\n570/570 [==============================] - 10s 18ms/step - loss: 211.4317 - synergy_loss: 34.8488 - class_loss: 0.2893\nEpoch 127/1000\n570/570 [==============================] - 10s 18ms/step - loss: 210.5688 - synergy_loss: 34.9807 - class_loss: 0.2910\nEpoch 128/1000\n570/570 [==============================] - 10s 17ms/step - loss: 206.6579 - synergy_loss: 32.3619 - class_loss: 0.2877\nEpoch 129/1000\n570/570 [==============================] - 10s 18ms/step - loss: 206.7737 - synergy_loss: 33.5487 - class_loss: 0.2836\nEpoch 130/1000\n570/570 [==============================] - 10s 18ms/step - loss: 206.6219 - synergy_loss: 34.5199 - class_loss: 0.2861\nEpoch 131/1000\n570/570 [==============================] - 10s 17ms/step - loss: 207.4601 - synergy_loss: 36.1310 - class_loss: 0.2904\nEpoch 132/1000\n570/570 [==============================] - 10s 17ms/step - loss: 202.2388 - synergy_loss: 31.9791 - class_loss: 0.2788\nEpoch 133/1000\n570/570 [==============================] - 10s 18ms/step - loss: 200.0486 - synergy_loss: 31.0972 - class_loss: 0.2748\nEpoch 134/1000\n570/570 [==============================] - 10s 17ms/step - loss: 199.9130 - synergy_loss: 32.0855 - class_loss: 0.2797\nEpoch 135/1000\n570/570 [==============================] - 10s 17ms/step - loss: 201.0822 - synergy_loss: 34.3374 - class_loss: 0.2760\nEpoch 136/1000\n570/570 [==============================] - 11s 19ms/step - loss: 198.3664 - synergy_loss: 32.5877 - class_loss: 0.2742\nEpoch 137/1000\n570/570 [==============================] - 10s 17ms/step - loss: 195.8197 - synergy_loss: 31.1233 - class_loss: 0.2687\nEpoch 138/1000\n570/570 [==============================] - 10s 17ms/step - loss: 194.3384 - synergy_loss: 30.8160 - class_loss: 0.2653\nEpoch 139/1000\n570/570 [==============================] - 10s 18ms/step - loss: 194.3121 - synergy_loss: 31.9138 - class_loss: 0.2673\nEpoch 140/1000\n570/570 [==============================] - 10s 17ms/step - loss: 191.6682 - synergy_loss: 30.3882 - class_loss: 0.2639\nEpoch 141/1000\n570/570 [==============================] - 10s 17ms/step - loss: 191.9796 - synergy_loss: 31.7812 - class_loss: 0.2595\nEpoch 142/1000\n570/570 [==============================] - 10s 18ms/step - loss: 190.7617 - synergy_loss: 31.6617 - class_loss: 0.2634\nEpoch 143/1000\n570/570 [==============================] - 10s 17ms/step - loss: 188.7380 - synergy_loss: 30.7085 - class_loss: 0.2587\nEpoch 144/1000\n570/570 [==============================] - 10s 17ms/step - loss: 189.9720 - synergy_loss: 32.9907 - class_loss: 0.2587\nEpoch 145/1000\n570/570 [==============================] - 10s 18ms/step - loss: 187.0668 - synergy_loss: 30.8173 - class_loss: 0.2540\nEpoch 146/1000\n570/570 [==============================] - 10s 17ms/step - loss: 184.0218 - synergy_loss: 28.9565 - class_loss: 0.2490\nEpoch 147/1000\n570/570 [==============================] - 10s 17ms/step - loss: 182.9380 - synergy_loss: 29.0487 - class_loss: 0.2499\nEpoch 148/1000\n570/570 [==============================] - 10s 18ms/step - loss: 180.7731 - synergy_loss: 28.1614 - class_loss: 0.2483\nEpoch 149/1000\n570/570 [==============================] - 10s 18ms/step - loss: 181.5280 - synergy_loss: 30.0876 - class_loss: 0.2433\nEpoch 150/1000\n570/570 [==============================] - 10s 17ms/step - loss: 191.7032 - synergy_loss: 40.5667 - class_loss: 0.2545\nEpoch 151/1000\n570/570 [==============================] - 10s 18ms/step - loss: 179.1637 - synergy_loss: 28.7195 - class_loss: 0.2455\nEpoch 152/1000\n570/570 [==============================] - 10s 18ms/step - loss: 179.3644 - synergy_loss: 29.8631 - class_loss: 0.2439\nEpoch 153/1000\n570/570 [==============================] - 10s 17ms/step - loss: 175.2582 - synergy_loss: 26.8812 - class_loss: 0.2381\nEpoch 154/1000\n570/570 [==============================] - 10s 17ms/step - loss: 173.9221 - synergy_loss: 26.7384 - class_loss: 0.2330\nEpoch 155/1000\n570/570 [==============================] - 10s 18ms/step - loss: 173.5796 - synergy_loss: 27.5298 - class_loss: 0.2313\nEpoch 156/1000\n570/570 [==============================] - 10s 17ms/step - loss: 172.7973 - synergy_loss: 27.8275 - class_loss: 0.2333\nEpoch 157/1000\n570/570 [==============================] - 10s 18ms/step - loss: 170.9232 - synergy_loss: 27.1253 - class_loss: 0.2316\nEpoch 158/1000\n570/570 [==============================] - 10s 18ms/step - loss: 171.8999 - synergy_loss: 29.0644 - class_loss: 0.2273\nEpoch 159/1000\n570/570 [==============================] - 10s 17ms/step - loss: 169.4738 - synergy_loss: 27.7019 - class_loss: 0.2322\nEpoch 160/1000\n570/570 [==============================] - 10s 18ms/step - loss: 168.6413 - synergy_loss: 27.9591 - class_loss: 0.2272\nEpoch 161/1000\n570/570 [==============================] - 10s 18ms/step - loss: 166.7521 - synergy_loss: 27.1494 - class_loss: 0.2267\nEpoch 162/1000\n570/570 [==============================] - 10s 17ms/step - loss: 165.4046 - synergy_loss: 26.9646 - class_loss: 0.2246\nEpoch 163/1000\n570/570 [==============================] - 10s 18ms/step - loss: 165.1487 - synergy_loss: 27.6968 - class_loss: 0.2241\nEpoch 164/1000\n570/570 [==============================] - 10s 18ms/step - loss: 162.3372 - synergy_loss: 25.9498 - class_loss: 0.2221\nEpoch 165/1000\n570/570 [==============================] - 10s 17ms/step - loss: 161.4482 - synergy_loss: 26.2489 - class_loss: 0.2154\nEpoch 166/1000\n570/570 [==============================] - 10s 18ms/step - loss: 163.8074 - synergy_loss: 29.4760 - class_loss: 0.2192\nEpoch 167/1000\n570/570 [==============================] - 10s 18ms/step - loss: 159.6206 - synergy_loss: 26.2563 - class_loss: 0.2180\nEpoch 168/1000\n570/570 [==============================] - 10s 18ms/step - loss: 157.7811 - synergy_loss: 25.5539 - class_loss: 0.2122\nEpoch 169/1000\n570/570 [==============================] - 10s 18ms/step - loss: 157.9296 - synergy_loss: 26.7063 - class_loss: 0.2160\nEpoch 170/1000\n570/570 [==============================] - 10s 18ms/step - loss: 161.8598 - synergy_loss: 31.0232 - class_loss: 0.2212\nEpoch 171/1000\n570/570 [==============================] - 10s 18ms/step - loss: 156.1964 - synergy_loss: 26.2491 - class_loss: 0.2114\nEpoch 172/1000\n570/570 [==============================] - 10s 18ms/step - loss: 154.4758 - synergy_loss: 25.6309 - class_loss: 0.2082\nEpoch 173/1000\n570/570 [==============================] - 10s 18ms/step - loss: 153.8849 - synergy_loss: 25.9112 - class_loss: 0.2108\nEpoch 174/1000\n570/570 [==============================] - 10s 18ms/step - loss: 151.7640 - synergy_loss: 24.8481 - class_loss: 0.2019\nEpoch 175/1000\n570/570 [==============================] - 10s 17ms/step - loss: 151.3422 - synergy_loss: 25.4507 - class_loss: 0.2038\nEpoch 176/1000\n570/570 [==============================] - 10s 17ms/step - loss: 149.5076 - synergy_loss: 24.6239 - class_loss: 0.2038\nEpoch 177/1000\n570/570 [==============================] - 10s 18ms/step - loss: 148.7467 - synergy_loss: 24.8646 - class_loss: 0.2059\nEpoch 178/1000\n570/570 [==============================] - 10s 17ms/step - loss: 147.8414 - synergy_loss: 24.9795 - class_loss: 0.2066\nEpoch 179/1000\n570/570 [==============================] - 10s 17ms/step - loss: 147.2395 - synergy_loss: 25.3314 - class_loss: 0.1990\nEpoch 180/1000\n570/570 [==============================] - 10s 18ms/step - loss: 146.1860 - synergy_loss: 25.2823 - class_loss: 0.2004\nEpoch 181/1000\n570/570 [==============================] - 10s 18ms/step - loss: 146.3441 - synergy_loss: 26.3233 - class_loss: 0.1952\nEpoch 182/1000\n570/570 [==============================] - 10s 17ms/step - loss: 144.6147 - synergy_loss: 25.3600 - class_loss: 0.1990\nEpoch 183/1000\n570/570 [==============================] - 10s 18ms/step - loss: 142.6567 - synergy_loss: 24.5229 - class_loss: 0.1946\nEpoch 184/1000\n570/570 [==============================] - 10s 18ms/step - loss: 141.5645 - synergy_loss: 24.3918 - class_loss: 0.1960\nEpoch 185/1000\n570/570 [==============================] - 10s 17ms/step - loss: 141.2844 - synergy_loss: 25.0224 - class_loss: 0.1919\nEpoch 186/1000\n570/570 [==============================] - 10s 18ms/step - loss: 141.3758 - synergy_loss: 25.7285 - class_loss: 0.1915\nEpoch 187/1000\n570/570 [==============================] - 10s 18ms/step - loss: 138.9325 - synergy_loss: 24.2448 - class_loss: 0.1920\nEpoch 188/1000\n570/570 [==============================] - 10s 17ms/step - loss: 137.6273 - synergy_loss: 23.9651 - class_loss: 0.1863\nEpoch 189/1000\n570/570 [==============================] - 10s 18ms/step - loss: 137.2682 - synergy_loss: 24.4992 - class_loss: 0.1885\nEpoch 190/1000\n570/570 [==============================] - 10s 18ms/step - loss: 136.4503 - synergy_loss: 24.2997 - class_loss: 0.1927\nEpoch 191/1000\n570/570 [==============================] - 10s 17ms/step - loss: 135.7643 - synergy_loss: 24.5044 - class_loss: 0.1823\nEpoch 192/1000\n570/570 [==============================] - 10s 17ms/step - loss: 133.4121 - synergy_loss: 23.0460 - class_loss: 0.1831\nEpoch 193/1000\n570/570 [==============================] - 10s 18ms/step - loss: 133.6587 - synergy_loss: 24.1048 - class_loss: 0.1865\nEpoch 194/1000\n570/570 [==============================] - 10s 17ms/step - loss: 131.1266 - synergy_loss: 22.5753 - class_loss: 0.1823\nEpoch 195/1000\n570/570 [==============================] - 10s 18ms/step - loss: 131.8907 - synergy_loss: 24.1368 - class_loss: 0.1778\nEpoch 196/1000\n570/570 [==============================] - 11s 19ms/step - loss: 131.0530 - synergy_loss: 24.0743 - class_loss: 0.1783\nEpoch 197/1000\n570/570 [==============================] - 10s 18ms/step - loss: 129.2748 - synergy_loss: 23.0924 - class_loss: 0.1774\nEpoch 198/1000\n570/570 [==============================] - 11s 19ms/step - loss: 135.2455 - synergy_loss: 29.0615 - class_loss: 0.1859\nEpoch 199/1000\n570/570 [==============================] - 10s 18ms/step - loss: 127.9044 - synergy_loss: 22.6841 - class_loss: 0.1755\nEpoch 200/1000\n570/570 [==============================] - 10s 17ms/step - loss: 126.8922 - synergy_loss: 22.5864 - class_loss: 0.1748\nEpoch 201/1000\n570/570 [==============================] - 10s 17ms/step - loss: 125.5811 - synergy_loss: 22.1628 - class_loss: 0.1752\nEpoch 202/1000\n570/570 [==============================] - 11s 19ms/step - loss: 125.8278 - synergy_loss: 23.1128 - class_loss: 0.1775\nEpoch 203/1000\n570/570 [==============================] - 10s 17ms/step - loss: 123.7364 - synergy_loss: 21.9851 - class_loss: 0.1686\nEpoch 204/1000\n570/570 [==============================] - 10s 17ms/step - loss: 124.1242 - synergy_loss: 23.1914 - class_loss: 0.1729\nEpoch 205/1000\n570/570 [==============================] - 10s 18ms/step - loss: 134.3284 - synergy_loss: 32.6551 - class_loss: 0.1956\nEpoch 206/1000\n570/570 [==============================] - 10s 17ms/step - loss: 122.3521 - synergy_loss: 21.6507 - class_loss: 0.1680\nEpoch 207/1000\n570/570 [==============================] - 10s 17ms/step - loss: 121.3312 - synergy_loss: 21.5547 - class_loss: 0.1673\nEpoch 208/1000\n570/570 [==============================] - 11s 19ms/step - loss: 120.5750 - synergy_loss: 21.6347 - class_loss: 0.1664\nEpoch 209/1000\n570/570 [==============================] - 10s 17ms/step - loss: 119.7185 - synergy_loss: 21.5760 - class_loss: 0.1637\nEpoch 210/1000\n570/570 [==============================] - 10s 17ms/step - loss: 119.3071 - synergy_loss: 21.9199 - class_loss: 0.1633\nEpoch 211/1000\n570/570 [==============================] - 10s 18ms/step - loss: 119.4340 - synergy_loss: 22.7464 - class_loss: 0.1701\nEpoch 212/1000\n570/570 [==============================] - 10s 17ms/step - loss: 118.2221 - synergy_loss: 22.2088 - class_loss: 0.1625\nEpoch 213/1000\n570/570 [==============================] - 10s 17ms/step - loss: 117.9128 - synergy_loss: 22.5959 - class_loss: 0.1668\nEpoch 214/1000\n570/570 [==============================] - 10s 18ms/step - loss: 116.8107 - synergy_loss: 22.2312 - class_loss: 0.1598\nEpoch 215/1000\n570/570 [==============================] - 10s 18ms/step - loss: 116.7203 - synergy_loss: 22.8033 - class_loss: 0.1567\nEpoch 216/1000\n570/570 [==============================] - 10s 17ms/step - loss: 114.6313 - synergy_loss: 21.4972 - class_loss: 0.1618\nEpoch 217/1000\n570/570 [==============================] - 10s 17ms/step - loss: 114.8315 - synergy_loss: 22.4740 - class_loss: 0.1591\nEpoch 218/1000\n570/570 [==============================] - 10s 18ms/step - loss: 114.3445 - synergy_loss: 22.6199 - class_loss: 0.1641\nEpoch 219/1000\n570/570 [==============================] - 10s 18ms/step - loss: 113.9950 - synergy_loss: 22.8101 - class_loss: 0.1607\nEpoch 220/1000\n570/570 [==============================] - 10s 18ms/step - loss: 111.6576 - synergy_loss: 21.2591 - class_loss: 0.1559\nEpoch 221/1000\n570/570 [==============================] - 10s 18ms/step - loss: 111.8697 - synergy_loss: 22.1122 - class_loss: 0.1547\nEpoch 222/1000\n570/570 [==============================] - 10s 17ms/step - loss: 111.5094 - synergy_loss: 22.3542 - class_loss: 0.1583\nEpoch 223/1000\n570/570 [==============================] - 10s 18ms/step - loss: 109.6248 - synergy_loss: 21.2332 - class_loss: 0.1537\nEpoch 224/1000\n570/570 [==============================] - 10s 18ms/step - loss: 108.6160 - synergy_loss: 20.8611 - class_loss: 0.1591\nEpoch 225/1000\n570/570 [==============================] - 10s 17ms/step - loss: 109.0998 - synergy_loss: 21.9741 - class_loss: 0.1609\nEpoch 226/1000\n570/570 [==============================] - 10s 18ms/step - loss: 107.9460 - synergy_loss: 21.3881 - class_loss: 0.1484\nEpoch 227/1000\n570/570 [==============================] - 10s 18ms/step - loss: 109.6070 - synergy_loss: 23.3958 - class_loss: 0.1591\nEpoch 228/1000\n570/570 [==============================] - 10s 17ms/step - loss: 106.3905 - synergy_loss: 20.8386 - class_loss: 0.1496\nEpoch 229/1000\n570/570 [==============================] - 10s 18ms/step - loss: 105.7691 - synergy_loss: 20.9494 - class_loss: 0.1483\nEpoch 230/1000\n570/570 [==============================] - 10s 18ms/step - loss: 105.2594 - synergy_loss: 20.9473 - class_loss: 0.1491\nEpoch 231/1000\n570/570 [==============================] - 10s 17ms/step - loss: 104.0157 - synergy_loss: 20.4127 - class_loss: 0.1477\nEpoch 232/1000\n570/570 [==============================] - 10s 18ms/step - loss: 105.3102 - synergy_loss: 22.0689 - class_loss: 0.1500\nEpoch 233/1000\n570/570 [==============================] - 10s 18ms/step - loss: 104.7071 - synergy_loss: 21.9920 - class_loss: 0.1555\nEpoch 234/1000\n570/570 [==============================] - 10s 17ms/step - loss: 102.6099 - synergy_loss: 20.5055 - class_loss: 0.1442\nEpoch 235/1000\n570/570 [==============================] - 10s 18ms/step - loss: 101.9682 - synergy_loss: 20.4889 - class_loss: 0.1493\nEpoch 236/1000\n570/570 [==============================] - 10s 17ms/step - loss: 101.7102 - synergy_loss: 20.6753 - class_loss: 0.1503\nEpoch 237/1000\n570/570 [==============================] - 11s 18ms/step - loss: 100.5161 - synergy_loss: 20.1712 - class_loss: 0.1413\nEpoch 238/1000\n570/570 [==============================] - 10s 17ms/step - loss: 100.6446 - synergy_loss: 20.8609 - class_loss: 0.1485\nEpoch 239/1000\n570/570 [==============================] - 10s 17ms/step - loss: 99.7949 - synergy_loss: 20.4383 - class_loss: 0.1459\nEpoch 240/1000\n570/570 [==============================] - 10s 18ms/step - loss: 98.8567 - synergy_loss: 20.1663 - class_loss: 0.1442\nEpoch 241/1000\n570/570 [==============================] - 10s 17ms/step - loss: 102.1208 - synergy_loss: 23.5054 - class_loss: 0.1462\nEpoch 242/1000\n570/570 [==============================] - 10s 17ms/step - loss: 98.0740 - synergy_loss: 20.0892 - class_loss: 0.1399\nEpoch 243/1000\n570/570 [==============================] - 10s 18ms/step - loss: 96.5661 - synergy_loss: 19.2267 - class_loss: 0.1398\nEpoch 244/1000\n570/570 [==============================] - 10s 18ms/step - loss: 96.3779 - synergy_loss: 19.6089 - class_loss: 0.1368\nEpoch 245/1000\n570/570 [==============================] - 10s 17ms/step - loss: 96.5474 - synergy_loss: 20.2773 - class_loss: 0.1425\nEpoch 246/1000\n570/570 [==============================] - 10s 18ms/step - loss: 96.4208 - synergy_loss: 20.6092 - class_loss: 0.1388\nEpoch 247/1000\n570/570 [==============================] - 10s 17ms/step - loss: 96.6151 - synergy_loss: 21.2067 - class_loss: 0.1400\nEpoch 248/1000\n570/570 [==============================] - 10s 18ms/step - loss: 104.3874 - synergy_loss: 28.6266 - class_loss: 0.1593\nEpoch 249/1000\n570/570 [==============================] - 10s 18ms/step - loss: 96.3166 - synergy_loss: 20.8920 - class_loss: 0.1444\nEpoch 250/1000\n570/570 [==============================] - 10s 18ms/step - loss: 93.5629 - synergy_loss: 18.7980 - class_loss: 0.1302\nEpoch 251/1000\n570/570 [==============================] - 10s 17ms/step - loss: 94.4252 - synergy_loss: 20.2385 - class_loss: 0.1326\nEpoch 252/1000\n570/570 [==============================] - 10s 18ms/step - loss: 93.5706 - synergy_loss: 19.7996 - class_loss: 0.1377\nEpoch 253/1000\n570/570 [==============================] - 10s 18ms/step - loss: 92.7408 - synergy_loss: 19.5138 - class_loss: 0.1337\nEpoch 254/1000\n570/570 [==============================] - 10s 18ms/step - loss: 92.0900 - synergy_loss: 19.3704 - class_loss: 0.1306\nEpoch 255/1000\n570/570 [==============================] - 10s 18ms/step - loss: 91.9049 - synergy_loss: 19.6773 - class_loss: 0.1340\nEpoch 256/1000\n570/570 [==============================] - 10s 18ms/step - loss: 91.5281 - synergy_loss: 19.7180 - class_loss: 0.1362\nEpoch 257/1000\n570/570 [==============================] - 10s 17ms/step - loss: 91.3279 - synergy_loss: 19.9443 - class_loss: 0.1327\nEpoch 258/1000\n570/570 [==============================] - 10s 17ms/step - loss: 90.6713 - synergy_loss: 19.7450 - class_loss: 0.1331\nEpoch 259/1000\n570/570 [==============================] - 10s 18ms/step - loss: 89.9211 - synergy_loss: 19.4910 - class_loss: 0.1362\nEpoch 260/1000\n570/570 [==============================] - 10s 18ms/step - loss: 89.0758 - synergy_loss: 19.1475 - class_loss: 0.1284\nEpoch 261/1000\n570/570 [==============================] - 10s 17ms/step - loss: 89.5668 - synergy_loss: 20.0117 - class_loss: 0.1353\nEpoch 262/1000\n570/570 [==============================] - 10s 18ms/step - loss: 89.3752 - synergy_loss: 20.2392 - class_loss: 0.1309\nEpoch 263/1000\n570/570 [==============================] - 10s 18ms/step - loss: 88.7476 - synergy_loss: 19.8676 - class_loss: 0.1268\nEpoch 264/1000\n570/570 [==============================] - 10s 17ms/step - loss: 87.0738 - synergy_loss: 18.7624 - class_loss: 0.1254\nEpoch 265/1000\n570/570 [==============================] - 10s 18ms/step - loss: 87.1908 - synergy_loss: 19.3188 - class_loss: 0.1272\nEpoch 266/1000\n570/570 [==============================] - 10s 17ms/step - loss: 89.0274 - synergy_loss: 21.3116 - class_loss: 0.1366\nEpoch 267/1000\n570/570 [==============================] - 10s 18ms/step - loss: 89.4939 - synergy_loss: 21.8601 - class_loss: 0.1374\nEpoch 268/1000\n570/570 [==============================] - 10s 18ms/step - loss: 86.0352 - synergy_loss: 18.8948 - class_loss: 0.1245\nEpoch 269/1000\n570/570 [==============================] - 10s 18ms/step - loss: 84.9834 - synergy_loss: 18.4197 - class_loss: 0.1256\nEpoch 270/1000\n570/570 [==============================] - 10s 17ms/step - loss: 84.7574 - synergy_loss: 18.6598 - class_loss: 0.1201\nEpoch 271/1000\n570/570 [==============================] - 10s 18ms/step - loss: 85.0027 - synergy_loss: 19.2093 - class_loss: 0.1267\nEpoch 272/1000\n570/570 [==============================] - 10s 18ms/step - loss: 85.1270 - synergy_loss: 19.6958 - class_loss: 0.1255\nEpoch 273/1000\n570/570 [==============================] - 10s 17ms/step - loss: 83.5277 - synergy_loss: 18.5302 - class_loss: 0.1235\nEpoch 274/1000\n570/570 [==============================] - 10s 18ms/step - loss: 84.0860 - synergy_loss: 19.4725 - class_loss: 0.1238\nEpoch 275/1000\n570/570 [==============================] - 10s 18ms/step - loss: 84.2942 - synergy_loss: 19.8239 - class_loss: 0.1284\nEpoch 276/1000\n570/570 [==============================] - 10s 17ms/step - loss: 82.9318 - synergy_loss: 18.8762 - class_loss: 0.1229\nEpoch 277/1000\n570/570 [==============================] - 10s 18ms/step - loss: 82.2522 - synergy_loss: 18.6344 - class_loss: 0.1289\nEpoch 278/1000\n570/570 [==============================] - 10s 18ms/step - loss: 82.6534 - synergy_loss: 19.3294 - class_loss: 0.1205\nEpoch 279/1000\n570/570 [==============================] - 10s 18ms/step - loss: 91.4477 - synergy_loss: 27.3476 - class_loss: 0.1534\nEpoch 280/1000\n570/570 [==============================] - 10s 17ms/step - loss: 81.3052 - synergy_loss: 17.6849 - class_loss: 0.1234\nEpoch 281/1000\n570/570 [==============================] - 11s 18ms/step - loss: 80.3081 - synergy_loss: 17.2893 - class_loss: 0.1176\nEpoch 282/1000\n570/570 [==============================] - 10s 18ms/step - loss: 79.9563 - synergy_loss: 17.4223 - class_loss: 0.1165\nEpoch 283/1000\n570/570 [==============================] - 10s 17ms/step - loss: 80.9261 - synergy_loss: 18.7408 - class_loss: 0.1209\nEpoch 284/1000\n570/570 [==============================] - 10s 18ms/step - loss: 79.4452 - synergy_loss: 17.7032 - class_loss: 0.1167\nEpoch 285/1000\n570/570 [==============================] - 10s 17ms/step - loss: 79.1155 - synergy_loss: 17.7962 - class_loss: 0.1212\nEpoch 286/1000\n570/570 [==============================] - 10s 17ms/step - loss: 79.2359 - synergy_loss: 18.2685 - class_loss: 0.1165\nEpoch 287/1000\n570/570 [==============================] - 10s 18ms/step - loss: 80.0689 - synergy_loss: 19.3184 - class_loss: 0.1169\nEpoch 288/1000\n570/570 [==============================] - 10s 17ms/step - loss: 78.2891 - synergy_loss: 17.9799 - class_loss: 0.1191\nEpoch 289/1000\n570/570 [==============================] - 10s 17ms/step - loss: 78.3147 - synergy_loss: 18.3290 - class_loss: 0.1180\nEpoch 290/1000\n570/570 [==============================] - 10s 18ms/step - loss: 80.0115 - synergy_loss: 20.0923 - class_loss: 0.1278\nEpoch 291/1000\n570/570 [==============================] - 10s 18ms/step - loss: 78.5025 - synergy_loss: 18.8029 - class_loss: 0.1205\nEpoch 292/1000\n570/570 [==============================] - 10s 17ms/step - loss: 77.3894 - synergy_loss: 18.1369 - class_loss: 0.1138\nEpoch 293/1000\n570/570 [==============================] - 11s 19ms/step - loss: 76.8320 - synergy_loss: 17.9944 - class_loss: 0.1156\nEpoch 294/1000\n570/570 [==============================] - 10s 17ms/step - loss: 76.1488 - synergy_loss: 17.7128 - class_loss: 0.1175\nEpoch 295/1000\n570/570 [==============================] - 10s 17ms/step - loss: 80.0321 - synergy_loss: 21.4854 - class_loss: 0.1274\nEpoch 296/1000\n570/570 [==============================] - 10s 18ms/step - loss: 76.9436 - synergy_loss: 18.6986 - class_loss: 0.1181\nEpoch 297/1000\n570/570 [==============================] - 10s 17ms/step - loss: 75.8651 - synergy_loss: 18.0372 - class_loss: 0.1141\nEpoch 298/1000\n570/570 [==============================] - 10s 17ms/step - loss: 76.4641 - synergy_loss: 18.7848 - class_loss: 0.1151\nEpoch 299/1000\n570/570 [==============================] - 10s 18ms/step - loss: 74.9640 - synergy_loss: 17.6279 - class_loss: 0.1149\nEpoch 300/1000\n570/570 [==============================] - 10s 18ms/step - loss: 74.6613 - synergy_loss: 17.7173 - class_loss: 0.1098\nEpoch 301/1000\n570/570 [==============================] - 10s 17ms/step - loss: 75.4693 - synergy_loss: 18.7621 - class_loss: 0.1102\nEpoch 302/1000\n570/570 [==============================] - 10s 18ms/step - loss: 74.0202 - synergy_loss: 17.6398 - class_loss: 0.1093\nEpoch 303/1000\n570/570 [==============================] - 11s 19ms/step - loss: 75.4596 - synergy_loss: 19.2105 - class_loss: 0.1179\nEpoch 304/1000\n570/570 [==============================] - 10s 18ms/step - loss: 73.1186 - synergy_loss: 17.2947 - class_loss: 0.1132\nEpoch 305/1000\n570/570 [==============================] - 10s 18ms/step - loss: 73.0491 - synergy_loss: 17.5330 - class_loss: 0.1124\nEpoch 306/1000\n570/570 [==============================] - 10s 18ms/step - loss: 74.8888 - synergy_loss: 19.4839 - class_loss: 0.1177\nEpoch 307/1000\n570/570 [==============================] - 10s 17ms/step - loss: 72.7996 - synergy_loss: 17.7312 - class_loss: 0.1091\nEpoch 308/1000\n570/570 [==============================] - 10s 18ms/step - loss: 72.5252 - synergy_loss: 17.7465 - class_loss: 0.1106\nEpoch 309/1000\n570/570 [==============================] - 10s 18ms/step - loss: 72.6999 - synergy_loss: 18.2093 - class_loss: 0.1089\nEpoch 310/1000\n570/570 [==============================] - 10s 17ms/step - loss: 71.7469 - synergy_loss: 17.5115 - class_loss: 0.1068\nEpoch 311/1000\n570/570 [==============================] - 10s 18ms/step - loss: 71.6714 - synergy_loss: 17.7017 - class_loss: 0.1057\nEpoch 312/1000\n570/570 [==============================] - 10s 18ms/step - loss: 70.7870 - synergy_loss: 17.2206 - class_loss: 0.1079\nEpoch 313/1000\n570/570 [==============================] - 10s 17ms/step - loss: 71.6669 - synergy_loss: 18.2344 - class_loss: 0.1164\nEpoch 314/1000\n570/570 [==============================] - 10s 18ms/step - loss: 71.4554 - synergy_loss: 18.2734 - class_loss: 0.1084\nEpoch 315/1000\n570/570 [==============================] - 10s 18ms/step - loss: 70.5669 - synergy_loss: 17.6848 - class_loss: 0.1050\nEpoch 316/1000\n570/570 [==============================] - 10s 17ms/step - loss: 70.1251 - synergy_loss: 17.5415 - class_loss: 0.1106\nEpoch 317/1000\n570/570 [==============================] - 10s 18ms/step - loss: 68.8960 - synergy_loss: 16.6473 - class_loss: 0.1047\nEpoch 318/1000\n570/570 [==============================] - 10s 18ms/step - loss: 69.3597 - synergy_loss: 17.4062 - class_loss: 0.1054\nEpoch 319/1000\n570/570 [==============================] - 10s 17ms/step - loss: 69.4134 - synergy_loss: 17.6369 - class_loss: 0.1067\nEpoch 320/1000\n570/570 [==============================] - 10s 18ms/step - loss: 73.0468 - synergy_loss: 21.2682 - class_loss: 0.1238\nEpoch 321/1000\n570/570 [==============================] - 10s 18ms/step - loss: 69.9320 - synergy_loss: 17.9860 - class_loss: 0.1140\nEpoch 322/1000\n570/570 [==============================] - 10s 17ms/step - loss: 68.2882 - synergy_loss: 16.7556 - class_loss: 0.1019\nEpoch 323/1000\n570/570 [==============================] - 10s 18ms/step - loss: 68.0156 - synergy_loss: 16.8565 - class_loss: 0.1028\nEpoch 324/1000\n570/570 [==============================] - 10s 17ms/step - loss: 68.8433 - synergy_loss: 17.9572 - class_loss: 0.1023\nEpoch 325/1000\n570/570 [==============================] - 10s 18ms/step - loss: 68.4887 - synergy_loss: 17.6796 - class_loss: 0.1051\nEpoch 326/1000\n570/570 [==============================] - 10s 18ms/step - loss: 67.9125 - synergy_loss: 17.3672 - class_loss: 0.1033\nEpoch 327/1000\n570/570 [==============================] - 10s 18ms/step - loss: 66.8080 - synergy_loss: 16.6020 - class_loss: 0.1052\nEpoch 328/1000\n570/570 [==============================] - 10s 18ms/step - loss: 66.3454 - synergy_loss: 16.4774 - class_loss: 0.1016\nEpoch 329/1000\n570/570 [==============================] - 10s 17ms/step - loss: 66.5593 - synergy_loss: 16.9548 - class_loss: 0.1023\nEpoch 330/1000\n570/570 [==============================] - 10s 17ms/step - loss: 66.3455 - synergy_loss: 16.9753 - class_loss: 0.1034\nEpoch 331/1000\n570/570 [==============================] - 10s 18ms/step - loss: 69.8095 - synergy_loss: 20.2611 - class_loss: 0.1130\nEpoch 332/1000\n570/570 [==============================] - 10s 18ms/step - loss: 72.6760 - synergy_loss: 22.5380 - class_loss: 0.1252\nEpoch 333/1000\n570/570 [==============================] - 10s 17ms/step - loss: 69.1606 - synergy_loss: 19.1536 - class_loss: 0.1143\nEpoch 334/1000\n570/570 [==============================] - 10s 18ms/step - loss: 65.4884 - synergy_loss: 15.9854 - class_loss: 0.1042\nEpoch 335/1000\n570/570 [==============================] - 10s 18ms/step - loss: 64.6156 - synergy_loss: 15.5871 - class_loss: 0.0987\nEpoch 336/1000\n570/570 [==============================] - 10s 18ms/step - loss: 64.3963 - synergy_loss: 15.7515 - class_loss: 0.0977\nEpoch 337/1000\n570/570 [==============================] - 10s 18ms/step - loss: 64.5919 - synergy_loss: 16.2765 - class_loss: 0.0980\nEpoch 338/1000\n570/570 [==============================] - 10s 18ms/step - loss: 66.7099 - synergy_loss: 18.4134 - class_loss: 0.1044\nEpoch 339/1000\n570/570 [==============================] - 10s 17ms/step - loss: 64.5207 - synergy_loss: 16.4767 - class_loss: 0.0986\nEpoch 340/1000\n570/570 [==============================] - 10s 18ms/step - loss: 63.6112 - synergy_loss: 15.9557 - class_loss: 0.0997\nEpoch 341/1000\n570/570 [==============================] - 10s 18ms/step - loss: 64.1028 - synergy_loss: 16.6764 - class_loss: 0.1008\nEpoch 342/1000\n570/570 [==============================] - 10s 18ms/step - loss: 64.7873 - synergy_loss: 17.5113 - class_loss: 0.0976\nEpoch 343/1000\n570/570 [==============================] - 10s 18ms/step - loss: 63.9128 - synergy_loss: 16.8734 - class_loss: 0.0990\nEpoch 344/1000\n570/570 [==============================] - 10s 18ms/step - loss: 63.0893 - synergy_loss: 16.3488 - class_loss: 0.0966\nEpoch 345/1000\n570/570 [==============================] - 10s 18ms/step - loss: 62.5119 - synergy_loss: 16.0796 - class_loss: 0.0961\nEpoch 346/1000\n570/570 [==============================] - 10s 18ms/step - loss: 62.4143 - synergy_loss: 16.2113 - class_loss: 0.0971\nEpoch 347/1000\n570/570 [==============================] - 11s 19ms/step - loss: 62.9760 - synergy_loss: 17.0018 - class_loss: 0.1040\nEpoch 348/1000\n570/570 [==============================] - 10s 17ms/step - loss: 63.6524 - synergy_loss: 17.7127 - class_loss: 0.1037\nEpoch 349/1000\n570/570 [==============================] - 10s 17ms/step - loss: 62.9942 - synergy_loss: 17.2887 - class_loss: 0.1018\nEpoch 350/1000\n570/570 [==============================] - 11s 19ms/step - loss: 61.5281 - synergy_loss: 16.1077 - class_loss: 0.0918\nEpoch 351/1000\n570/570 [==============================] - 10s 17ms/step - loss: 61.3091 - synergy_loss: 16.1605 - class_loss: 0.0955\nEpoch 352/1000\n570/570 [==============================] - 10s 17ms/step - loss: 68.5177 - synergy_loss: 23.4115 - class_loss: 0.1117\nEpoch 353/1000\n570/570 [==============================] - 10s 18ms/step - loss: 69.5680 - synergy_loss: 23.4153 - class_loss: 0.1276\nEpoch 354/1000\n570/570 [==============================] - 10s 18ms/step - loss: 62.1842 - synergy_loss: 16.4322 - class_loss: 0.0984\nEpoch 355/1000\n570/570 [==============================] - 10s 18ms/step - loss: 60.7051 - synergy_loss: 15.3975 - class_loss: 0.0947\nEpoch 356/1000\n570/570 [==============================] - 11s 19ms/step - loss: 60.1010 - synergy_loss: 15.1961 - class_loss: 0.0905\nEpoch 357/1000\n570/570 [==============================] - 10s 17ms/step - loss: 62.5346 - synergy_loss: 17.7470 - class_loss: 0.0995\nEpoch 358/1000\n570/570 [==============================] - 10s 17ms/step - loss: 60.2454 - synergy_loss: 15.7106 - class_loss: 0.0913\nEpoch 359/1000\n570/570 [==============================] - 11s 18ms/step - loss: 59.9382 - synergy_loss: 15.7426 - class_loss: 0.0935\nEpoch 360/1000\n570/570 [==============================] - 10s 17ms/step - loss: 59.6609 - synergy_loss: 15.7272 - class_loss: 0.0936\nEpoch 361/1000\n570/570 [==============================] - 10s 17ms/step - loss: 60.4607 - synergy_loss: 16.7179 - class_loss: 0.0946\nEpoch 362/1000\n570/570 [==============================] - 11s 19ms/step - loss: 59.7160 - synergy_loss: 16.1528 - class_loss: 0.0942\nEpoch 363/1000\n570/570 [==============================] - 10s 17ms/step - loss: 59.5469 - synergy_loss: 16.1950 - class_loss: 0.0918\nEpoch 364/1000\n570/570 [==============================] - 10s 17ms/step - loss: 58.8718 - synergy_loss: 15.8248 - class_loss: 0.0902\nEpoch 365/1000\n570/570 [==============================] - 11s 19ms/step - loss: 59.0261 - synergy_loss: 16.2200 - class_loss: 0.0925\nEpoch 366/1000\n570/570 [==============================] - 10s 17ms/step - loss: 59.1182 - synergy_loss: 16.4474 - class_loss: 0.0886\nEpoch 367/1000\n570/570 [==============================] - 10s 18ms/step - loss: 59.2966 - synergy_loss: 16.7443 - class_loss: 0.0960\nEpoch 368/1000\n570/570 [==============================] - 10s 18ms/step - loss: 61.0136 - synergy_loss: 18.3931 - class_loss: 0.0973\nEpoch 369/1000\n570/570 [==============================] - 10s 18ms/step - loss: 57.9069 - synergy_loss: 15.6048 - class_loss: 0.0894\nEpoch 370/1000\n570/570 [==============================] - 10s 17ms/step - loss: 57.7732 - synergy_loss: 15.7416 - class_loss: 0.0884\nEpoch 371/1000\n570/570 [==============================] - 10s 18ms/step - loss: 57.4122 - synergy_loss: 15.6213 - class_loss: 0.0902\nEpoch 372/1000\n570/570 [==============================] - 10s 18ms/step - loss: 57.4477 - synergy_loss: 15.8644 - class_loss: 0.0903\nEpoch 373/1000\n570/570 [==============================] - 10s 17ms/step - loss: 57.1815 - synergy_loss: 15.7952 - class_loss: 0.0942\nEpoch 374/1000\n570/570 [==============================] - 10s 18ms/step - loss: 56.8865 - synergy_loss: 15.7173 - class_loss: 0.0940\nEpoch 375/1000\n570/570 [==============================] - 10s 18ms/step - loss: 57.1082 - synergy_loss: 16.1071 - class_loss: 0.0895\nEpoch 376/1000\n570/570 [==============================] - 10s 17ms/step - loss: 56.7895 - synergy_loss: 16.0021 - class_loss: 0.0852\nEpoch 377/1000\n570/570 [==============================] - 10s 17ms/step - loss: 57.1730 - synergy_loss: 16.4523 - class_loss: 0.0949\nEpoch 378/1000\n570/570 [==============================] - 10s 18ms/step - loss: 56.0123 - synergy_loss: 15.5123 - class_loss: 0.0885\nEpoch 379/1000\n570/570 [==============================] - 10s 17ms/step - loss: 56.5109 - synergy_loss: 16.1672 - class_loss: 0.0928\nEpoch 380/1000\n570/570 [==============================] - 10s 18ms/step - loss: 56.0175 - synergy_loss: 15.8651 - class_loss: 0.0848\nEpoch 381/1000\n570/570 [==============================] - 10s 18ms/step - loss: 57.3455 - synergy_loss: 17.2278 - class_loss: 0.0953\nEpoch 382/1000\n570/570 [==============================] - 10s 17ms/step - loss: 56.0048 - synergy_loss: 16.0865 - class_loss: 0.0881\nEpoch 383/1000\n570/570 [==============================] - 10s 18ms/step - loss: 54.9177 - synergy_loss: 15.2244 - class_loss: 0.0874\nEpoch 384/1000\n570/570 [==============================] - 10s 18ms/step - loss: 54.3616 - synergy_loss: 14.9439 - class_loss: 0.0855\nEpoch 385/1000\n570/570 [==============================] - 10s 17ms/step - loss: 54.8834 - synergy_loss: 15.6179 - class_loss: 0.0895\nEpoch 386/1000\n570/570 [==============================] - 10s 18ms/step - loss: 55.0794 - synergy_loss: 15.9668 - class_loss: 0.0900\nEpoch 387/1000\n570/570 [==============================] - 10s 18ms/step - loss: 54.2684 - synergy_loss: 15.3844 - class_loss: 0.0873\nEpoch 388/1000\n570/570 [==============================] - 10s 17ms/step - loss: 53.7868 - synergy_loss: 15.1325 - class_loss: 0.0839\nEpoch 389/1000\n570/570 [==============================] - 10s 18ms/step - loss: 54.6646 - synergy_loss: 16.0904 - class_loss: 0.0966\nEpoch 390/1000\n570/570 [==============================] - 10s 17ms/step - loss: 54.1768 - synergy_loss: 15.7476 - class_loss: 0.0879\nEpoch 391/1000\n570/570 [==============================] - 10s 18ms/step - loss: 53.5598 - synergy_loss: 15.3454 - class_loss: 0.0848\nEpoch 392/1000\n570/570 [==============================] - 10s 17ms/step - loss: 53.7710 - synergy_loss: 15.7496 - class_loss: 0.0851\nEpoch 393/1000\n570/570 [==============================] - 10s 17ms/step - loss: 54.3216 - synergy_loss: 16.2904 - class_loss: 0.0899\nEpoch 394/1000\n570/570 [==============================] - 10s 18ms/step - loss: 53.5089 - synergy_loss: 15.6167 - class_loss: 0.0878\nEpoch 395/1000\n570/570 [==============================] - 10s 17ms/step - loss: 52.3847 - synergy_loss: 14.7883 - class_loss: 0.0868\nEpoch 396/1000\n570/570 [==============================] - 10s 17ms/step - loss: 52.3040 - synergy_loss: 14.9241 - class_loss: 0.0842\nEpoch 397/1000\n570/570 [==============================] - 10s 18ms/step - loss: 52.2455 - synergy_loss: 15.0262 - class_loss: 0.0817\nEpoch 398/1000\n570/570 [==============================] - 10s 18ms/step - loss: 52.5533 - synergy_loss: 15.5177 - class_loss: 0.0830\nEpoch 399/1000\n570/570 [==============================] - 10s 17ms/step - loss: 53.1958 - synergy_loss: 16.1596 - class_loss: 0.0912\nEpoch 400/1000\n570/570 [==============================] - 10s 18ms/step - loss: 51.9954 - synergy_loss: 15.2257 - class_loss: 0.0865\nEpoch 401/1000\n570/570 [==============================] - 10s 17ms/step - loss: 52.4320 - synergy_loss: 15.8161 - class_loss: 0.0868\nEpoch 402/1000\n570/570 [==============================] - 10s 17ms/step - loss: 53.9344 - synergy_loss: 17.1492 - class_loss: 0.0919\nEpoch 403/1000\n570/570 [==============================] - 10s 18ms/step - loss: 51.3440 - synergy_loss: 14.8514 - class_loss: 0.0835\nEpoch 404/1000\n570/570 [==============================] - 10s 17ms/step - loss: 50.7423 - synergy_loss: 14.5084 - class_loss: 0.0794\nEpoch 405/1000\n570/570 [==============================] - 10s 17ms/step - loss: 50.8962 - synergy_loss: 14.8716 - class_loss: 0.0842\nEpoch 406/1000\n570/570 [==============================] - 10s 18ms/step - loss: 50.7157 - synergy_loss: 14.8821 - class_loss: 0.0846\nEpoch 407/1000\n570/570 [==============================] - 10s 17ms/step - loss: 51.1216 - synergy_loss: 15.3678 - class_loss: 0.0895\nEpoch 408/1000\n570/570 [==============================] - 10s 18ms/step - loss: 50.2333 - synergy_loss: 14.6846 - class_loss: 0.0844\nEpoch 409/1000\n570/570 [==============================] - 10s 17ms/step - loss: 50.2530 - synergy_loss: 14.8934 - class_loss: 0.0795\nEpoch 410/1000\n570/570 [==============================] - 11s 19ms/step - loss: 50.6078 - synergy_loss: 15.3074 - class_loss: 0.0875\nEpoch 411/1000\n570/570 [==============================] - 10s 18ms/step - loss: 50.5674 - synergy_loss: 15.4329 - class_loss: 0.0850\nEpoch 412/1000\n570/570 [==============================] - 10s 17ms/step - loss: 49.4933 - synergy_loss: 14.4936 - class_loss: 0.0832\nEpoch 413/1000\n570/570 [==============================] - 10s 18ms/step - loss: 50.5742 - synergy_loss: 15.6561 - class_loss: 0.0872\nEpoch 414/1000\n570/570 [==============================] - 10s 18ms/step - loss: 49.9832 - synergy_loss: 15.1829 - class_loss: 0.0774\nEpoch 415/1000\n570/570 [==============================] - 10s 17ms/step - loss: 49.8973 - synergy_loss: 15.1769 - class_loss: 0.0851\nEpoch 416/1000\n570/570 [==============================] - 10s 18ms/step - loss: 53.7570 - synergy_loss: 18.8289 - class_loss: 0.0965\nEpoch 417/1000\n570/570 [==============================] - 10s 18ms/step - loss: 49.9705 - synergy_loss: 15.1666 - class_loss: 0.0816\nEpoch 418/1000\n570/570 [==============================] - 10s 17ms/step - loss: 49.4725 - synergy_loss: 14.8941 - class_loss: 0.0828\nEpoch 419/1000\n570/570 [==============================] - 10s 18ms/step - loss: 48.4108 - synergy_loss: 14.1351 - class_loss: 0.0772\nEpoch 420/1000\n570/570 [==============================] - 10s 18ms/step - loss: 48.1304 - synergy_loss: 14.0892 - class_loss: 0.0823\nEpoch 421/1000\n570/570 [==============================] - 10s 17ms/step - loss: 48.3631 - synergy_loss: 14.5161 - class_loss: 0.0770\nEpoch 422/1000\n570/570 [==============================] - 11s 19ms/step - loss: 48.4740 - synergy_loss: 14.7606 - class_loss: 0.0793\nEpoch 423/1000\n570/570 [==============================] - 10s 17ms/step - loss: 48.4830 - synergy_loss: 14.8885 - class_loss: 0.0825\nEpoch 424/1000\n570/570 [==============================] - 10s 17ms/step - loss: 48.7407 - synergy_loss: 15.2643 - class_loss: 0.0788\nEpoch 425/1000\n570/570 [==============================] - 10s 18ms/step - loss: 48.1477 - synergy_loss: 14.8167 - class_loss: 0.0767\nEpoch 426/1000\n570/570 [==============================] - 10s 17ms/step - loss: 50.2140 - synergy_loss: 16.7696 - class_loss: 0.0906\nEpoch 427/1000\n570/570 [==============================] - 10s 17ms/step - loss: 48.3203 - synergy_loss: 14.9654 - class_loss: 0.0804\nEpoch 428/1000\n570/570 [==============================] - 10s 17ms/step - loss: 47.9360 - synergy_loss: 14.7759 - class_loss: 0.0797\nEpoch 429/1000\n570/570 [==============================] - 10s 18ms/step - loss: 47.6091 - synergy_loss: 14.6133 - class_loss: 0.0797\nEpoch 430/1000\n570/570 [==============================] - 10s 17ms/step - loss: 46.9150 - synergy_loss: 14.1155 - class_loss: 0.0775\nEpoch 431/1000\n570/570 [==============================] - 10s 18ms/step - loss: 47.3810 - synergy_loss: 14.7263 - class_loss: 0.0786\nEpoch 432/1000\n570/570 [==============================] - 10s 18ms/step - loss: 46.7115 - synergy_loss: 14.2494 - class_loss: 0.0737\nEpoch 433/1000\n570/570 [==============================] - 10s 18ms/step - loss: 46.1951 - synergy_loss: 13.9291 - class_loss: 0.0779\nEpoch 434/1000\n570/570 [==============================] - 10s 18ms/step - loss: 46.7880 - synergy_loss: 14.6544 - class_loss: 0.0796\nEpoch 435/1000\n570/570 [==============================] - 11s 19ms/step - loss: 46.4253 - synergy_loss: 14.4144 - class_loss: 0.0810\nEpoch 436/1000\n570/570 [==============================] - 10s 18ms/step - loss: 46.9378 - synergy_loss: 14.9386 - class_loss: 0.0818\nEpoch 437/1000\n570/570 [==============================] - 11s 19ms/step - loss: 49.6630 - synergy_loss: 17.4501 - class_loss: 0.0973\nEpoch 438/1000\n570/570 [==============================] - 11s 19ms/step - loss: 46.9768 - synergy_loss: 14.7742 - class_loss: 0.0805\nEpoch 439/1000\n570/570 [==============================] - 10s 18ms/step - loss: 46.1002 - synergy_loss: 14.1769 - class_loss: 0.0746\nEpoch 440/1000\n570/570 [==============================] - 10s 18ms/step - loss: 45.6400 - synergy_loss: 13.9530 - class_loss: 0.0787\nEpoch 441/1000\n570/570 [==============================] - 11s 19ms/step - loss: 45.2011 - synergy_loss: 13.7251 - class_loss: 0.0772\nEpoch 442/1000\n570/570 [==============================] - 10s 18ms/step - loss: 44.7942 - synergy_loss: 13.5576 - class_loss: 0.0735\nEpoch 443/1000\n570/570 [==============================] - 10s 18ms/step - loss: 45.5272 - synergy_loss: 14.3863 - class_loss: 0.0742\nEpoch 444/1000\n570/570 [==============================] - 11s 19ms/step - loss: 46.5950 - synergy_loss: 15.4971 - class_loss: 0.0785\nEpoch 445/1000\n570/570 [==============================] - 10s 18ms/step - loss: 47.3636 - synergy_loss: 16.0221 - class_loss: 0.0900\nEpoch 446/1000\n570/570 [==============================] - 10s 17ms/step - loss: 45.2315 - synergy_loss: 14.1141 - class_loss: 0.0780\nEpoch 447/1000\n570/570 [==============================] - 10s 18ms/step - loss: 45.0292 - synergy_loss: 14.1315 - class_loss: 0.0736\nEpoch 448/1000\n570/570 [==============================] - 10s 17ms/step - loss: 44.3819 - synergy_loss: 13.6960 - class_loss: 0.0732\nEpoch 449/1000\n570/570 [==============================] - 10s 17ms/step - loss: 44.6437 - synergy_loss: 14.1175 - class_loss: 0.0762\nEpoch 450/1000\n570/570 [==============================] - 10s 18ms/step - loss: 44.5125 - synergy_loss: 14.1071 - class_loss: 0.0778\nEpoch 451/1000\n570/570 [==============================] - 10s 17ms/step - loss: 44.1325 - synergy_loss: 13.8838 - class_loss: 0.0743\nEpoch 452/1000\n570/570 [==============================] - 10s 17ms/step - loss: 43.7659 - synergy_loss: 13.6984 - class_loss: 0.0747\nEpoch 453/1000\n570/570 [==============================] - 10s 18ms/step - loss: 46.1770 - synergy_loss: 15.9242 - class_loss: 0.0864\nEpoch 454/1000\n570/570 [==============================] - 10s 17ms/step - loss: 44.8690 - synergy_loss: 14.6729 - class_loss: 0.0730\nEpoch 455/1000\n570/570 [==============================] - 10s 17ms/step - loss: 43.9477 - synergy_loss: 13.9697 - class_loss: 0.0754\nEpoch 456/1000\n570/570 [==============================] - 10s 17ms/step - loss: 43.7792 - synergy_loss: 13.9623 - class_loss: 0.0719\nEpoch 457/1000\n570/570 [==============================] - 11s 18ms/step - loss: 43.5580 - synergy_loss: 13.9176 - class_loss: 0.0751\nEpoch 458/1000\n570/570 [==============================] - 10s 17ms/step - loss: 43.7366 - synergy_loss: 14.1953 - class_loss: 0.0749\nEpoch 459/1000\n570/570 [==============================] - 10s 18ms/step - loss: 43.3331 - synergy_loss: 13.9169 - class_loss: 0.0748\nEpoch 460/1000\n570/570 [==============================] - 10s 18ms/step - loss: 44.7107 - synergy_loss: 15.3200 - class_loss: 0.0807\nEpoch 461/1000\n570/570 [==============================] - 10s 17ms/step - loss: 44.0872 - synergy_loss: 14.5727 - class_loss: 0.0766\nEpoch 462/1000\n570/570 [==============================] - 10s 17ms/step - loss: 44.4377 - synergy_loss: 14.9379 - class_loss: 0.0850\nEpoch 463/1000\n570/570 [==============================] - 10s 18ms/step - loss: 43.0225 - synergy_loss: 13.7108 - class_loss: 0.0699\nEpoch 464/1000\n570/570 [==============================] - 10s 17ms/step - loss: 43.9314 - synergy_loss: 14.6864 - class_loss: 0.0793\nEpoch 465/1000\n570/570 [==============================] - 10s 17ms/step - loss: 44.3406 - synergy_loss: 15.0374 - class_loss: 0.0779\nEpoch 466/1000\n570/570 [==============================] - 10s 18ms/step - loss: 43.4836 - synergy_loss: 14.3442 - class_loss: 0.0738\nEpoch 467/1000\n570/570 [==============================] - 10s 17ms/step - loss: 43.6075 - synergy_loss: 14.3779 - class_loss: 0.0805\nEpoch 468/1000\n570/570 [==============================] - 10s 18ms/step - loss: 42.1223 - synergy_loss: 13.1829 - class_loss: 0.0757\nEpoch 469/1000\n570/570 [==============================] - 10s 18ms/step - loss: 42.4212 - synergy_loss: 13.6360 - class_loss: 0.0766\nEpoch 470/1000\n570/570 [==============================] - 10s 17ms/step - loss: 41.7438 - synergy_loss: 13.1870 - class_loss: 0.0693\nEpoch 471/1000\n570/570 [==============================] - 10s 17ms/step - loss: 41.9430 - synergy_loss: 13.5528 - class_loss: 0.0720\nEpoch 472/1000\n570/570 [==============================] - 10s 18ms/step - loss: 41.7217 - synergy_loss: 13.4914 - class_loss: 0.0700\nEpoch 473/1000\n570/570 [==============================] - 10s 17ms/step - loss: 42.4944 - synergy_loss: 14.2731 - class_loss: 0.0771\nEpoch 474/1000\n570/570 [==============================] - 10s 17ms/step - loss: 45.4981 - synergy_loss: 16.9596 - class_loss: 0.0910\nEpoch 475/1000\n570/570 [==============================] - 10s 17ms/step - loss: 44.5312 - synergy_loss: 15.9105 - class_loss: 0.0838\nEpoch 476/1000\n570/570 [==============================] - 10s 18ms/step - loss: 41.9333 - synergy_loss: 13.4281 - class_loss: 0.0752\nEpoch 477/1000\n570/570 [==============================] - 10s 17ms/step - loss: 41.3378 - synergy_loss: 13.1133 - class_loss: 0.0732\nEpoch 478/1000\n570/570 [==============================] - 10s 17ms/step - loss: 40.7580 - synergy_loss: 12.7893 - class_loss: 0.0689\nEpoch 479/1000\n570/570 [==============================] - 10s 18ms/step - loss: 40.9020 - synergy_loss: 13.1296 - class_loss: 0.0692\nEpoch 480/1000\n570/570 [==============================] - 10s 18ms/step - loss: 41.1979 - synergy_loss: 13.5498 - class_loss: 0.0686\nEpoch 481/1000\n570/570 [==============================] - 10s 18ms/step - loss: 41.1748 - synergy_loss: 13.6244 - class_loss: 0.0710\nEpoch 482/1000\n570/570 [==============================] - 10s 18ms/step - loss: 41.2924 - synergy_loss: 13.8077 - class_loss: 0.0737\nEpoch 483/1000\n570/570 [==============================] - 10s 17ms/step - loss: 41.8432 - synergy_loss: 14.4232 - class_loss: 0.0782\nEpoch 484/1000\n570/570 [==============================] - 10s 17ms/step - loss: 41.4581 - synergy_loss: 14.1005 - class_loss: 0.0732\nEpoch 485/1000\n570/570 [==============================] - 10s 18ms/step - loss: 40.9960 - synergy_loss: 13.7506 - class_loss: 0.0696\nEpoch 486/1000\n570/570 [==============================] - 10s 17ms/step - loss: 40.0873 - synergy_loss: 13.0388 - class_loss: 0.0713\nEpoch 487/1000\n570/570 [==============================] - 10s 17ms/step - loss: 40.3508 - synergy_loss: 13.3872 - class_loss: 0.0762\nEpoch 488/1000\n570/570 [==============================] - 10s 18ms/step - loss: 40.9225 - synergy_loss: 14.0308 - class_loss: 0.0727\nEpoch 489/1000\n570/570 [==============================] - 10s 18ms/step - loss: 40.0785 - synergy_loss: 13.3583 - class_loss: 0.0692\nEpoch 490/1000\n570/570 [==============================] - 10s 18ms/step - loss: 40.1745 - synergy_loss: 13.5461 - class_loss: 0.0721\nEpoch 491/1000\n570/570 [==============================] - 10s 18ms/step - loss: 40.0969 - synergy_loss: 13.5489 - class_loss: 0.0726\nEpoch 492/1000\n570/570 [==============================] - 10s 17ms/step - loss: 39.5560 - synergy_loss: 13.1278 - class_loss: 0.0703\nEpoch 493/1000\n570/570 [==============================] - 10s 18ms/step - loss: 44.1679 - synergy_loss: 17.4987 - class_loss: 0.0832\nEpoch 494/1000\n570/570 [==============================] - 10s 17ms/step - loss: 41.6752 - synergy_loss: 14.7468 - class_loss: 0.0773\nEpoch 495/1000\n570/570 [==============================] - 10s 18ms/step - loss: 39.7389 - synergy_loss: 13.0883 - class_loss: 0.0751\nEpoch 496/1000\n570/570 [==============================] - 10s 18ms/step - loss: 39.1585 - synergy_loss: 12.8032 - class_loss: 0.0705\nEpoch 497/1000\n570/570 [==============================] - 10s 17ms/step - loss: 40.8622 - synergy_loss: 14.4326 - class_loss: 0.0710\nEpoch 498/1000\n570/570 [==============================] - 10s 18ms/step - loss: 39.3786 - synergy_loss: 13.0829 - class_loss: 0.0682\nEpoch 499/1000\n570/570 [==============================] - 10s 17ms/step - loss: 39.0105 - synergy_loss: 12.9346 - class_loss: 0.0696\nEpoch 500/1000\n570/570 [==============================] - 10s 17ms/step - loss: 38.8458 - synergy_loss: 12.9121 - class_loss: 0.0703\nEpoch 501/1000\n570/570 [==============================] - 10s 18ms/step - loss: 38.6270 - synergy_loss: 12.8484 - class_loss: 0.0683\nEpoch 502/1000\n570/570 [==============================] - 10s 17ms/step - loss: 38.6665 - synergy_loss: 13.0306 - class_loss: 0.0706\nEpoch 503/1000\n570/570 [==============================] - 10s 17ms/step - loss: 39.0696 - synergy_loss: 13.4561 - class_loss: 0.0720\nEpoch 504/1000\n570/570 [==============================] - 10s 18ms/step - loss: 40.8663 - synergy_loss: 15.0729 - class_loss: 0.0804\nEpoch 505/1000\n570/570 [==============================] - 10s 18ms/step - loss: 39.1091 - synergy_loss: 13.3835 - class_loss: 0.0719\nEpoch 506/1000\n570/570 [==============================] - 10s 17ms/step - loss: 38.2463 - synergy_loss: 12.7902 - class_loss: 0.0675\nEpoch 507/1000\n570/570 [==============================] - 10s 18ms/step - loss: 38.2561 - synergy_loss: 12.9531 - class_loss: 0.0652\nEpoch 508/1000\n570/570 [==============================] - 10s 18ms/step - loss: 38.7976 - synergy_loss: 13.5366 - class_loss: 0.0686\nEpoch 509/1000\n570/570 [==============================] - 10s 17ms/step - loss: 38.4775 - synergy_loss: 13.3185 - class_loss: 0.0729\nEpoch 510/1000\n570/570 [==============================] - 10s 18ms/step - loss: 37.9722 - synergy_loss: 12.9328 - class_loss: 0.0662\nEpoch 511/1000\n570/570 [==============================] - 10s 18ms/step - loss: 38.1267 - synergy_loss: 13.1936 - class_loss: 0.0686\nEpoch 512/1000\n570/570 [==============================] - 10s 18ms/step - loss: 38.2388 - synergy_loss: 13.3153 - class_loss: 0.0755\nEpoch 513/1000\n570/570 [==============================] - 10s 18ms/step - loss: 40.1164 - synergy_loss: 15.1245 - class_loss: 0.0779\nEpoch 514/1000\n570/570 [==============================] - 10s 18ms/step - loss: 38.0049 - synergy_loss: 13.1224 - class_loss: 0.0665\nEpoch 515/1000\n570/570 [==============================] - 10s 17ms/step - loss: 37.4310 - synergy_loss: 12.7174 - class_loss: 0.0679\nEpoch 516/1000\n570/570 [==============================] - 10s 17ms/step - loss: 37.8510 - synergy_loss: 13.2803 - class_loss: 0.0672\nEpoch 517/1000\n570/570 [==============================] - 11s 19ms/step - loss: 37.6346 - synergy_loss: 13.1470 - class_loss: 0.0709\nEpoch 518/1000\n570/570 [==============================] - 10s 17ms/step - loss: 37.2810 - synergy_loss: 12.8952 - class_loss: 0.0703\nEpoch 519/1000\n570/570 [==============================] - 10s 17ms/step - loss: 37.3888 - synergy_loss: 13.0900 - class_loss: 0.0685\nEpoch 520/1000\n570/570 [==============================] - 11s 19ms/step - loss: 39.1048 - synergy_loss: 14.6521 - class_loss: 0.0759\nEpoch 521/1000\n570/570 [==============================] - 10s 17ms/step - loss: 40.7103 - synergy_loss: 15.9351 - class_loss: 0.0809\nEpoch 522/1000\n570/570 [==============================] - 10s 17ms/step - loss: 37.0959 - synergy_loss: 12.5915 - class_loss: 0.0705\nEpoch 523/1000\n570/570 [==============================] - 10s 18ms/step - loss: 36.6658 - synergy_loss: 12.3871 - class_loss: 0.0633\nEpoch 524/1000\n570/570 [==============================] - 10s 17ms/step - loss: 36.5092 - synergy_loss: 12.4001 - class_loss: 0.0670\nEpoch 525/1000\n570/570 [==============================] - 10s 17ms/step - loss: 36.5497 - synergy_loss: 12.5739 - class_loss: 0.0664\nEpoch 526/1000\n570/570 [==============================] - 10s 18ms/step - loss: 36.7464 - synergy_loss: 12.8811 - class_loss: 0.0633\nEpoch 527/1000\n570/570 [==============================] - 10s 17ms/step - loss: 38.0323 - synergy_loss: 14.0319 - class_loss: 0.0764\nEpoch 528/1000\n570/570 [==============================] - 10s 17ms/step - loss: 36.9342 - synergy_loss: 13.0723 - class_loss: 0.0710\nEpoch 529/1000\n570/570 [==============================] - 11s 19ms/step - loss: 36.7525 - synergy_loss: 13.0304 - class_loss: 0.0657\nEpoch 530/1000\n570/570 [==============================] - 10s 17ms/step - loss: 36.3986 - synergy_loss: 12.7744 - class_loss: 0.0672\nEpoch 531/1000\n570/570 [==============================] - 10s 17ms/step - loss: 36.5396 - synergy_loss: 13.0130 - class_loss: 0.0650\nEpoch 532/1000\n570/570 [==============================] - 10s 18ms/step - loss: 36.4011 - synergy_loss: 12.9364 - class_loss: 0.0671\nEpoch 533/1000\n570/570 [==============================] - 10s 18ms/step - loss: 36.0186 - synergy_loss: 12.6820 - class_loss: 0.0655\nEpoch 534/1000\n570/570 [==============================] - 10s 18ms/step - loss: 36.2493 - synergy_loss: 12.9733 - class_loss: 0.0692\nEpoch 535/1000\n570/570 [==============================] - 10s 17ms/step - loss: 35.8138 - synergy_loss: 12.6327 - class_loss: 0.0673\nEpoch 536/1000\n570/570 [==============================] - 10s 18ms/step - loss: 37.3147 - synergy_loss: 13.9892 - class_loss: 0.0715\nEpoch 537/1000\n570/570 [==============================] - 10s 17ms/step - loss: 35.7022 - synergy_loss: 12.5963 - class_loss: 0.0672\nEpoch 538/1000\n570/570 [==============================] - 10s 17ms/step - loss: 35.6280 - synergy_loss: 12.6766 - class_loss: 0.0619\nEpoch 539/1000\n570/570 [==============================] - 10s 18ms/step - loss: 35.5852 - synergy_loss: 12.7300 - class_loss: 0.0693\nEpoch 540/1000\n570/570 [==============================] - 10s 17ms/step - loss: 35.2923 - synergy_loss: 12.5131 - class_loss: 0.0675\nEpoch 541/1000\n570/570 [==============================] - 10s 18ms/step - loss: 35.6075 - synergy_loss: 12.8867 - class_loss: 0.0641\nEpoch 542/1000\n570/570 [==============================] - 10s 18ms/step - loss: 35.7762 - synergy_loss: 13.1022 - class_loss: 0.0694\nEpoch 543/1000\n570/570 [==============================] - 10s 17ms/step - loss: 36.8819 - synergy_loss: 14.0535 - class_loss: 0.0689\nEpoch 544/1000\n570/570 [==============================] - 10s 17ms/step - loss: 38.2726 - synergy_loss: 15.3266 - class_loss: 0.0763\nEpoch 545/1000\n570/570 [==============================] - 10s 18ms/step - loss: 35.5466 - synergy_loss: 12.5761 - class_loss: 0.0678\nEpoch 546/1000\n570/570 [==============================] - 10s 17ms/step - loss: 34.7871 - synergy_loss: 12.0743 - class_loss: 0.0646\nEpoch 547/1000\n570/570 [==============================] - 10s 18ms/step - loss: 34.7189 - synergy_loss: 12.1827 - class_loss: 0.0652\nEpoch 548/1000\n570/570 [==============================] - 10s 18ms/step - loss: 34.5635 - synergy_loss: 12.1936 - class_loss: 0.0610\nEpoch 549/1000\n570/570 [==============================] - 10s 18ms/step - loss: 35.1007 - synergy_loss: 12.7363 - class_loss: 0.0655\nEpoch 550/1000\n570/570 [==============================] - 10s 18ms/step - loss: 34.8848 - synergy_loss: 12.6464 - class_loss: 0.0598\nEpoch 551/1000\n570/570 [==============================] - 10s 18ms/step - loss: 34.9270 - synergy_loss: 12.7544 - class_loss: 0.0653\nEpoch 552/1000\n570/570 [==============================] - 10s 18ms/step - loss: 34.5442 - synergy_loss: 12.4505 - class_loss: 0.0628\nEpoch 553/1000\n570/570 [==============================] - 10s 18ms/step - loss: 34.5084 - synergy_loss: 12.5049 - class_loss: 0.0647\nEpoch 554/1000\n570/570 [==============================] - 11s 18ms/step - loss: 34.8679 - synergy_loss: 12.8779 - class_loss: 0.0682\nEpoch 555/1000\n570/570 [==============================] - 11s 19ms/step - loss: 35.1278 - synergy_loss: 13.1735 - class_loss: 0.0658\nEpoch 556/1000\n570/570 [==============================] - 10s 18ms/step - loss: 34.5676 - synergy_loss: 12.6924 - class_loss: 0.0642\nEpoch 557/1000\n570/570 [==============================] - 10s 18ms/step - loss: 34.3034 - synergy_loss: 12.5484 - class_loss: 0.0612\nEpoch 558/1000\n570/570 [==============================] - 11s 19ms/step - loss: 34.9565 - synergy_loss: 13.1387 - class_loss: 0.0668\nEpoch 559/1000\n570/570 [==============================] - 10s 18ms/step - loss: 42.0806 - synergy_loss: 19.8586 - class_loss: 0.0885\nEpoch 560/1000\n570/570 [==============================] - 10s 18ms/step - loss: 35.7106 - synergy_loss: 13.2498 - class_loss: 0.0712\nEpoch 561/1000\n570/570 [==============================] - 11s 18ms/step - loss: 34.6276 - synergy_loss: 12.3958 - class_loss: 0.0697\nEpoch 562/1000\n570/570 [==============================] - 10s 18ms/step - loss: 33.9161 - synergy_loss: 11.9033 - class_loss: 0.0639\nEpoch 563/1000\n570/570 [==============================] - 10s 18ms/step - loss: 33.4950 - synergy_loss: 11.6976 - class_loss: 0.0615\nEpoch 564/1000\n570/570 [==============================] - 10s 18ms/step - loss: 34.3976 - synergy_loss: 12.6520 - class_loss: 0.0639\nEpoch 565/1000\n570/570 [==============================] - 10s 18ms/step - loss: 33.5935 - synergy_loss: 11.9891 - class_loss: 0.0624\nEpoch 566/1000\n570/570 [==============================] - 10s 17ms/step - loss: 34.0163 - synergy_loss: 12.4907 - class_loss: 0.0659\nEpoch 567/1000\n570/570 [==============================] - 10s 18ms/step - loss: 35.3269 - synergy_loss: 13.6768 - class_loss: 0.0724\nEpoch 568/1000\n570/570 [==============================] - 10s 17ms/step - loss: 34.1005 - synergy_loss: 12.5982 - class_loss: 0.0625\nEpoch 569/1000\n570/570 [==============================] - 10s 17ms/step - loss: 33.4529 - synergy_loss: 12.0941 - class_loss: 0.0637\nEpoch 570/1000\n570/570 [==============================] - 10s 18ms/step - loss: 33.7030 - synergy_loss: 12.4527 - class_loss: 0.0628\nEpoch 571/1000\n570/570 [==============================] - 10s 17ms/step - loss: 35.3569 - synergy_loss: 13.9819 - class_loss: 0.0641\nEpoch 572/1000\n570/570 [==============================] - 10s 17ms/step - loss: 34.0590 - synergy_loss: 12.8115 - class_loss: 0.0651\nEpoch 573/1000\n570/570 [==============================] - 10s 18ms/step - loss: 33.6059 - synergy_loss: 12.4217 - class_loss: 0.0637\nEpoch 574/1000\n570/570 [==============================] - 10s 17ms/step - loss: 33.3956 - synergy_loss: 12.3259 - class_loss: 0.0639\nEpoch 575/1000\n570/570 [==============================] - 10s 17ms/step - loss: 33.2452 - synergy_loss: 12.2774 - class_loss: 0.0654\nEpoch 576/1000\n570/570 [==============================] - 10s 18ms/step - loss: 33.1365 - synergy_loss: 12.2639 - class_loss: 0.0630\nEpoch 577/1000\n570/570 [==============================] - 10s 18ms/step - loss: 33.1161 - synergy_loss: 12.3441 - class_loss: 0.0626\nEpoch 578/1000\n570/570 [==============================] - 10s 17ms/step - loss: 34.4244 - synergy_loss: 13.4826 - class_loss: 0.0713\nEpoch 579/1000\n570/570 [==============================] - 10s 17ms/step - loss: 33.2357 - synergy_loss: 12.4109 - class_loss: 0.0612\nEpoch 580/1000\n570/570 [==============================] - 10s 18ms/step - loss: 32.9187 - synergy_loss: 12.2422 - class_loss: 0.0627\nEpoch 581/1000\n570/570 [==============================] - 10s 17ms/step - loss: 32.7365 - synergy_loss: 12.1691 - class_loss: 0.0617\nEpoch 582/1000\n570/570 [==============================] - 10s 17ms/step - loss: 33.8928 - synergy_loss: 13.2438 - class_loss: 0.0671\nEpoch 583/1000\n570/570 [==============================] - 10s 18ms/step - loss: 36.4395 - synergy_loss: 15.4076 - class_loss: 0.0786\nEpoch 584/1000\n570/570 [==============================] - 10s 17ms/step - loss: 32.6442 - synergy_loss: 11.8292 - class_loss: 0.0615\nEpoch 585/1000\n570/570 [==============================] - 10s 17ms/step - loss: 33.1462 - synergy_loss: 12.5342 - class_loss: 0.0635\nEpoch 586/1000\n570/570 [==============================] - 10s 18ms/step - loss: 32.6219 - synergy_loss: 12.0081 - class_loss: 0.0648\nEpoch 587/1000\n570/570 [==============================] - 10s 17ms/step - loss: 32.4095 - synergy_loss: 11.9534 - class_loss: 0.0625\nEpoch 588/1000\n570/570 [==============================] - 10s 17ms/step - loss: 32.7148 - synergy_loss: 12.2984 - class_loss: 0.0610\nEpoch 589/1000\n570/570 [==============================] - 11s 19ms/step - loss: 36.2960 - synergy_loss: 15.7538 - class_loss: 0.0777\nEpoch 590/1000\n570/570 [==============================] - 10s 17ms/step - loss: 35.2135 - synergy_loss: 14.1253 - class_loss: 0.0746\nEpoch 591/1000\n570/570 [==============================] - 10s 17ms/step - loss: 32.5253 - synergy_loss: 11.7322 - class_loss: 0.0614\nEpoch 592/1000\n570/570 [==============================] - 10s 18ms/step - loss: 31.7582 - synergy_loss: 11.2407 - class_loss: 0.0576\nEpoch 593/1000\n570/570 [==============================] - 10s 18ms/step - loss: 31.8296 - synergy_loss: 11.4990 - class_loss: 0.0566\nEpoch 594/1000\n570/570 [==============================] - 10s 17ms/step - loss: 31.7969 - synergy_loss: 11.6143 - class_loss: 0.0594\nEpoch 595/1000\n570/570 [==============================] - 10s 18ms/step - loss: 32.3617 - synergy_loss: 12.2205 - class_loss: 0.0585\nEpoch 596/1000\n570/570 [==============================] - 10s 18ms/step - loss: 32.1655 - synergy_loss: 12.1158 - class_loss: 0.0589\nEpoch 597/1000\n570/570 [==============================] - 10s 17ms/step - loss: 32.1129 - synergy_loss: 12.1280 - class_loss: 0.0633\nEpoch 598/1000\n570/570 [==============================] - 10s 18ms/step - loss: 32.4393 - synergy_loss: 12.5180 - class_loss: 0.0604\nEpoch 599/1000\n570/570 [==============================] - 10s 18ms/step - loss: 32.5021 - synergy_loss: 12.5284 - class_loss: 0.0614\nEpoch 600/1000\n570/570 [==============================] - 10s 18ms/step - loss: 32.2409 - synergy_loss: 12.3267 - class_loss: 0.0632\nEpoch 601/1000\n570/570 [==============================] - 10s 18ms/step - loss: 32.1227 - synergy_loss: 12.3062 - class_loss: 0.0612\nEpoch 602/1000\n570/570 [==============================] - 10s 18ms/step - loss: 34.5828 - synergy_loss: 14.5761 - class_loss: 0.0751\nEpoch 603/1000\n570/570 [==============================] - 10s 17ms/step - loss: 32.6795 - synergy_loss: 12.5634 - class_loss: 0.0642\nEpoch 604/1000\n570/570 [==============================] - 10s 17ms/step - loss: 32.8375 - synergy_loss: 12.7680 - class_loss: 0.0648\nEpoch 605/1000\n570/570 [==============================] - 10s 18ms/step - loss: 31.6431 - synergy_loss: 11.7830 - class_loss: 0.0590\nEpoch 606/1000\n570/570 [==============================] - 10s 17ms/step - loss: 31.1353 - synergy_loss: 11.4483 - class_loss: 0.0594\nEpoch 607/1000\n570/570 [==============================] - 10s 17ms/step - loss: 31.3093 - synergy_loss: 11.7121 - class_loss: 0.0607\nEpoch 608/1000\n570/570 [==============================] - 10s 18ms/step - loss: 31.4264 - synergy_loss: 11.9293 - class_loss: 0.0608\nEpoch 609/1000\n570/570 [==============================] - 10s 17ms/step - loss: 32.0268 - synergy_loss: 12.4979 - class_loss: 0.0640\nEpoch 610/1000\n570/570 [==============================] - 10s 17ms/step - loss: 32.1785 - synergy_loss: 12.6549 - class_loss: 0.0612\nEpoch 611/1000\n570/570 [==============================] - 11s 19ms/step - loss: 31.7312 - synergy_loss: 12.2297 - class_loss: 0.0615\nEpoch 612/1000\n570/570 [==============================] - 10s 18ms/step - loss: 30.9100 - synergy_loss: 11.6120 - class_loss: 0.0586\nEpoch 613/1000\n570/570 [==============================] - 11s 19ms/step - loss: 31.1719 - synergy_loss: 11.9419 - class_loss: 0.0632\nEpoch 614/1000\n570/570 [==============================] - 10s 18ms/step - loss: 32.0426 - synergy_loss: 12.7035 - class_loss: 0.0615\nEpoch 615/1000\n570/570 [==============================] - 11s 19ms/step - loss: 31.1777 - synergy_loss: 11.9534 - class_loss: 0.0606\nEpoch 616/1000\n570/570 [==============================] - 10s 18ms/step - loss: 36.5548 - synergy_loss: 16.7029 - class_loss: 0.0869\nEpoch 617/1000\n570/570 [==============================] - 11s 19ms/step - loss: 31.7602 - synergy_loss: 12.0061 - class_loss: 0.0636\nEpoch 618/1000\n570/570 [==============================] - 10s 18ms/step - loss: 30.6980 - synergy_loss: 11.2268 - class_loss: 0.0572\nEpoch 619/1000\n570/570 [==============================] - 10s 18ms/step - loss: 30.2853 - synergy_loss: 11.0449 - class_loss: 0.0609\nEpoch 620/1000\n570/570 [==============================] - 11s 19ms/step - loss: 30.6057 - synergy_loss: 11.4891 - class_loss: 0.0608\nEpoch 621/1000\n570/570 [==============================] - 10s 17ms/step - loss: 30.6130 - synergy_loss: 11.6043 - class_loss: 0.0548\nEpoch 622/1000\n570/570 [==============================] - 10s 17ms/step - loss: 31.2111 - synergy_loss: 12.2164 - class_loss: 0.0629\nEpoch 623/1000\n570/570 [==============================] - 10s 18ms/step - loss: 31.3483 - synergy_loss: 12.2980 - class_loss: 0.0599\nEpoch 624/1000\n570/570 [==============================] - 10s 18ms/step - loss: 30.8046 - synergy_loss: 11.8954 - class_loss: 0.0595\nEpoch 625/1000\n570/570 [==============================] - 10s 17ms/step - loss: 31.3781 - synergy_loss: 12.4566 - class_loss: 0.0569\nEpoch 626/1000\n570/570 [==============================] - 10s 18ms/step - loss: 31.2645 - synergy_loss: 12.3706 - class_loss: 0.0659\nEpoch 627/1000\n570/570 [==============================] - 10s 17ms/step - loss: 30.6097 - synergy_loss: 11.7652 - class_loss: 0.0640\nEpoch 628/1000\n570/570 [==============================] - 10s 17ms/step - loss: 30.0882 - synergy_loss: 11.4112 - class_loss: 0.0563\nEpoch 629/1000\n570/570 [==============================] - 10s 17ms/step - loss: 30.6288 - synergy_loss: 11.9591 - class_loss: 0.0591\nEpoch 630/1000\n570/570 [==============================] - 10s 18ms/step - loss: 31.1706 - synergy_loss: 12.4639 - class_loss: 0.0578\nEpoch 631/1000\n570/570 [==============================] - 10s 17ms/step - loss: 31.2506 - synergy_loss: 12.5936 - class_loss: 0.0633\nEpoch 632/1000\n570/570 [==============================] - 10s 17ms/step - loss: 30.8235 - synergy_loss: 12.1012 - class_loss: 0.0652\nEpoch 633/1000\n570/570 [==============================] - 10s 18ms/step - loss: 30.2856 - synergy_loss: 11.6845 - class_loss: 0.0606\nEpoch 634/1000\n570/570 [==============================] - 10s 17ms/step - loss: 30.5567 - synergy_loss: 12.0122 - class_loss: 0.0620\nEpoch 635/1000\n570/570 [==============================] - 10s 17ms/step - loss: 31.0516 - synergy_loss: 12.4863 - class_loss: 0.0592\nEpoch 636/1000\n570/570 [==============================] - 11s 19ms/step - loss: 30.0361 - synergy_loss: 11.5739 - class_loss: 0.0559\nEpoch 637/1000\n570/570 [==============================] - 10s 17ms/step - loss: 30.2542 - synergy_loss: 11.9023 - class_loss: 0.0577\nEpoch 638/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.9678 - synergy_loss: 11.6815 - class_loss: 0.0542\nEpoch 639/1000\n570/570 [==============================] - 10s 18ms/step - loss: 30.1450 - synergy_loss: 11.8380 - class_loss: 0.0639\nEpoch 640/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.6754 - synergy_loss: 11.4891 - class_loss: 0.0557\nEpoch 641/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.4689 - synergy_loss: 11.3985 - class_loss: 0.0569\nEpoch 642/1000\n570/570 [==============================] - 10s 18ms/step - loss: 30.0069 - synergy_loss: 11.9471 - class_loss: 0.0598\nEpoch 643/1000\n570/570 [==============================] - 10s 18ms/step - loss: 30.5017 - synergy_loss: 12.3709 - class_loss: 0.0626\nEpoch 644/1000\n570/570 [==============================] - 10s 18ms/step - loss: 30.1053 - synergy_loss: 12.0135 - class_loss: 0.0582\nEpoch 645/1000\n570/570 [==============================] - 10s 18ms/step - loss: 30.4615 - synergy_loss: 12.3100 - class_loss: 0.0661\nEpoch 646/1000\n570/570 [==============================] - 10s 17ms/step - loss: 31.2932 - synergy_loss: 13.0316 - class_loss: 0.0634\nEpoch 647/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.5069 - synergy_loss: 11.4313 - class_loss: 0.0556\nEpoch 648/1000\n570/570 [==============================] - 11s 19ms/step - loss: 29.5743 - synergy_loss: 11.5551 - class_loss: 0.0584\nEpoch 649/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.4210 - synergy_loss: 11.5094 - class_loss: 0.0565\nEpoch 650/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.2974 - synergy_loss: 11.4933 - class_loss: 0.0547\nEpoch 651/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.8611 - synergy_loss: 12.0021 - class_loss: 0.0603\nEpoch 652/1000\n570/570 [==============================] - 11s 19ms/step - loss: 29.7445 - synergy_loss: 11.8732 - class_loss: 0.0578\nEpoch 653/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.2451 - synergy_loss: 11.4823 - class_loss: 0.0593\nEpoch 654/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.9290 - synergy_loss: 11.2791 - class_loss: 0.0583\nEpoch 655/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.7979 - synergy_loss: 11.2575 - class_loss: 0.0570\nEpoch 656/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.6783 - synergy_loss: 12.0239 - class_loss: 0.0596\nEpoch 657/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.3419 - synergy_loss: 11.7399 - class_loss: 0.0560\nEpoch 658/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.3783 - synergy_loss: 11.8090 - class_loss: 0.0603\nEpoch 659/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.6244 - synergy_loss: 12.0456 - class_loss: 0.0606\nEpoch 660/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.6396 - synergy_loss: 11.1802 - class_loss: 0.0536\nEpoch 661/1000\n570/570 [==============================] - 11s 19ms/step - loss: 34.9301 - synergy_loss: 16.9457 - class_loss: 0.0801\nEpoch 662/1000\n570/570 [==============================] - 10s 18ms/step - loss: 31.7056 - synergy_loss: 13.3841 - class_loss: 0.0742\nEpoch 663/1000\n570/570 [==============================] - 10s 18ms/step - loss: 30.2761 - synergy_loss: 12.1180 - class_loss: 0.0647\nEpoch 664/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.0492 - synergy_loss: 11.0726 - class_loss: 0.0570\nEpoch 665/1000\n570/570 [==============================] - 10s 17ms/step - loss: 28.7127 - synergy_loss: 10.9325 - class_loss: 0.0526\nEpoch 666/1000\n570/570 [==============================] - 10s 17ms/step - loss: 28.1691 - synergy_loss: 10.6211 - class_loss: 0.0500\nEpoch 667/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.3593 - synergy_loss: 10.9339 - class_loss: 0.0556\nEpoch 668/1000\n570/570 [==============================] - 10s 17ms/step - loss: 28.6306 - synergy_loss: 11.2950 - class_loss: 0.0551\nEpoch 669/1000\n570/570 [==============================] - 10s 17ms/step - loss: 28.7255 - synergy_loss: 11.4493 - class_loss: 0.0569\nEpoch 670/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.5707 - synergy_loss: 11.3538 - class_loss: 0.0588\nEpoch 671/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.2089 - synergy_loss: 11.9632 - class_loss: 0.0575\nEpoch 672/1000\n570/570 [==============================] - 10s 17ms/step - loss: 28.5562 - synergy_loss: 11.4016 - class_loss: 0.0580\nEpoch 673/1000\n570/570 [==============================] - 11s 19ms/step - loss: 28.5912 - synergy_loss: 11.4786 - class_loss: 0.0573\nEpoch 674/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.0996 - synergy_loss: 11.9582 - class_loss: 0.0546\nEpoch 675/1000\n570/570 [==============================] - 10s 17ms/step - loss: 28.4726 - synergy_loss: 11.3956 - class_loss: 0.0585\nEpoch 676/1000\n570/570 [==============================] - 11s 19ms/step - loss: 28.1678 - synergy_loss: 11.1932 - class_loss: 0.0586\nEpoch 677/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.6806 - synergy_loss: 11.7285 - class_loss: 0.0563\nEpoch 678/1000\n570/570 [==============================] - 10s 17ms/step - loss: 28.3833 - synergy_loss: 11.4395 - class_loss: 0.0566\nEpoch 679/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.1780 - synergy_loss: 11.3421 - class_loss: 0.0571\nEpoch 680/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.0421 - synergy_loss: 12.1279 - class_loss: 0.0617\nEpoch 681/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.6072 - synergy_loss: 11.6690 - class_loss: 0.0625\nEpoch 682/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.9144 - synergy_loss: 11.1253 - class_loss: 0.0544\nEpoch 683/1000\n570/570 [==============================] - 11s 19ms/step - loss: 27.9345 - synergy_loss: 11.2235 - class_loss: 0.0529\nEpoch 684/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.1744 - synergy_loss: 12.2934 - class_loss: 0.0625\nEpoch 685/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.0833 - synergy_loss: 12.1178 - class_loss: 0.0619\nEpoch 686/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.8042 - synergy_loss: 11.8386 - class_loss: 0.0602\nEpoch 687/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.8747 - synergy_loss: 11.0718 - class_loss: 0.0530\nEpoch 688/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.6088 - synergy_loss: 10.9559 - class_loss: 0.0546\nEpoch 689/1000\n570/570 [==============================] - 11s 18ms/step - loss: 27.5687 - synergy_loss: 11.0259 - class_loss: 0.0545\nEpoch 690/1000\n570/570 [==============================] - 10s 17ms/step - loss: 28.9620 - synergy_loss: 12.2547 - class_loss: 0.0592\nEpoch 691/1000\n570/570 [==============================] - 10s 18ms/step - loss: 32.1950 - synergy_loss: 15.2148 - class_loss: 0.0729\nEpoch 692/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.4927 - synergy_loss: 12.3014 - class_loss: 0.0651\nEpoch 693/1000\n570/570 [==============================] - 10s 17ms/step - loss: 27.6630 - synergy_loss: 10.7479 - class_loss: 0.0539\nEpoch 694/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.4822 - synergy_loss: 10.7248 - class_loss: 0.0551\nEpoch 695/1000\n570/570 [==============================] - 11s 19ms/step - loss: 27.0457 - synergy_loss: 10.4710 - class_loss: 0.0540\nEpoch 696/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.6628 - synergy_loss: 11.1733 - class_loss: 0.0533\nEpoch 697/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.9084 - synergy_loss: 12.3447 - class_loss: 0.0600\nEpoch 698/1000\n570/570 [==============================] - 10s 18ms/step - loss: 31.5663 - synergy_loss: 14.3951 - class_loss: 0.0757\nEpoch 699/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.9435 - synergy_loss: 11.0073 - class_loss: 0.0547\nEpoch 700/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.7965 - synergy_loss: 11.0409 - class_loss: 0.0550\nEpoch 701/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.5210 - synergy_loss: 10.8804 - class_loss: 0.0538\nEpoch 702/1000\n570/570 [==============================] - 10s 17ms/step - loss: 27.2102 - synergy_loss: 10.7325 - class_loss: 0.0558\nEpoch 703/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.1950 - synergy_loss: 10.8374 - class_loss: 0.0530\nEpoch 704/1000\n570/570 [==============================] - 11s 19ms/step - loss: 27.2396 - synergy_loss: 10.9892 - class_loss: 0.0551\nEpoch 705/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.3735 - synergy_loss: 11.1470 - class_loss: 0.0582\nEpoch 706/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.0392 - synergy_loss: 12.7498 - class_loss: 0.0601\nEpoch 707/1000\n570/570 [==============================] - 11s 19ms/step - loss: 28.3572 - synergy_loss: 11.8363 - class_loss: 0.0589\nEpoch 708/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.4608 - synergy_loss: 11.1167 - class_loss: 0.0532\nEpoch 709/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.9749 - synergy_loss: 10.7683 - class_loss: 0.0546\nEpoch 710/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.9556 - synergy_loss: 10.8900 - class_loss: 0.0539\nEpoch 711/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.1630 - synergy_loss: 11.1249 - class_loss: 0.0546\nEpoch 712/1000\n570/570 [==============================] - 10s 18ms/step - loss: 29.1286 - synergy_loss: 12.8995 - class_loss: 0.0627\nEpoch 713/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.6218 - synergy_loss: 11.3643 - class_loss: 0.0558\nEpoch 714/1000\n570/570 [==============================] - 11s 18ms/step - loss: 27.0227 - synergy_loss: 10.9361 - class_loss: 0.0531\nEpoch 715/1000\n570/570 [==============================] - 10s 18ms/step - loss: 30.7099 - synergy_loss: 14.4255 - class_loss: 0.0713\nEpoch 716/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.4741 - synergy_loss: 11.9814 - class_loss: 0.0585\nEpoch 717/1000\n570/570 [==============================] - 11s 19ms/step - loss: 27.3071 - synergy_loss: 11.0212 - class_loss: 0.0513\nEpoch 718/1000\n570/570 [==============================] - 11s 18ms/step - loss: 26.3599 - synergy_loss: 10.2949 - class_loss: 0.0520\nEpoch 719/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.9444 - synergy_loss: 10.9615 - class_loss: 0.0557\nEpoch 720/1000\n570/570 [==============================] - 11s 18ms/step - loss: 26.7529 - synergy_loss: 10.8137 - class_loss: 0.0546\nEpoch 721/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.1806 - synergy_loss: 11.2615 - class_loss: 0.0553\nEpoch 722/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.1257 - synergy_loss: 11.2569 - class_loss: 0.0529\nEpoch 723/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.7519 - synergy_loss: 10.9798 - class_loss: 0.0523\nEpoch 724/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.9345 - synergy_loss: 11.1617 - class_loss: 0.0541\nEpoch 725/1000\n570/570 [==============================] - 10s 17ms/step - loss: 27.2441 - synergy_loss: 11.4590 - class_loss: 0.0581\nEpoch 726/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.5007 - synergy_loss: 10.8085 - class_loss: 0.0560\nEpoch 727/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.7784 - synergy_loss: 11.1583 - class_loss: 0.0512\nEpoch 728/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.1688 - synergy_loss: 11.4918 - class_loss: 0.0607\nEpoch 729/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.7431 - synergy_loss: 11.1356 - class_loss: 0.0509\nEpoch 730/1000\n570/570 [==============================] - 10s 18ms/step - loss: 37.7418 - synergy_loss: 21.1727 - class_loss: 0.1113\nEpoch 731/1000\n570/570 [==============================] - 10s 18ms/step - loss: 30.4452 - synergy_loss: 13.6702 - class_loss: 0.0849\nEpoch 732/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.2119 - synergy_loss: 10.6696 - class_loss: 0.0652\nEpoch 733/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.8424 - synergy_loss: 10.5824 - class_loss: 0.0578\nEpoch 734/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.3032 - synergy_loss: 10.2755 - class_loss: 0.0578\nEpoch 735/1000\n570/570 [==============================] - 11s 18ms/step - loss: 26.3080 - synergy_loss: 10.4662 - class_loss: 0.0555\nEpoch 736/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.8797 - synergy_loss: 11.1101 - class_loss: 0.0550\nEpoch 737/1000\n570/570 [==============================] - 10s 17ms/step - loss: 27.7099 - synergy_loss: 11.8735 - class_loss: 0.0625\nEpoch 738/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.5578 - synergy_loss: 11.7114 - class_loss: 0.0600\nEpoch 739/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.5489 - synergy_loss: 10.8344 - class_loss: 0.0557\nEpoch 740/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.3535 - synergy_loss: 10.7422 - class_loss: 0.0544\nEpoch 741/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.2504 - synergy_loss: 10.7678 - class_loss: 0.0511\nEpoch 742/1000\n570/570 [==============================] - 11s 19ms/step - loss: 26.3216 - synergy_loss: 10.8896 - class_loss: 0.0525\nEpoch 743/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.3500 - synergy_loss: 10.9774 - class_loss: 0.0537\nEpoch 744/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.4853 - synergy_loss: 11.1630 - class_loss: 0.0526\nEpoch 745/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.1957 - synergy_loss: 10.9184 - class_loss: 0.0525\nEpoch 746/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.9740 - synergy_loss: 10.7693 - class_loss: 0.0533\nEpoch 747/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.4152 - synergy_loss: 11.2107 - class_loss: 0.0561\nEpoch 748/1000\n570/570 [==============================] - 11s 18ms/step - loss: 26.5445 - synergy_loss: 11.2811 - class_loss: 0.0556\nEpoch 749/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.4222 - synergy_loss: 13.0101 - class_loss: 0.0648\nEpoch 750/1000\n570/570 [==============================] - 10s 18ms/step - loss: 31.8903 - synergy_loss: 15.8707 - class_loss: 0.0806\nEpoch 751/1000\n570/570 [==============================] - 11s 18ms/step - loss: 27.8811 - synergy_loss: 11.7639 - class_loss: 0.0622\nEpoch 752/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.9941 - synergy_loss: 10.2026 - class_loss: 0.0498\nEpoch 753/1000\n570/570 [==============================] - 10s 17ms/step - loss: 25.7849 - synergy_loss: 10.2323 - class_loss: 0.0492\nEpoch 754/1000\n570/570 [==============================] - 11s 19ms/step - loss: 25.7456 - synergy_loss: 10.3691 - class_loss: 0.0521\nEpoch 755/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.5418 - synergy_loss: 10.3063 - class_loss: 0.0481\nEpoch 756/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.7459 - synergy_loss: 10.6030 - class_loss: 0.0516\nEpoch 757/1000\n570/570 [==============================] - 11s 19ms/step - loss: 26.4443 - synergy_loss: 11.2419 - class_loss: 0.0556\nEpoch 758/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.0525 - synergy_loss: 10.9103 - class_loss: 0.0566\nEpoch 759/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.2250 - synergy_loss: 11.1108 - class_loss: 0.0531\nEpoch 760/1000\n570/570 [==============================] - 11s 18ms/step - loss: 26.3316 - synergy_loss: 11.2534 - class_loss: 0.0530\nEpoch 761/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.6273 - synergy_loss: 10.6522 - class_loss: 0.0502\nEpoch 762/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.8272 - synergy_loss: 10.8693 - class_loss: 0.0519\nEpoch 763/1000\n570/570 [==============================] - 11s 19ms/step - loss: 25.7569 - synergy_loss: 10.8454 - class_loss: 0.0538\nEpoch 764/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.9068 - synergy_loss: 10.9870 - class_loss: 0.0513\nEpoch 765/1000\n570/570 [==============================] - 11s 18ms/step - loss: 26.8888 - synergy_loss: 11.9356 - class_loss: 0.0614\nEpoch 766/1000\n570/570 [==============================] - 11s 19ms/step - loss: 26.3426 - synergy_loss: 11.2022 - class_loss: 0.0576\nEpoch 767/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.2578 - synergy_loss: 10.3450 - class_loss: 0.0518\nEpoch 768/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.4785 - synergy_loss: 10.6563 - class_loss: 0.0509\nEpoch 769/1000\n570/570 [==============================] - 11s 18ms/step - loss: 25.0009 - synergy_loss: 10.2908 - class_loss: 0.0481\nEpoch 770/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.5308 - synergy_loss: 10.8067 - class_loss: 0.0532\nEpoch 771/1000\n570/570 [==============================] - 10s 17ms/step - loss: 25.7097 - synergy_loss: 10.9711 - class_loss: 0.0568\nEpoch 772/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.5594 - synergy_loss: 10.8719 - class_loss: 0.0514\nEpoch 773/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.7712 - synergy_loss: 12.8348 - class_loss: 0.0589\nEpoch 774/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.6315 - synergy_loss: 10.7824 - class_loss: 0.0512\nEpoch 775/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.9658 - synergy_loss: 10.2950 - class_loss: 0.0515\nEpoch 776/1000\n570/570 [==============================] - 11s 19ms/step - loss: 24.8100 - synergy_loss: 10.2314 - class_loss: 0.0543\nEpoch 777/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.0813 - synergy_loss: 10.5652 - class_loss: 0.0469\nEpoch 778/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.0026 - synergy_loss: 10.5449 - class_loss: 0.0508\nEpoch 779/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.3463 - synergy_loss: 11.7517 - class_loss: 0.0540\nEpoch 780/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.7009 - synergy_loss: 11.0565 - class_loss: 0.0519\nEpoch 781/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.3192 - synergy_loss: 10.7564 - class_loss: 0.0564\nEpoch 782/1000\n570/570 [==============================] - 11s 19ms/step - loss: 25.4417 - synergy_loss: 10.8964 - class_loss: 0.0559\nEpoch 783/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.9343 - synergy_loss: 10.4495 - class_loss: 0.0492\nEpoch 784/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.4682 - synergy_loss: 10.9952 - class_loss: 0.0527\nEpoch 785/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.3769 - synergy_loss: 10.9286 - class_loss: 0.0530\nEpoch 786/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.4301 - synergy_loss: 10.9565 - class_loss: 0.0542\nEpoch 787/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.6487 - synergy_loss: 11.1298 - class_loss: 0.0541\nEpoch 788/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.2711 - synergy_loss: 10.8311 - class_loss: 0.0539\nEpoch 789/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.9315 - synergy_loss: 10.5421 - class_loss: 0.0492\nEpoch 790/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.8080 - synergy_loss: 10.4721 - class_loss: 0.0499\nEpoch 791/1000\n570/570 [==============================] - 11s 18ms/step - loss: 25.0802 - synergy_loss: 10.7745 - class_loss: 0.0525\nEpoch 792/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.6997 - synergy_loss: 10.4576 - class_loss: 0.0487\nEpoch 793/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.0322 - synergy_loss: 10.7809 - class_loss: 0.0518\nEpoch 794/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.7469 - synergy_loss: 10.5401 - class_loss: 0.0509\nEpoch 795/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.8388 - synergy_loss: 10.6821 - class_loss: 0.0516\nEpoch 796/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.7060 - synergy_loss: 10.5714 - class_loss: 0.0501\nEpoch 797/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.6813 - synergy_loss: 11.4089 - class_loss: 0.0567\nEpoch 798/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.1829 - synergy_loss: 10.9458 - class_loss: 0.0501\nEpoch 799/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.5321 - synergy_loss: 10.3850 - class_loss: 0.0497\nEpoch 800/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.3196 - synergy_loss: 10.3050 - class_loss: 0.0476\nEpoch 801/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.7442 - synergy_loss: 10.7108 - class_loss: 0.0518\nEpoch 802/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.6438 - synergy_loss: 10.6277 - class_loss: 0.0524\nEpoch 803/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.3542 - synergy_loss: 11.1716 - class_loss: 0.0552\nEpoch 804/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.2999 - synergy_loss: 10.2716 - class_loss: 0.0483\nEpoch 805/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.8082 - synergy_loss: 12.6686 - class_loss: 0.0668\nEpoch 806/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.3109 - synergy_loss: 13.4632 - class_loss: 0.0700\nEpoch 807/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.0949 - synergy_loss: 10.4656 - class_loss: 0.0534\nEpoch 808/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.3303 - synergy_loss: 9.9539 - class_loss: 0.0477\nEpoch 809/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.9780 - synergy_loss: 9.8095 - class_loss: 0.0471\nEpoch 810/1000\n570/570 [==============================] - 11s 18ms/step - loss: 25.4170 - synergy_loss: 11.3212 - class_loss: 0.0528\nEpoch 811/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.3807 - synergy_loss: 11.8695 - class_loss: 0.0652\nEpoch 812/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.6611 - synergy_loss: 10.3333 - class_loss: 0.0508\nEpoch 813/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.2284 - synergy_loss: 10.9063 - class_loss: 0.0567\nEpoch 814/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.2435 - synergy_loss: 10.0973 - class_loss: 0.0494\nEpoch 815/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.9397 - synergy_loss: 9.9549 - class_loss: 0.0485\nEpoch 816/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.5388 - synergy_loss: 10.5874 - class_loss: 0.0475\nEpoch 817/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.4649 - synergy_loss: 10.5328 - class_loss: 0.0486\nEpoch 818/1000\n570/570 [==============================] - 10s 17ms/step - loss: 24.1651 - synergy_loss: 10.3116 - class_loss: 0.0478\nEpoch 819/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.2506 - synergy_loss: 10.4376 - class_loss: 0.0475\nEpoch 820/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.0567 - synergy_loss: 10.3026 - class_loss: 0.0507\nEpoch 821/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.3371 - synergy_loss: 12.2553 - class_loss: 0.0658\nEpoch 822/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.9616 - synergy_loss: 11.6952 - class_loss: 0.0600\nEpoch 823/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.1678 - synergy_loss: 10.9480 - class_loss: 0.0554\nEpoch 824/1000\n570/570 [==============================] - 10s 18ms/step - loss: 27.6341 - synergy_loss: 13.1873 - class_loss: 0.0701\nEpoch 825/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.5140 - synergy_loss: 10.1789 - class_loss: 0.0470\nEpoch 826/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.0220 - synergy_loss: 9.9340 - class_loss: 0.0464\nEpoch 827/1000\n570/570 [==============================] - 10s 17ms/step - loss: 24.6017 - synergy_loss: 10.4852 - class_loss: 0.0526\nEpoch 828/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.9619 - synergy_loss: 10.0181 - class_loss: 0.0483\nEpoch 829/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.2486 - synergy_loss: 11.3093 - class_loss: 0.0579\nEpoch 830/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.4507 - synergy_loss: 11.2759 - class_loss: 0.0584\nEpoch 831/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.2772 - synergy_loss: 10.2684 - class_loss: 0.0491\nEpoch 832/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.6258 - synergy_loss: 10.5905 - class_loss: 0.0537\nEpoch 833/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.0691 - synergy_loss: 10.1713 - class_loss: 0.0510\nEpoch 834/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.7739 - synergy_loss: 10.0186 - class_loss: 0.0447\nEpoch 835/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.8157 - synergy_loss: 10.1541 - class_loss: 0.0487\nEpoch 836/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.0814 - synergy_loss: 10.4454 - class_loss: 0.0489\nEpoch 837/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.4129 - synergy_loss: 10.7645 - class_loss: 0.0500\nEpoch 838/1000\n570/570 [==============================] - 11s 19ms/step - loss: 24.2260 - synergy_loss: 10.5961 - class_loss: 0.0483\nEpoch 839/1000\n570/570 [==============================] - 10s 17ms/step - loss: 24.0048 - synergy_loss: 10.4198 - class_loss: 0.0489\nEpoch 840/1000\n570/570 [==============================] - 10s 17ms/step - loss: 24.0468 - synergy_loss: 10.4945 - class_loss: 0.0501\nEpoch 841/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.7955 - synergy_loss: 10.3116 - class_loss: 0.0491\nEpoch 842/1000\n570/570 [==============================] - 10s 17ms/step - loss: 23.8006 - synergy_loss: 10.3429 - class_loss: 0.0510\nEpoch 843/1000\n570/570 [==============================] - 10s 17ms/step - loss: 23.6146 - synergy_loss: 10.2236 - class_loss: 0.0454\nEpoch 844/1000\n570/570 [==============================] - 11s 19ms/step - loss: 23.8095 - synergy_loss: 10.4451 - class_loss: 0.0495\nEpoch 845/1000\n570/570 [==============================] - 10s 17ms/step - loss: 23.9957 - synergy_loss: 10.5878 - class_loss: 0.0491\nEpoch 846/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.8186 - synergy_loss: 10.4517 - class_loss: 0.0514\nEpoch 847/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.3494 - synergy_loss: 10.8959 - class_loss: 0.0512\nEpoch 848/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.8963 - synergy_loss: 14.8240 - class_loss: 0.0758\nEpoch 849/1000\n570/570 [==============================] - 10s 17ms/step - loss: 25.0764 - synergy_loss: 11.0215 - class_loss: 0.0531\nEpoch 850/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.5701 - synergy_loss: 9.8023 - class_loss: 0.0475\nEpoch 851/1000\n570/570 [==============================] - 10s 17ms/step - loss: 23.2294 - synergy_loss: 9.6638 - class_loss: 0.0457\nEpoch 852/1000\n570/570 [==============================] - 10s 17ms/step - loss: 23.0895 - synergy_loss: 9.6847 - class_loss: 0.0454\nEpoch 853/1000\n570/570 [==============================] - 11s 18ms/step - loss: 23.4997 - synergy_loss: 10.1470 - class_loss: 0.0481\nEpoch 854/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.2456 - synergy_loss: 10.8134 - class_loss: 0.0585\nEpoch 855/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.0483 - synergy_loss: 10.5302 - class_loss: 0.0525\nEpoch 856/1000\n570/570 [==============================] - 11s 18ms/step - loss: 23.5985 - synergy_loss: 10.2215 - class_loss: 0.0453\nEpoch 857/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.4071 - synergy_loss: 10.1094 - class_loss: 0.0475\nEpoch 858/1000\n570/570 [==============================] - 10s 17ms/step - loss: 23.2413 - synergy_loss: 10.0056 - class_loss: 0.0481\nEpoch 859/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.6596 - synergy_loss: 10.4440 - class_loss: 0.0463\nEpoch 860/1000\n570/570 [==============================] - 11s 19ms/step - loss: 23.8693 - synergy_loss: 10.6199 - class_loss: 0.0504\nEpoch 861/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.4217 - synergy_loss: 11.0784 - class_loss: 0.0580\nEpoch 862/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.7926 - synergy_loss: 14.8579 - class_loss: 0.0723\nEpoch 863/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.8544 - synergy_loss: 10.0801 - class_loss: 0.0489\nEpoch 864/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.3870 - synergy_loss: 11.7156 - class_loss: 0.0567\nEpoch 865/1000\n570/570 [==============================] - 10s 18ms/step - loss: 31.2183 - synergy_loss: 17.2984 - class_loss: 0.0699\nEpoch 866/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.7685 - synergy_loss: 12.2791 - class_loss: 0.0690\nEpoch 867/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.3570 - synergy_loss: 10.1133 - class_loss: 0.0530\nEpoch 868/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.4819 - synergy_loss: 9.5268 - class_loss: 0.0461\nEpoch 869/1000\n570/570 [==============================] - 11s 19ms/step - loss: 27.4377 - synergy_loss: 13.0115 - class_loss: 0.0781\nEpoch 870/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.5264 - synergy_loss: 11.1306 - class_loss: 0.0610\nEpoch 871/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.4932 - synergy_loss: 9.3826 - class_loss: 0.0462\nEpoch 872/1000\n570/570 [==============================] - 11s 18ms/step - loss: 23.8870 - synergy_loss: 9.9627 - class_loss: 0.0463\nEpoch 873/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.9370 - synergy_loss: 10.1154 - class_loss: 0.0497\nEpoch 874/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.6187 - synergy_loss: 9.9708 - class_loss: 0.0463\nEpoch 875/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.6172 - synergy_loss: 10.1003 - class_loss: 0.0453\nEpoch 876/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.3605 - synergy_loss: 9.9705 - class_loss: 0.0433\nEpoch 877/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.3931 - synergy_loss: 10.0806 - class_loss: 0.0489\nEpoch 878/1000\n570/570 [==============================] - 11s 19ms/step - loss: 23.5237 - synergy_loss: 10.2477 - class_loss: 0.0491\nEpoch 879/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.4870 - synergy_loss: 10.2753 - class_loss: 0.0481\nEpoch 880/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.0385 - synergy_loss: 10.8059 - class_loss: 0.0486\nEpoch 881/1000\n570/570 [==============================] - 11s 19ms/step - loss: 24.7310 - synergy_loss: 11.3002 - class_loss: 0.0573\nEpoch 882/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.2339 - synergy_loss: 10.0146 - class_loss: 0.0446\nEpoch 883/1000\n570/570 [==============================] - 11s 19ms/step - loss: 23.3860 - synergy_loss: 10.2097 - class_loss: 0.0497\nEpoch 884/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.1647 - synergy_loss: 10.0983 - class_loss: 0.0467\nEpoch 885/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.5746 - synergy_loss: 10.5226 - class_loss: 0.0483\nEpoch 886/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.6253 - synergy_loss: 11.3019 - class_loss: 0.0545\nEpoch 887/1000\n570/570 [==============================] - 11s 18ms/step - loss: 23.1748 - synergy_loss: 10.0124 - class_loss: 0.0467\nEpoch 888/1000\n570/570 [==============================] - 10s 17ms/step - loss: 22.8534 - synergy_loss: 9.8559 - class_loss: 0.0439\nEpoch 889/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.3184 - synergy_loss: 10.3100 - class_loss: 0.0464\nEpoch 890/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.0715 - synergy_loss: 10.0917 - class_loss: 0.0454\nEpoch 891/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.0591 - synergy_loss: 10.1607 - class_loss: 0.0455\nEpoch 892/1000\n570/570 [==============================] - 10s 17ms/step - loss: 23.0020 - synergy_loss: 10.1552 - class_loss: 0.0470\nEpoch 893/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.2258 - synergy_loss: 10.3783 - class_loss: 0.0512\nEpoch 894/1000\n570/570 [==============================] - 11s 18ms/step - loss: 23.4867 - synergy_loss: 10.5823 - class_loss: 0.0494\nEpoch 895/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.8072 - synergy_loss: 9.9956 - class_loss: 0.0470\nEpoch 896/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.7472 - synergy_loss: 10.0181 - class_loss: 0.0460\nEpoch 897/1000\n570/570 [==============================] - 11s 18ms/step - loss: 26.2424 - synergy_loss: 12.9043 - class_loss: 0.0715\nEpoch 898/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.0161 - synergy_loss: 10.7253 - class_loss: 0.0561\nEpoch 899/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.9319 - synergy_loss: 10.6409 - class_loss: 0.0500\nEpoch 900/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.9674 - synergy_loss: 9.8659 - class_loss: 0.0451\nEpoch 901/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.4300 - synergy_loss: 9.5118 - class_loss: 0.0482\nEpoch 902/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.5601 - synergy_loss: 9.7664 - class_loss: 0.0422\nEpoch 903/1000\n570/570 [==============================] - 11s 20ms/step - loss: 22.8871 - synergy_loss: 10.0982 - class_loss: 0.0467\nEpoch 904/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.6689 - synergy_loss: 9.9191 - class_loss: 0.0465\nEpoch 905/1000\n570/570 [==============================] - 11s 19ms/step - loss: 23.1783 - synergy_loss: 10.4257 - class_loss: 0.0508\nEpoch 906/1000\n570/570 [==============================] - 12s 21ms/step - loss: 23.3901 - synergy_loss: 10.6015 - class_loss: 0.0509\nEpoch 907/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.5029 - synergy_loss: 9.8213 - class_loss: 0.0448\nEpoch 908/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.5625 - synergy_loss: 9.9708 - class_loss: 0.0458\nEpoch 909/1000\n570/570 [==============================] - 11s 19ms/step - loss: 23.0332 - synergy_loss: 10.4275 - class_loss: 0.0499\nEpoch 910/1000\n570/570 [==============================] - 11s 19ms/step - loss: 23.3546 - synergy_loss: 10.6017 - class_loss: 0.0508\nEpoch 911/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.3809 - synergy_loss: 9.7920 - class_loss: 0.0437\nEpoch 912/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.1831 - synergy_loss: 9.6765 - class_loss: 0.0427\nEpoch 913/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.2760 - synergy_loss: 9.8395 - class_loss: 0.0468\nEpoch 914/1000\n570/570 [==============================] - 11s 19ms/step - loss: 23.2566 - synergy_loss: 10.6831 - class_loss: 0.0518\nEpoch 915/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.5350 - synergy_loss: 10.0163 - class_loss: 0.0483\nEpoch 916/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.8709 - synergy_loss: 10.3344 - class_loss: 0.0498\nEpoch 917/1000\n570/570 [==============================] - 11s 20ms/step - loss: 23.2333 - synergy_loss: 10.5954 - class_loss: 0.0524\nEpoch 918/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.2330 - synergy_loss: 9.7399 - class_loss: 0.0448\nEpoch 919/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.5091 - synergy_loss: 10.0434 - class_loss: 0.0474\nEpoch 920/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.7564 - synergy_loss: 10.2379 - class_loss: 0.0503\nEpoch 921/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.4323 - synergy_loss: 9.9936 - class_loss: 0.0426\nEpoch 922/1000\n570/570 [==============================] - 10s 18ms/step - loss: 21.9867 - synergy_loss: 9.6624 - class_loss: 0.0437\nEpoch 923/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.5157 - synergy_loss: 10.1407 - class_loss: 0.0514\nEpoch 924/1000\n570/570 [==============================] - 10s 18ms/step - loss: 31.3128 - synergy_loss: 17.7496 - class_loss: 0.1032\nEpoch 925/1000\n570/570 [==============================] - 10s 18ms/step - loss: 25.6928 - synergy_loss: 12.1643 - class_loss: 0.0712\nEpoch 926/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.9801 - synergy_loss: 9.6843 - class_loss: 0.0519\nEpoch 927/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.2903 - synergy_loss: 9.2698 - class_loss: 0.0437\nEpoch 928/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.2058 - synergy_loss: 9.3783 - class_loss: 0.0433\nEpoch 929/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.4287 - synergy_loss: 9.7280 - class_loss: 0.0426\nEpoch 930/1000\n570/570 [==============================] - 10s 18ms/step - loss: 26.4044 - synergy_loss: 13.2811 - class_loss: 0.0706\nEpoch 931/1000\n570/570 [==============================] - 10s 18ms/step - loss: 23.2797 - synergy_loss: 10.1556 - class_loss: 0.0531\nEpoch 932/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.2783 - synergy_loss: 9.4155 - class_loss: 0.0427\nEpoch 933/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.0271 - synergy_loss: 9.3680 - class_loss: 0.0437\nEpoch 934/1000\n570/570 [==============================] - 10s 18ms/step - loss: 21.9944 - synergy_loss: 9.4923 - class_loss: 0.0428\nEpoch 935/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.3552 - synergy_loss: 9.8954 - class_loss: 0.0462\nEpoch 936/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.8658 - synergy_loss: 10.3907 - class_loss: 0.0474\nEpoch 937/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.7960 - synergy_loss: 10.2591 - class_loss: 0.0500\nEpoch 938/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.2577 - synergy_loss: 9.8594 - class_loss: 0.0434\nEpoch 939/1000\n570/570 [==============================] - 10s 18ms/step - loss: 21.8903 - synergy_loss: 9.6012 - class_loss: 0.0461\nEpoch 940/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.0458 - synergy_loss: 9.8160 - class_loss: 0.0445\nEpoch 941/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.3010 - synergy_loss: 10.0709 - class_loss: 0.0457\nEpoch 942/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.2262 - synergy_loss: 10.0096 - class_loss: 0.0462\nEpoch 943/1000\n570/570 [==============================] - 10s 18ms/step - loss: 22.2512 - synergy_loss: 10.0596 - class_loss: 0.0464\nEpoch 944/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.2360 - synergy_loss: 10.0352 - class_loss: 0.0474\nEpoch 945/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.4351 - synergy_loss: 15.4436 - class_loss: 0.0818\nEpoch 946/1000\n570/570 [==============================] - 11s 19ms/step - loss: 26.6899 - synergy_loss: 13.6421 - class_loss: 0.0697\nEpoch 947/1000\n570/570 [==============================] - 11s 19ms/step - loss: 23.9972 - synergy_loss: 10.8418 - class_loss: 0.0608\nEpoch 948/1000\n570/570 [==============================] - 10s 18ms/step - loss: 28.3114 - synergy_loss: 14.9971 - class_loss: 0.0716\nEpoch 949/1000\n570/570 [==============================] - 10s 18ms/step - loss: 24.0462 - synergy_loss: 10.6114 - class_loss: 0.0609\nEpoch 950/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.4693 - synergy_loss: 9.3440 - class_loss: 0.0490\nEpoch 951/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.3620 - synergy_loss: 9.4482 - class_loss: 0.0460\nEpoch 952/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.1835 - synergy_loss: 9.3799 - class_loss: 0.0460\nEpoch 953/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.1936 - synergy_loss: 9.5228 - class_loss: 0.0437\nEpoch 954/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.1617 - synergy_loss: 9.5881 - class_loss: 0.0457\nEpoch 955/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.0923 - synergy_loss: 9.6485 - class_loss: 0.0422\nEpoch 956/1000\n570/570 [==============================] - 11s 20ms/step - loss: 22.2696 - synergy_loss: 9.8926 - class_loss: 0.0477\nEpoch 957/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.4351 - synergy_loss: 9.9968 - class_loss: 0.0484\nEpoch 958/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.1588 - synergy_loss: 9.8252 - class_loss: 0.0451\nEpoch 959/1000\n570/570 [==============================] - 11s 20ms/step - loss: 22.4086 - synergy_loss: 10.0764 - class_loss: 0.0499\nEpoch 960/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.1781 - synergy_loss: 9.9144 - class_loss: 0.0466\nEpoch 961/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.0026 - synergy_loss: 9.8142 - class_loss: 0.0439\nEpoch 962/1000\n570/570 [==============================] - 11s 19ms/step - loss: 23.3791 - synergy_loss: 11.0992 - class_loss: 0.0523\nEpoch 963/1000\n570/570 [==============================] - 11s 19ms/step - loss: 23.9158 - synergy_loss: 11.4217 - class_loss: 0.0582\nEpoch 964/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.0403 - synergy_loss: 9.6868 - class_loss: 0.0440\nEpoch 965/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.8754 - synergy_loss: 9.6565 - class_loss: 0.0462\nEpoch 966/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.8837 - synergy_loss: 9.7279 - class_loss: 0.0481\nEpoch 967/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.7206 - synergy_loss: 9.6693 - class_loss: 0.0449\nEpoch 968/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.8681 - synergy_loss: 9.8517 - class_loss: 0.0449\nEpoch 969/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.8238 - synergy_loss: 9.8362 - class_loss: 0.0437\nEpoch 970/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.9331 - synergy_loss: 9.9496 - class_loss: 0.0457\nEpoch 971/1000\n570/570 [==============================] - 11s 20ms/step - loss: 22.6037 - synergy_loss: 10.4446 - class_loss: 0.0571\nEpoch 972/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.0834 - synergy_loss: 9.9986 - class_loss: 0.0428\nEpoch 973/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.6377 - synergy_loss: 9.6379 - class_loss: 0.0432\nEpoch 974/1000\n570/570 [==============================] - 11s 20ms/step - loss: 21.2733 - synergy_loss: 9.4008 - class_loss: 0.0431\nEpoch 975/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.6579 - synergy_loss: 9.7979 - class_loss: 0.0441\nEpoch 976/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.7003 - synergy_loss: 9.8541 - class_loss: 0.0451\nEpoch 977/1000\n570/570 [==============================] - 11s 20ms/step - loss: 21.9017 - synergy_loss: 10.0424 - class_loss: 0.0466\nEpoch 978/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.0029 - synergy_loss: 10.1408 - class_loss: 0.0459\nEpoch 979/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.6776 - synergy_loss: 9.8442 - class_loss: 0.0454\nEpoch 980/1000\n570/570 [==============================] - 11s 20ms/step - loss: 21.5903 - synergy_loss: 9.8001 - class_loss: 0.0474\nEpoch 981/1000\n570/570 [==============================] - 11s 18ms/step - loss: 21.7843 - synergy_loss: 9.9994 - class_loss: 0.0456\nEpoch 982/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.8871 - synergy_loss: 10.0811 - class_loss: 0.0446\nEpoch 983/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.3899 - synergy_loss: 9.6540 - class_loss: 0.0460\nEpoch 984/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.2749 - synergy_loss: 9.5899 - class_loss: 0.0450\nEpoch 985/1000\n570/570 [==============================] - 11s 20ms/step - loss: 21.6657 - synergy_loss: 9.9535 - class_loss: 0.0438\nEpoch 986/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.5960 - synergy_loss: 9.8952 - class_loss: 0.0494\nEpoch 987/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.2468 - synergy_loss: 9.5878 - class_loss: 0.0414\nEpoch 988/1000\n570/570 [==============================] - 11s 20ms/step - loss: 21.4286 - synergy_loss: 9.7958 - class_loss: 0.0421\nEpoch 989/1000\n570/570 [==============================] - 11s 19ms/step - loss: 22.9233 - synergy_loss: 10.9873 - class_loss: 0.0555\nEpoch 990/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.4280 - synergy_loss: 9.6424 - class_loss: 0.0453\nEpoch 991/1000\n570/570 [==============================] - 11s 20ms/step - loss: 21.0436 - synergy_loss: 9.4081 - class_loss: 0.0414\nEpoch 992/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.5246 - synergy_loss: 9.8711 - class_loss: 0.0494\nEpoch 993/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.8069 - synergy_loss: 10.1045 - class_loss: 0.0492\nEpoch 994/1000\n570/570 [==============================] - 11s 20ms/step - loss: 21.5271 - synergy_loss: 9.8905 - class_loss: 0.0450\nEpoch 995/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.0607 - synergy_loss: 9.5065 - class_loss: 0.0418\nEpoch 996/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.0367 - synergy_loss: 9.5355 - class_loss: 0.0447\nEpoch 997/1000\n570/570 [==============================] - 11s 20ms/step - loss: 21.3173 - synergy_loss: 9.7778 - class_loss: 0.0457\nEpoch 998/1000\n570/570 [==============================] - 11s 19ms/step - loss: 21.5343 - synergy_loss: 9.9633 - class_loss: 0.0479\nEpoch 999/1000\n570/570 [==============================] - 11s 19ms/step - loss: 25.2619 - synergy_loss: 13.3294 - class_loss: 0.0679\nEpoch 1000/1000\n570/570 [==============================] - 11s 20ms/step - loss: 22.6144 - synergy_loss: 10.4280 - class_loss: 0.0552\n283/283 [==============================] - 2s 5ms/step\nmsynergy_mean_squared_error 191.09236\nmclass_mean_squared_error 9.012154\nmsynergy_r2_score 0.5420881740995789\nmsynergy_pear (array([0.7426431706935934], dtype=object), 0.0)\nmsynergy_spear SpearmanrResult(correlation=0.7293353229806627, pvalue=0.0)\nmsclass_roc_curve 0.9477115289460664\nmclass_accuracy_scorer 0.9390618149934239\nmclass_cohen_kappa_score 0.6167419944806611\nmclass_precision_score 0.9548872180451128\nmclass_average_precision_score 0.8157357521269409\n","output_type":"stream"}]},{"cell_type":"code","source":"   \n\nfrom IPython.display import FileLink   \nnp.savetxt('npred_syn0.csv', ap11 ,delimiter=',')\nFileLink(r'npred_syn0.csv')\n\nnp.savetxt('npred_cls0.csv', ap22 ,delimiter=',') \nFileLink(r'npred_cls0.csv')\n\nnp.savetxt('ntest_syn0.csv', test_synergy1 ,delimiter=',')\nFileLink(r'ntest_syn0.csv')\n\nnp.savetxt('ntest_cls0.csv', test_class1 ,delimiter=',')\nFileLink(r'ntest_cls0.csv')\n\n","metadata":{"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/ntest_cls0.csv","text/html":"<a href='ntest_cls0.csv' target='_blank'>ntest_cls0.csv</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"ap111,ap221= model_att1.predict( [test_f_drug1,test_f_drug2,test_cell_line,test_ddi,test_graph1,test_graph2])\n\nap22=[]\ntest_class1=[]   \nl=len(ap221)  \nl1=int(l/2)\nfor i in range(l1) : \n#     for j in range(2):\n    ap=(ap221[i]+ap221[i+l1])/2\n#         ap[0,j]=(ap221[i,j]+ap221[i+l1],j)/2\n    ap22.append(ap)    \n    aap=(test_class[i]+test_class[i+l1])/2\n    test_class1.append(aap)\n    \n    \n\n# aap2=convert_tobin(ap22)\n\ntest_class11 = keras.utils.to_categorical(test_class1, num_classes=3)\naap2 = np.argmax(ap22, axis=1)\n# aclass_error1=roc_curve(auc(test_class1, ap22))\naclass_error1=roc_auc_score(test_class11.ravel(), np.array(ap22).ravel(),multi_class=\"ovr\")\naclass_error11=accuracy_score(test_class1, aap2)\naclass_error21=cohen_kappa_score(test_class1, aap2)\n\nprint(\"msclass_roc_curve\",aclass_error1)\nprint(\"mclass_accuracy_scorer\",aclass_error11)\nprint(\"mclass_cohen_kappa_score\",aclass_error21)\n\n# cross_att1 = model_att1.evaluate([test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,test_ddi],[test_synergy,test_class])\n# print(cross_att1)\n\naclass_pear1= precision_score(test_class1, aap2,average='macro')\naclass_spear1= average_precision_score(test_class11, ap22)\nprint(\"mclass_precision_score\",aclass_pear1)\nprint(\"mclass_average_precision_score\",aclass_spear1)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T13:42:48.501643Z","iopub.execute_input":"2023-06-07T13:42:48.502007Z","iopub.status.idle":"2023-06-07T13:42:57.551719Z","shell.execute_reply.started":"2023-06-07T13:42:48.501976Z","shell.execute_reply":"2023-06-07T13:42:57.548585Z"}}},{"cell_type":"markdown","source":" on = 0.0\n    best_kappa=0.0  \n\n    # Perform grid search to find the optimal threshold\n    for threshold in thresholds:   \n        # Convert predicted probabilities to binary predictions based on the threshold\n        y_pred = (y_pred_prob >= threshold).astype(int) \n\n        # Calculate AUC and average precision using the binary predictions\n        auc = accuracy_score(y_true, y_pred)\n        avg_precision = precision_score(y_true, y_pred)\n        kappa=cohen_kappa_score(y_true, y_pred)\n        # Check if the current AUC and average precision are better than the previous best values\n        if auc > best_auc or (auc == best_auc and avg_precision > best_avg_precision):\n            best_auc = auc\n            best_avg_precision = avg_precision\n            best_threshold = threshold\n            best_kappa=kappa\n\n    return best_threshold, best_auc, best_avg_precision,best_kappa\n\nbest_threshold, best_auc, best_avg_precision,best_kappa =find_optimal_threshold(test_class1, ap22)\nprint(\"Best Threshold:\", best_threshold)\nprint(\"Best accuracy:\", best_auc)\nprint(\"Best  Precision:\", best_avg_precision)\nprint(\"Best  kappa:\", best_kappa)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-10T13:39:13.666657Z","iopub.execute_input":"2023-07-10T13:39:13.667045Z","iopub.status.idle":"2023-07-10T13:39:13.677740Z","shell.execute_reply.started":"2023-07-10T13:39:13.667011Z","shell.execute_reply":"2023-07-10T13:39:13.675994Z"}}},{"cell_type":"markdown","source":"weights = [model1.get_weights() for model1 in model_at]\nnew_weights = list()\nfor weights_list_tuple in zip(*weights): \n    new_weights.append(\n        np.array([np.array(w).mean(axis=0) for w in zip(*weights_list_tuple)])\n    )\n\nnew_model= generate_network_att1(train_f_drug1,train_a_drug1,train_f_drug2,train_a_drug2,train_cell_line, inDrop, drop,train_ddi)   \n# new_model=tf.keras.models.load_model(mod1, compile=False)\nnew_model.set_weights(new_weights)\nwap111,wap221= new_model.predict( [test_f_drug1,test_a_drug1,test_f_drug2,test_a_drug2,test_cell_line,test_ddi])\n\nwap11=[]\nwtest_synergy1=[]\nl=len(wap111)\nl1=int(l/2)\nfor i in range(l1) :\n    ap=(wap111[i]+wap111[i+l1])/2\n    wap11.append(ap)\n    waap=(test_synergy[i]+test_synergy[i+l1])/2\n    test_synergy1.append(waap)\n    \n    \nwap22=[]\nwtest_class1=[]\nl=len(ap221)\nl1=int(l/2)\nfor i in range(l1) :\n    ap=(wap221[i]+wap221[i+l1])/2\n    wap22.append(ap)\n    waap=(test_class[i]+test_class[i+l1])/2\n    wtest_class1.append(waap)\n    \n    \n    \nwasynergy_error1=mean_squared_error(wtest_synergy1, wap11)\nwasynergy_error11=mean_absolute_error(wtest_synergy1, wap11)\nwasynergy_error21=r2_score(wtest_synergy1, wap11)\n\nprint(\"msynergy_mean_squared_error\",wasynergy_error1)\nprint(\"mclass_mean_squared_error\",wasynergy_error11)\nprint(\"msynergy_r2_score\",wasynergy_error21)\n\nwasynergy_pear1= pearsonr(wtest_synergy1, wap11)\nwasynergy_spear1= spearmanr(wtest_synergy1, wap11)\nprint(\"msynergy_pear\",wasynergy_pear1)\nprint(\"msynergy_spear\",wasynergy_spear1)\nx=mod1.get_weights\nx1=mod1.get_weights\nxx=(x+x1)/2","metadata":{}},{"cell_type":"markdown","source":"##### ","metadata":{}}]}