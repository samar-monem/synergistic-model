{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"test_fold=2 #choose fold from[0,1,2,3,4] ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!pip install openpyxl\n!pip install xlrd==2.0.1\n!pip install Pandas==1.3.5\n# !pip install Pandas==1.1.5\n!pip install  gdown\nimport tensorflow as tf\n\nimport numpy as np\nimport pandas as pd\n\ndef normalize1(X, means1=None, std1=None, means2=None, std2=None, feat_filt=None, norm='tanh_norm'):\n    if std1 is None:\n        std1 = np.nanstd(X, axis=0)\n    if feat_filt is None:\n        feat_filt = std1!=0\n    X = X[:,feat_filt]\n    X = np.ascontiguousarray(X)\n    if means1 is None:\n        means1 = np.mean(X, axis=0)\n    X = (X-means1)/std1[feat_filt]\n    if norm == 'norm':\n        return(X, means1, std1, feat_filt)\n    elif norm == 'tanh':\n        return(np.tanh(X), means1, std1, feat_filt)\n    elif norm == 'tanh_norm':\n        X = np.tanh(X)\n        if means2 is None:\n            means2 = np.mean(X, axis=0)\n        if std2 is None:\n            std2 = np.std(X, axis=0)\n        X = (X-means2)/std2\n        X[:,std2==0]=0\n        return(X, means1, std1, means2, std2, feat_filt) \n\n\n    \ndef get_data():\n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  smiles=pd.read_excel('pubchem.xls', header=None)\n  smiles=np.array(smiles)\n\n\n  !gdown --id 10ztxKtGSVU7p9yPoCsTnapVLThWpjjYT\n  data_to_repeat=pd.read_excel('oneil.xlsx', header=None)\n  data_to_repeat=np.array(data_to_repeat)\n  \n\n  !gdown https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\n  unique_drugs=pd.read_excel('pubchem.xls', header=None)\n  unique_drugs=np.array(unique_drugs)         \n\n  !gdown --id 1rVt2qEH-LMzk86ig6c8Qjll-08LhsxBv\n  feature_cell=pd.read_excel('cell_expression875.xlsx')\n  feature_cell=np.array(feature_cell)\n  feature_cell[:,1:]=(2**feature_cell[:,1:])-1\n    \n  !gdown --id 14upx46iIPcO80y_tcvbaS3uwMkZfKLVw\n  redkit_drug=pd.read_excel('redkit_drug.xlsx',header=None)\n  redkit_drug=np.array(redkit_drug)\n  \n    \n  !gdown --id 1Yf9YQRSq3Bf0bIwncpoANS5Ui1H3W_fH\n  graph=pd.read_excel('graph_emb1.xlsx',header=None)\n  graph=np.array(graph)\n  \n    \n  !gdown --id 1HQpJQ6yeZYnGK6Uy0AyIvzYsNkzBAeOD\n  model = tf.keras.models.load_model(\"model.h5\")\n\n  !gdown --id 1AZX2h806qMcMp63hnJB81JFYMjzEChpA\n  unique_drugs1=pd.read_excel('unique394_drugl.xlsx', header=None)\n  unique_drugs1=np.array(unique_drugs1)\n\n\n  return smiles[1:,1],data_to_repeat[1:,:],unique_drugs[1:,0],feature_cell,redkit_drug,model,graph,unique_drugs1\n\n\n\ndef repeat_smiles1(data_to_repeat,unique_drugs,feature,unique_cell,redkit_drug,graph):\n  unique_redkit_feature=redkit_drug[:,1:]\n  unique_redkit_name=redkit_drug[:,0]\n  unique_feature=unique_cell[:,1:]\n  unique_name=unique_cell[:,0]\n  d1=data_to_repeat[:,0]\n  d2=data_to_repeat[:,1]\n  c=data_to_repeat[:,2]\n  unique_drugs=feature[:,0]\n  feature=feature[:,1:]  \n  f_drug1=[]\n  f_drug2=[]\n  feature_cell=[]\n  redkit_d1=[]\n  redkit_d2=[]\n  graph1=[]\n  graph2=[]\n  for i in range(len(d1)):\n    n1=d1[i]\n    n2=d2[i]\n    cc=c[i]\n    k1= [m for m, v in enumerate(unique_drugs) if n1 in v]\n    k2=[m for m, v in enumerate(unique_drugs) if n2 in v]\n    cc1=[m for m, v in enumerate(unique_name) if cc in v]\n    r1= [m for m, v in enumerate(unique_redkit_name) if n1 in v]\n    r2=[m for m, v in enumerate(unique_redkit_name) if n2 in v]\n    f_drug1.append(feature[k1[0]])\n    f_drug2.append(feature[k2[0]])\n    graph1.append(graph[k1[0]])\n    graph2.append(graph[k2[0]])\n    feature_cell.append(unique_feature[cc1[0]])\n    redkit_d1.append(unique_redkit_feature[r1[0]])\n    redkit_d2.append(unique_redkit_feature[r2[0]])\n\n  return f_drug1,f_drug2,feature_cell,redkit_d1,redkit_d2,graph1,graph2\n\n\n\n\ndef train_test_input(f_drug1,f_drug2,cell_line,index_train,index_test,synery,class1,redkit_d1,redkit_d2,graph1,graph2):\n  train_f_drug1=[]\n  train_f_drug2=[]\n  train_cell_line=[]\n  train_synergy=[]\n  train_class=[]\n  train_redkit_d1=[]\n  train_redkit_d2=[]\n  test_f_drug1=[]\n  test_f_drug2=[]\n  test_cell_line=[]\n  test_synergy=[]\n  test_class=[]\n  test_redkit_d1=[]\n  test_redkit_d2=[]\n  train_graph1=[]\n  test_graph1=[]\n  train_graph2=[]\n  test_graph2=[]\n  for i in range(len(index_train)):\n      \n      train_f_drug1.append(f_drug1[index_train[i]])\n      train_f_drug2.append(f_drug2[index_train[i]])\n      train_cell_line.append(cell_line[index_train[i]])\n      train_synergy.append(synergy[index_train[i]])\n      train_class.append(class1[index_train[i]])\n      train_redkit_d1.append(redkit_d1[index_train[i]])\n      train_redkit_d2.append(redkit_d2[index_train[i]])\n      train_graph1.append(graph1[index_train[i]])\n      train_graph2.append(graph2[index_train[i]])\n\n  for ii in range(len(index_test)):\n      \n      test_f_drug1.append(f_drug1[index_test[ii]])\n      test_f_drug2.append(f_drug2[index_test[ii]])\n      test_cell_line.append(cell_line[index_test[ii]])\n      test_synergy.append(synergy[index_test[ii]])\n      test_class.append(class1[index_test[ii]])\n      test_redkit_d1.append(redkit_d1[index_test[ii]])\n      test_redkit_d2.append(redkit_d2[index_test[ii]])\n      test_graph1.append(graph1[index_test[ii]])\n      test_graph2.append(graph2[index_test[ii]])\n\n  return train_f_drug1,train_f_drug2,train_cell_line,test_f_drug1,test_f_drug2,test_cell_line,train_synergy,train_class,test_synergy,test_class,train_redkit_d1,train_redkit_d2,test_redkit_d1,test_redkit_d2,train_graph1,test_graph1,train_graph2,test_graph2\n\n\n\ndef preprocess(index_train,index_test):\n    index_train1=[]\n    index_test1=[]\n    index_train2=(index_train)[0]\n    index_test2=(index_test)[0]\n    for i in range(len((index_train2))):\n        index_train1.append((index_train2[i]))\n        \n    for ii in range(len(index_test2)):\n        index_test1.append((index_test2[ii]))\n        \n    return index_train1,index_test1\n\ndef get_data_me2(s):\n    \n    \n\n    !gdown 1C7Z2ziPdQVzH3omIdIfyJa7VUmog4IIk\n    labels = pd.read_csv('oneil.csv', index_col=0) \n    \n    h=len(np.array(labels))\n    #labels are duplicated for the two different ways of ordering in the data\n    labels = pd.concat([labels, labels]) \n    \n    test_fold =s\n   \n    idx_train = np.where(labels['fold']!=test_fold)\n    \n\n    idx_test = np.where(labels['fold']==test_fold)\n#     \n#    \n    return idx_train,idx_test\n\n\n\n\n\ndef convert_tobin(cc):\n    cb=[]\n    for i in range(len(cc)):\n        if(cc[i]>=0.5):\n            cb.append(1)\n        else:\n            cb.append(0)\n    return cb\n\n\ndef norm1(train_cell_line,test_cell_line,norm=\"tanh_norm\"):\n# norm = \"norm\"\n    if norm == \"tanh_norm\":\n        train_cell_line, mean, std, mean2, std2, feat_filt = normalize1(train_cell_line, norm=norm)\n        test_cell_line, mean, std, mean2, std2, feat_filt = normalize1(test_cell_line, mean, std, mean2, std2, \n                                                              feat_filt=feat_filt, norm=norm)\n    else:\n        train_cell_line, mean, std, feat_filt = normalize1(train_cell_line, norm=norm)\n        test_cell_line, mean, std, feat_filt = normalize1(test_cell_line, mean, std, feat_filt=feat_filt, norm=norm)\n    \n    return train_cell_line,test_cell_line\n\n\n\n    \n     \n\nsmiles,data_to_repeat,unique_drugs,unique_cell,redkit_drug,ddi_model,graph,feature=get_data()\n\ndata_to_repeat=np.r_[data_to_repeat,data_to_repeat]\nl=int((data_to_repeat.shape[0])/2)\ndata_to_repeat[l:,0]=data_to_repeat[0:l,1]\ndata_to_repeat[l:,1]=data_to_repeat[0:l,0]\nsynergy=data_to_repeat[:,3]\n\nclass1=[]\nfor i in range(len(synergy)):\n if(synergy[i]>=30):\n    class1.append(1)\n elif(synergy[i]<0): \n    class1.append(0)\n else:\n    class1.append(2)    \n\n\nf_drug1,f_drug2,feature_cell,redkit_d1,redkit_d2,graph1,graph2=repeat_smiles1(data_to_repeat,unique_drugs,feature,unique_cell,redkit_drug,graph)\n\n\n\n\nindex_train,index_test=get_data_me2(test_fold)\n\nindex_train,index_test=preprocess(index_train,index_test)\ntrain_f_drug1,train_f_drug2,train_cell_line,test_f_drug1,test_f_drug2,test_cell_line,train_synergy,train_class,test_synergy,test_class,train_redkit_d1,train_redkit_d2,test_redkit_d1,test_redkit_d2,train_graph1,test_graph1,train_graph2,test_graph2=train_test_input(f_drug1,f_drug2,feature_cell,index_train,index_test,synergy,class1,redkit_d1,redkit_d2,graph1,graph2)\ntrain_cell_line=np.array(train_cell_line).astype(float)\ntest_cell_line=np.array(test_cell_line).astype(float)  \ntrain_f_drug1=np.array(train_f_drug1).astype(float)\ntest_f_drug1=np.array(test_f_drug1).astype(float)\ntrain_f_drug2=np.array(train_f_drug2).astype(float)\ntest_f_drug2=np.array(test_f_drug2).astype(float)\n\ntrain_graph1=np.array(train_graph1).astype(float)\ntest_graph1=np.array(test_graph1).astype(float) \ntrain_graph2=np.array(train_graph2).astype(float)\ntest_graph2=np.array(test_graph2).astype(float)\n\ntrain_cell_line,test_cell_line=norm1(train_cell_line,test_cell_line)\n\ntrain_f_drug1,test_f_drug1=norm1(train_f_drug1,test_f_drug1)\ntrain_f_drug2,test_f_drug2=norm1(train_f_drug2,test_f_drug2)\n\ntrain_graph1,test_graph1=norm1(train_graph1,test_graph1)\ntrain_graph2,test_graph2=norm1(train_graph2,test_graph2)\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-07-13T10:28:46.577123Z","iopub.execute_input":"2023-07-13T10:28:46.577501Z","iopub.status.idle":"2023-07-13T10:32:27.407555Z","shell.execute_reply.started":"2023-07-13T10:28:46.577469Z","shell.execute_reply":"2023-07-13T10:32:27.406246Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting deepchem\n  Downloading deepchem-2.7.1-py3-none-any.whl (693 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m693.2/693.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.2.0)\nRequirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.23.5)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.5.3)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from deepchem) (1.2.2)\nCollecting scipy<1.9 (from deepchem)\n  Downloading scipy-1.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting rdkit (from deepchem)\n  Downloading rdkit-2023.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.7/29.7 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->deepchem) (2023.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rdkit->deepchem) (9.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->deepchem) (3.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->deepchem) (1.16.0)\nInstalling collected packages: scipy, rdkit, deepchem\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.11.1\n    Uninstalling scipy-1.11.1:\n      Successfully uninstalled scipy-1.11.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.8.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed deepchem-2.7.1 rdkit-2023.3.2 scipy-1.7.3\nCollecting pysmiles\n  Downloading pysmiles-1.0.2-py2.py3-none-any.whl (22 kB)\nCollecting pbr (from pysmiles)\n  Downloading pbr-5.11.1-py2.py3-none-any.whl (112 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting networkx~=2.0 (from pysmiles)\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pbr, networkx, pysmiles\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.1\n    Uninstalling networkx-3.1:\n      Successfully uninstalled networkx-3.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nscikit-image 0.21.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed networkx-2.8.8 pbr-5.11.1 pysmiles-1.0.2\nRequirement already satisfied: openpyxl in /opt/conda/lib/python3.10/site-packages (3.1.2)\nRequirement already satisfied: et-xmlfile in /opt/conda/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"Collecting PubChemPy\n  Downloading PubChemPy-1.0.4.tar.gz (29 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: PubChemPy\n  Building wheel for PubChemPy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PubChemPy: filename=PubChemPy-1.0.4-py3-none-any.whl size=13835 sha256=9eeb8ab2453a8b39ee5df7c71ea81560621b8b59a6153b49448ba9860aa118e7\n  Stored in directory: /root/.cache/pip/wheels/90/7c/45/18a0671e3c3316966ef7ed9ad2b3f3300a7e41d3421a44e799\nSuccessfully built PubChemPy\nInstalling collected packages: PubChemPy\nSuccessfully installed PubChemPy-1.0.4\nCollecting PyDrive\n  Downloading PyDrive-1.3.1.tar.gz (987 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m987.4/987.4 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /opt/conda/lib/python3.10/site-packages (from PyDrive) (2.92.0)\nRequirement already satisfied: oauth2client>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from PyDrive) (4.1.3)\nRequirement already satisfied: PyYAML>=3.0 in /opt/conda/lib/python3.10/site-packages (from PyDrive) (6.0)\nRequirement already satisfied: httplib2<1.dev0,>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (0.21.0)\nRequirement already satisfied: google-auth<3.0.0.dev0,>=1.19.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (2.20.0)\nRequirement already satisfied: google-auth-httplib2>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (0.1.0)\nRequirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (2.11.1)\nRequirement already satisfied: uritemplate<5,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\nRequirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=4.0.0->PyDrive) (0.2.7)\nRequirement already satisfied: rsa>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=4.0.0->PyDrive) (4.9)\nRequirement already satisfied: six>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from oauth2client>=4.0.0->PyDrive) (1.16.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.59.1)\nRequirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.20.3)\nRequirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.31.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.2->PyDrive) (4.2.4)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0.dev0,>=1.19.0->google-api-python-client>=1.2->PyDrive) (1.26.15)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<1.dev0,>=0.15.0->google-api-python-client>=1.2->PyDrive) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2023.5.7)\nBuilding wheels for collected packages: PyDrive\n  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for PyDrive: filename=PyDrive-1.3.1-py3-none-any.whl size=27453 sha256=4ae765671d45d8065188b4c7cd35f131ffd558c8a556a286664c9dbb9f2ee32d\n  Stored in directory: /root/.cache/pip/wheels/63/79/df/924c22c080c9dac1a57f611baa837fe0bc3daec1500b27f23b\nSuccessfully built PyDrive\nInstalling collected packages: PyDrive\nSuccessfully installed PyDrive-1.3.1\nCollecting xlrd==2.0.1\n  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: xlrd\nSuccessfully installed xlrd-2.0.1\nCollecting Pandas==1.3.5\n  Downloading pandas-1.3.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from Pandas==1.3.5) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from Pandas==1.3.5) (2023.3)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from Pandas==1.3.5) (1.23.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7.3->Pandas==1.3.5) (1.16.0)\nInstalling collected packages: Pandas\n  Attempting uninstall: Pandas\n    Found existing installation: pandas 1.5.3\n    Uninstalling pandas-1.5.3:\n      Successfully uninstalled pandas-1.5.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nbeatrix-jupyterlab 2023.621.222118 requires jupyter-server~=1.16, but you have jupyter-server 2.6.0 which is incompatible.\ncudf 23.6.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cuda 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nfeaturetools 1.26.0 requires pandas<2.0.0,>=1.5.0, but you have pandas 1.3.5 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.3.0 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.23.5 which is incompatible.\nraft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nwoodwork 0.24.0 requires pandas<2.0.0,>=1.4.3, but you have pandas 1.3.5 which is incompatible.\nxarray 2023.6.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Pandas-1.3.5\nCollecting gdown\n  Downloading gdown-4.7.1-py3-none-any.whl (15 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.12.2)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from gdown) (1.16.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.65.0)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.5.7)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nInstalling collected packages: gdown\nSuccessfully installed gdown-4.7.1\n","output_type":"stream"},{"name":"stderr","text":"<frozen importlib._bootstrap>:241: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n","output_type":"stream"},{"name":"stdout","text":"hjjbjh\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (6.0)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (3.9.0)\nRequirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from h5py) (1.23.5)\nDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 60.1MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=10ztxKtGSVU7p9yPoCsTnapVLThWpjjYT\nTo: /kaggle/working/oneil.xlsx\n100%|████████████████████████████████████████| 850k/850k [00:00<00:00, 69.7MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1m69fQ5l7q3dwQeud5M1nUVNhmrTGFbZc\nTo: /kaggle/working/pubchem.xls\n100%|██████████████████████████████████████| 27.1k/27.1k [00:00<00:00, 62.0MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1rVt2qEH-LMzk86ig6c8Qjll-08LhsxBv\nTo: /kaggle/working/cell_expression875.xlsx\n100%|█████████████████████████████████████████| 303k/303k [00:00<00:00, 116MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=14upx46iIPcO80y_tcvbaS3uwMkZfKLVw\nTo: /kaggle/working/redkit_drug.xlsx\n100%|██████████████████████████████████████| 68.6k/68.6k [00:00<00:00, 82.9MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1Yf9YQRSq3Bf0bIwncpoANS5Ui1H3W_fH\nTo: /kaggle/working/graph_emb1.xlsx\n100%|██████████████████████████████████████| 64.8k/64.8k [00:00<00:00, 89.2MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (uriginal): https://drive.google.com/uc?id=1HQpJQ6yeZYnGK6Uy0AyIvzYsNkzBAeOD\nFrom (redirected): https://drive.google.com/uc?id=1HQpJQ6yeZYnGK6Uy0AyIvzYsNkzBAeOD&confirm=t&uuid=7c22572c-7de3-41a3-8222-7542a5cef2aa\nTo: /kaggle/working/model.h5\n100%|████████████████████████████████████████| 567M/567M [00:11<00:00, 51.1MB/s]\n/opt/conda/lib/python3.10/site-packages/gdown/cli.py:126: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom: https://drive.google.com/uc?id=1AZX2h806qMcMp63hnJB81JFYMjzEChpA\nTo: /kaggle/working/unique394_drugl.xlsx\n100%|████████████████████████████████████████| 162k/162k [00:00<00:00, 86.6MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1C7Z2ziPdQVzH3omIdIfyJa7VUmog4IIk\nTo: /kaggle/working/oneil.csv\n100%|████████████████████████████████████████| 988k/988k [00:00<00:00, 83.5MB/s]\n(36426, 875)\n(9048, 875)\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install tensorflow\n!pip install spektral\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Minimum,Maximum,Add,Maximum,PReLU, Flatten,Reshape,Dropout, Input,Dense,Add,concatenate,BatchNormalization, Activation#,MultiHeadAttention,AdditiveAttention\nfrom tensorflow.keras.losses import CategoricalCrossentropy\nfrom tensorflow.keras.models import Model,load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.regularizers import l2\nfrom spektral.layers import GATConv, GlobalAvgPool,GlobalMaxPool, GCNConv,GlobalAttentionPool\nfrom spektral.transforms import LayerPreprocess\nfrom sklearn.metrics import confusion_matrix, mean_squared_error,mean_absolute_error,r2_score#,AUC\nfrom scipy.stats import pearsonr,spearmanr\nfrom sklearn.metrics import roc_curve,auc,accuracy_score,precision_score,cohen_kappa_score,precision_recall_curve,average_precision_score,roc_auc_score\nfrom keras.utils.vis_utils import plot_model\nfrom tensorflow.keras import regularizers\nnp.random.seed(10)\n\ntrain_cell_line=tf.convert_to_tensor(train_cell_line)\ntest_cell_line=tf.convert_to_tensor(test_cell_line)\ntrain_synergy=tf.convert_to_tensor(train_synergy)\ntrain_class=tf.convert_to_tensor(train_class)\ntest_synergy=tf.convert_to_tensor(test_synergy)\ntest_class=tf.convert_to_tensor(test_class)\ntest_redkit_d1=tf.convert_to_tensor(test_redkit_d1)\ntest_redkit_d2=tf.convert_to_tensor(test_redkit_d2)\ntrain_f_drug1=tf.convert_to_tensor(train_f_drug1)\ntest_f_drug1=tf.convert_to_tensor(test_f_drug1)\ntrain_f_drug2=tf.convert_to_tensor(train_f_drug2)\ntest_f_drug2=tf.convert_to_tensor(test_f_drug2)\ntrain_graph1=tf.convert_to_tensor(train_graph1)\ntest_graph1=tf.convert_to_tensor(test_graph1)\ntrain_graph2=tf.convert_to_tensor(train_graph2)\ntest_graph2=tf.convert_to_tensor(test_graph2)\n\n\n\ndef ddi_fun(train_redkit_d1,train_redkit_d2):\n    train_redkit_d1=np.array(train_redkit_d1).astype(float)\n    train_redkit_d2=np.array(train_redkit_d2).astype(float)\n    ddi_extractor = keras.Model(\n    inputs=ddi_model.inputs,\n    outputs=ddi_model.get_layer(name=\"features\").output,\n    )\n\n    train_ddi=ddi_extractor.predict([train_redkit_d1,train_redkit_d2])\n\n    return train_ddi\n\ntest_ddi=ddi_fun(test_redkit_d1,test_redkit_d2)\ntrain_ddi=ddi_fun(train_redkit_d1,train_redkit_d2)\ntrain_ddi=np.array(train_ddi).astype(float)\ntest_ddi=np.array(test_ddi).astype(float)\n\ntrain_ddi,test_ddi=norm1(train_ddi,test_ddi)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-13T10:32:27.411392Z","iopub.execute_input":"2023-07-13T10:32:27.411713Z","iopub.status.idle":"2023-07-13T10:34:26.451901Z","shell.execute_reply.started":"2023-07-13T10:32:27.411684Z","shell.execute_reply":"2023-07-13T10:34:26.450751Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.9.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.0)\nRequirement already satisfied: numpy<1.24,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.31.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (0.2.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.7.3)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\nCollecting numpy<1.24,>=1.22 (from tensorflow)\n  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Uninstalling numpy-1.23.5:\n      Successfully uninstalled numpy-1.23.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.6.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.6.1 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\nchex 0.1.81 requires numpy>=1.25.0, but you have numpy 1.22.4 which is incompatible.\ncudf 23.6.1 requires protobuf<4.22,>=4.21.6, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cuda 23.6.0 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\ndask-cudf 23.6.1 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nfeaturetools 1.26.0 requires pandas<2.0.0,>=1.5.0, but you have pandas 1.3.5 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\npointpats 2.3.0 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.22.4 which is incompatible.\nraft-dask 23.6.2 requires dask==2023.3.2, but you have dask 2023.7.0 which is incompatible.\nscikit-image 0.21.0 requires scipy>=1.8, but you have scipy 1.7.3 which is incompatible.\nwoodwork 0.24.0 requires pandas<2.0.0,>=1.4.3, but you have pandas 1.3.5 which is incompatible.\nxarray 2023.6.0 requires pandas>=1.4, but you have pandas 1.3.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.22.4\nCollecting spektral\n  Downloading spektral-1.3.0-py3-none-any.whl (140 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from spektral) (1.2.0)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from spektral) (4.9.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from spektral) (2.8.8)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from spektral) (1.22.4)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from spektral) (1.3.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from spektral) (2.31.0)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from spektral) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from spektral) (1.7.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from spektral) (4.65.0)\nRequirement already satisfied: tensorflow>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from spektral) (2.12.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.51.1)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (3.9.0)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (16.0.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (21.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow>=2.2.0->spektral) (0.31.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.10/site-packages (from pandas->spektral) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.10/site-packages (from pandas->spektral) (2023.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->spektral) (2023.5.7)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->spektral) (3.1.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow>=2.2.0->spektral) (0.2.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (3.4.3)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (2.3.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->tensorflow>=2.2.0->spektral) (3.0.9)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (1.3.1)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow>=2.2.0->spektral) (3.2.2)\nInstalling collected packages: spektral\nSuccessfully installed spektral-1.3.0\n283/283 [==============================] - 2s 2ms/step\n1139/1139 [==============================] - 3s 2ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# import tensorflow_datasets as tfds\nimport tensorflow as tf\ndef scaled_dot_product_attention(q, k, v, mask=None):\n  \"\"\"Calculate the attention weights.\n  q, k, v must have matching leading dimensions.\n  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n  The mask has different shapes depending on its type(padding or look ahead)\n  but it must be broadcastable for addition.\n\n  Args:\n    q: query shape == (..., seq_len_q, depth)\n    k: key shape == (..., seq_len_k, depth)\n    v: value shape == (..., seq_len_v, depth_v)\n    mask: Float tensor with shape broadcastable\n          to (..., seq_len_q, seq_len_k). Defaults to None.\n\n  Returns:\n    output, attention_weights\n  \"\"\"\n\n  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n\n  # scale matmul_qk\n  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n\n  # add the mask to the scaled tensor.\n  if mask is not None:\n    scaled_attention_logits += (mask * -1e9)\n\n  # softmax is normalized on the last axis (seq_len_k) so that the scores\n  # add up to 1.\n  attention_weights = tf.nn.softmax(scaled_attention_logits)#, axis=-1)#tf.nn.linear()#, axis=-1)  # (..., seq_len_q, seq_len_k)\n\n  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n\n  return output,attention_weights \n\n#multi-head\n\nclass MultiHeadAttention(tf.keras.layers.Layer):\n  def __init__(self,*, d_model, num_heads):\n    super(MultiHeadAttention, self).__init__()\n    self.num_heads = num_heads\n    self.d_model = d_model\n\n    assert d_model % self.num_heads == 0\n\n    self.depth = d_model // self.num_heads\n\n    self.wq = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wk = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n    self.wv = tf.keras.layers.Dense(d_model,activation='relu',use_bias='true')\n\n    self.dense = tf.keras.layers.Dense(d_model)#,activation='relu',use_bias='true')\n\n  def split_heads(self, x, batch_size):\n    \"\"\"Split the last dimension into (num_heads, depth).\n    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n    \"\"\"\n    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n    return tf.transpose(x, perm=[0, 2, 1, 3])\n\n  def call(self, v, k, q, mask=None):\n    batch_size = tf.shape(q)[0]\n\n    q = self.wq(q)  # (batch_size, seq_len, d_model)\n#     q=PReLU()(q)\n    k = self.wk(k)  # (batch_size, seq_len, d_model)\n#     k=PReLU()(k)\n    v = self.wv(v)  # (batch_size, seq_len, d_model)\n#     v=PReLU()(v)\n\n    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n\n    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n    scaled_attention, attention_weights = scaled_dot_product_attention(\n        q, k, v, mask)\n\n    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n\n    concat_attention = tf.reshape(scaled_attention,\n                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n\n    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n\n    return output#, attention_weights\n\n\n\nclass CrossStitch(tf.keras.layers.Layer):\n\n    \"\"\"Cross-Stitch implementation according to arXiv:1604.03539\n    Implementation adapted from https://github.com/helloyide/Cross-stitch-Networks-for-Multi-task-Learning\"\"\"\n\n    def __init__(self, num_tasks, *args, **kwargs):\n        \"\"\"initialize class variables\"\"\"\n        self.num_tasks = num_tasks\n        super(CrossStitch, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        \"\"\"initialize the kernel and set the instance to 'built'\"\"\"\n        self.kernel = self.add_weight(name=\"kernel\",\n                                      shape=(self.num_tasks,\n                                             self.num_tasks),\n                                      initializer='identity',\n                                      trainable=True)\n        super(CrossStitch, self).build(input_shape)\n\n    def call(self, xl):\n        \"\"\"\n        called by TensorFlow when the model gets build. \n        Returns a stacked tensor with num_tasks channels in the 0 dimension, \n        which need to be unstacked.\n        \"\"\"\n        if (len(xl) != self.num_tasks):\n            # should not happen\n            raise ValueError()\n\n        out_values = []\n        for this_task in range(self.num_tasks):\n            this_weight = self.kernel[this_task, this_task]\n            out = tf.math.scalar_mul(this_weight, xl[this_task])\n            for other_task in range(self.num_tasks):\n                if this_task == other_task:\n                    continue  # already weighted!\n                other_weight = self.kernel[this_task, other_task]\n#                 out += tf.math.scalar_mul(other_weight, xl[other_task])\n            out_values.append(out)\n        # HACK!\n        # unless we stack, and then unstack the tensors, TF (2.0.0) can't follow\n        # the graph, so it aborts during model initialization.\n        # return tf.stack(out_values, axis=0)\n        return out_values[0],out_values[1]\n\n    def compute_output_shape(self, input_shape):\n        return [self.num_tasks] + input_shape\n\n    def get_config(self):\n        \"\"\"implemented so keras can save the model to json/yml\"\"\"\n        config = {\n            \"num_tasks\": self.num_tasks\n        }\n        base_config = super(CrossStitch, self).get_config()\n        return dict(list(config.items()) + list(base_config.items()))\n\n    \n \n\n\nimport keras.backend as K\nfrom keras.optimizers import Adam\n\nclass AdamW(Adam):\n    def __init__(self, learning_rate=0.001, weight_decay=0.025, **kwargs):\n        super(AdamW, self).__init__(learning_rate, **kwargs)\n        self.weight_decay = K.variable(weight_decay, name='weight_decay')\n\n    def get_updates(self, loss, params):\n        # Apply weight decay to the parameters\n        decay_updates = [\n            K.update_add(param, -self.weight_decay * param)\n            for param in params\n            if param.name.endswith('kernel:0')  # Apply weight decay only to kernel weights\n        ]\n\n        # Call the parent get_updates() method to get the remaining updates\n        updates = super(AdamW, self).get_updates(loss, params)\n\n        # Combine the weight decay updates and other updates\n        updates.extend(decay_updates)\n\n        return updates\n\n    \nimport keras.backend as K\nfrom keras.optimizers import Optimizer\n\nclass AMSGrad(Optimizer):\n    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-7, **kwargs):\n        super(AMSGrad, self).__init__(**kwargs)\n        self.learning_rate = learning_rate\n        self.beta_1 = beta_1\n        self.beta_2 = beta_2\n        self.epsilon = epsilon\n        self.iterations = K.variable(0)\n        self.m = None\n        self.v = None\n\ndef get_updates(self, loss, params):\n    grads = self.get_gradients(loss, params)\n    self.updates = [K.update_add(self.iterations, 1)]\n\n    t = self.iterations + 1\n    lr_t = self.learning_rate * (K.sqrt(1.0 - K.pow(self.beta_2, t)) / (1.0 - K.pow(self.beta_1, t)))\n\n    shapes = [K.int_shape(p) for p in params]\n    self.m = [K.zeros(shape) for shape in shapes]\n    self.v = [K.zeros(shape) for shape in shapes]\n\n    for p, g, m, v in zip(params, grads, self.m, self.v):\n        m_t = (self.beta_1 * m) + (1.0 - self.beta_1) * g\n        v_t = (self.beta_2 * v) + (1.0 - self.beta_2) * K.square(g)\n        v_hat = K.maximum(v, v_t)  # AMSGrad modification\n\n        p_t = p - lr_t * m_t / (K.sqrt(v_hat) + self.epsilon)\n\n        self.updates.append(K.update(m, m_t))\n        self.updates.append(K.update(v, v_t))\n        self.updates.append(K.update(p, p_t))\n\n    return self.updates\n","metadata":{"execution":{"iopub.status.busy":"2023-07-13T10:34:26.453549Z","iopub.execute_input":"2023-07-13T10:34:26.453910Z","iopub.status.idle":"2023-07-13T10:34:26.496364Z","shell.execute_reply.started":"2023-07-13T10:34:26.453874Z","shell.execute_reply":"2023-07-13T10:34:26.495334Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\ndef generate_network_att1(x_in1,x_in2,cell, inDrop, drop,ddi,graph1,graph2):\n    # fill the architecture params from dict\n\n\n    cell_layers = [512,265,128]\n\n    snp_layers = [512,128]\n\n    ddi_layers=[1024,512,256]\n    layers=[256,512,256]\n    g_layers=[258,128]\n\n    dsn1_layers = [1024,512,256]\n    dsn2_layers = [1024,512,256]\n   \n    l2_reg = 1e-3  # L2 regularization rate\n\n    \n\n    snp_layers = [2048,1024,2048]\n    # contruct two parallel networks\n    for l in range(len(dsn1_layers)):\n        if l == 0:\n            x_in1    = Input(shape=(x_in1.shape[1],),name=\"input1\")\n            middle_layer = Dense(int(dsn1_layers[l]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(x_in1)\n            middle_layer = Dropout(float(inDrop))(middle_layer) \n        elif l == (len(dsn1_layers)-1):\n            dsn1_output = Dense(int(dsn1_layers[l]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(middle_layer)\n        else:\n            middle_layer = Dense(int(dsn1_layers[l]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(middle_layer)\n            middle_layer = Dropout(float(drop))(middle_layer)\n\n    \n    concatModel1 =  dsn1_output\n\n    \n    for l in range(len(dsn2_layers)):\n        if l == 0:\n            x_in2    = Input(shape=(x_in2.shape[1],),name=\"input2\")\n            middle_layer = Dense(int(dsn2_layers[l]), activation='relu', use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(x_in2)\n            middle_layer = Dropout(float(inDrop))(middle_layer)\n        elif l == (len(dsn2_layers)-1):\n            dsn2_output = Dense(int(dsn2_layers[l]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(middle_layer)\n        else:\n            middle_layer = Dense(int(dsn2_layers[l]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(middle_layer)\n            middle_layer = Dropout(float(drop))(middle_layer)\n    \n    concatModel2 = dsn2_output\n    \n\n    for cell_layer in range(len(cell_layers)):\n      if cell_layer == 0:\n        input_cell    = Input(shape=(cell[0].shape[0],))  #\n\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(input_cell)\n        cellFC = Dropout(float(drop))(cellFC)\n\n      elif cell_layer == (len(cell_layers)-1):\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n\n      else:\n        cellFC = Dense(int(cell_layers[cell_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(cellFC)\n        cellFC = Dropout(float(drop))(cellFC)\n#        \n\n\n    graph_in1=Input(shape=(graph1.shape[1],),name='graph1')\n\n    for g_layer in range(len(g_layers)):\n      if g_layer == 0:\n\n        graph_out1 = Dense(int(g_layers[g_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_in1)\n        graph_out1 = Dropout(float(drop))(graph_out1)\n#         \n        \n      elif g_layer == (len(g_layers)-1):\n        graph_out1 = Dense(int(g_layers[g_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_out1)\n#         \n      else:\n        graph_out1 = Dense(int(g_layers[g_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_out1)\n        graph_out1 = Dropout(float(drop))(graph_out1)\n\n    \n    \n    graph_in2=Input(shape=(graph2.shape[1],),name='graph2')\n\n    for g_layer in range(len(g_layers)):\n      if g_layer == 0:\n\n        graph_out2 = Dense(int(g_layers[g_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_in2)\n        graph_out2 = Dropout(float(drop))(graph_out2)\n        \n      elif g_layer == (len(g_layers)-1):\n        graph_out2 = Dense(int(g_layers[g_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_out2)\n\n\n      else:\n        graph_out2 = Dense(int(g_layers[g_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(graph_out2)\n        graph_out2 = Dropout(float(drop))(graph_out2)\n    \n    concatModel1=concatenate([concatModel1,graph_out1])\n    concatModel2=concatenate([concatModel2,graph_out2])\n    \n    \n    for layer in range(len(layers)):\n      if layer == 0:\n\n        concatModel1 = Dense(int(layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel1)\n        concatModel1 = Dropout(float(drop))(concatModel1)\n\n        \n      elif layer == (len(layers)-1):\n        concatModel1 = Dense(int(layers[layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel1)\n\n      else:\n        concatModel1 = Dense(int(layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel1)\n        concatModel1 = Dropout(float(drop))(concatModel1)\n    \n    \n    for layer in range(len(layers)):\n      if layer == 0:\n\n        concatModel2 = Dense(int(layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel2)\n        concatModel2 = Dropout(float(drop))(concatModel2)\n        \n      elif layer == (len(layers)-1):\n        concatModel2 = Dense(int(layers[layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel2)\n\n      else:\n        concatModel2 = Dense(int(layers[layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(concatModel2)\n        concatModel2 = Dropout(float(drop))(concatModel2)\n    \n    \n    ddi_out1=Input(shape=(ddi.shape[1],),name='ddi')\n\n    for ddi_layer in range(len(ddi_layers)):\n      if ddi_layer == 0:\n\n        ddi_out = Dense(int(ddi_layers[ddi_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(ddi_out1)\n        ddi_out = Dropout(float(drop))(ddi_out)\n\n        \n      elif ddi_layer == (len(ddi_layers)-1):\n        ddi_out = Dense(int(ddi_layers[ddi_layer]), activation='linear',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(ddi_out)\n\n      else:\n        ddi_out = Dense(int(ddi_layers[ddi_layer]), activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(ddi_out)\n        ddi_out = Dropout(float(drop))(ddi_out)\n\n    \n    mlayer =MultiHeadAttention(d_model=cellFC.shape[1], num_heads=4)\n    cellFC1= mlayer(cellFC,ddi_out,ddi_out)\n    cellFC1 = Reshape([cellFC1.shape[2]])(cellFC1)\n    cellFC=concatenate([cellFC1,cellFC])\n\n    \n    concatModel=concatenate([concatModel2,concatModel1,cellFC])\n    concatModel=BatchNormalization()(concatModel)\n    \n        \n   \n#    \n    layer1 =MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task1= layer1(concatModel,concatModel,concatModel)\n    layer2 = MultiHeadAttention(d_model=concatModel.shape[1], num_heads=4)\n    a_task2= layer2(concatModel,concatModel,concatModel)\n    task11 = Reshape([a_task1.shape[2]])(a_task1)\n    task22 = Reshape([a_task2.shape[2]])(a_task2)\n    task1=concatenate([task11,concatModel])\n    task2=concatenate([task22,concatModel])\n\n    r_task1,r_task2 = CrossStitch(2)([task1,task2])\n\n    \n    r_task1=Dense(2048,activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task1)\n    \n    r_task2=Dense(2048, activation='relu',use_bias=True,kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001))(r_task2)\n\n                             \n    r_task1,r_task2 = CrossStitch(2)([r_task1,r_task2])\n\n    \n    r_task1=concatenate([r_task1,task1])\n    r_task2=concatenate([r_task2,task2])\n\n    \n    \n    r_task1 = Dense(128, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fsynergy1')(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(128, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fclass1')(r_task2)\n\n   \n    r_task1 = Dense(64, activation='linear',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fsynergy2')(r_task1)\n    r_task1=PReLU()(r_task1)\n    r_task2 = Dense(64, activation='relu',kernel_initializer=\"he_normal\",kernel_regularizer=regularizers.L1(0.001),\n                     activity_regularizer=regularizers.L2(0.001),name='fclass2')(r_task2)\n#   \n\n    \n    snp_output1 = Dense(1, activation='linear',name='synergy')(r_task1)\n    snp_output2 = Dense(3, activation='sigmoid',name='class')(r_task2)\n    \n\n    model = Model(inputs=[x_in1,x_in2,input_cell,ddi_out1,graph_in1,graph_in2],outputs= [snp_output1,snp_output2])\n\n    print(model.summary())\n    return model\n\n\ndef trainer_att1(model, l_rate, train_f_drug1,train_f_drug2,train_cell_line,train_synergy,train_class,train_ddi,train_graph1,train_grpah2, epo, batch_size, earlyStop):\n\n    optimizer = AdamW(learning_rate=0.00001, weight_decay=0.025)\n\n    model.compile(optimizer=optimizer,loss={'synergy':'mse','class':'categorical_crossentropy'})\n\n    model.fit([train_f_drug1,train_f_drug2,train_cell_line,train_ddi,train_graph1,train_graph2],[train_synergy,train_class],shuffle=True, epochs=epo, batch_size=batch_size,verbose=1) \n                  \n\n    return model\n","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-07-13T10:34:26.498032Z","iopub.execute_input":"2023-07-13T10:34:26.498424Z","iopub.status.idle":"2023-07-13T10:34:26.563484Z","shell.execute_reply.started":"2023-07-13T10:34:26.498388Z","shell.execute_reply":"2023-07-13T10:34:26.562495Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#n\n\nl_rate = 0.00001\ninDrop = 0.2\ndrop = 0.2   \nmax_epoch =1000\nbatch_size = 64 \nearlyStop_patience = 20\n\n\n\nmodel_att1= generate_network_att1(train_f_drug1,train_f_drug2,train_cell_line, inDrop, drop,train_ddi,train_graph1,train_graph2)\n\ntrain_class1 = keras.utils.to_categorical(train_class, num_classes=3)\nmodel_att1=trainer_att1(model_att1, l_rate, train_f_drug1,train_f_drug2,train_cell_line,train_synergy,train_class1,train_ddi,train_graph1,train_graph2,max_epoch, batch_size,\n                                earlyStop_patience)\n\nap111,ap221= model_att1.predict( [test_f_drug1,test_f_drug2,test_cell_line,test_ddi,test_graph1,test_graph2])\n\npositive_negative_indices = np.where(test_class != 2)\nap221=ap221[positive_negative_indices]\ntest_class12=np.array(test_class)\ntest_class12=test_class12[positive_negative_indices]\nap221=ap221[:,1]\n\n\nap11=[]\ntest_synergy1=[]\nl=len(ap111)\nl1=int(l/2)\nfor i in range(l1) :\n    ap=(ap111[i]+ap111[i+l1])/2\n    ap11.append(ap)\n    aap=(test_synergy[i]+test_synergy[i+l1])/2\n    test_synergy1.append(aap)\n    \n    \nap22=[]\ntest_class1=[]\nl=len(ap221)\nl1=int(l/2)\nfor i in range(l1) :\n    ap=(ap221[i]+ap221[i+l1])/2\n    ap22.append(ap)\n    aap=(test_class12[i]+test_class12[i+l1])/2\n    test_class1.append(aap)\n    \n    \n    \nasynergy_error1=mean_squared_error(test_synergy1, ap11)\nasynergy_error11=mean_absolute_error(test_synergy1, ap11)\nasynergy_error21=r2_score(test_synergy1, ap11)\n\nprint(\"msynergy_mean_squared_error\",asynergy_error1)\nprint(\"mclass_mean_squared_error\",asynergy_error11)\nprint(\"msynergy_r2_score\",asynergy_error21)\n\nasynergy_pear1= pearsonr(test_synergy1, ap11)\nasynergy_spear1= spearmanr(test_synergy1, ap11)\nprint(\"msynergy_pear\",asynergy_pear1)\nprint(\"msynergy_spear\",asynergy_spear1)\n\naap2=convert_tobin(ap22)\n\naclass_error1=roc_auc_score(test_class1, ap22)\naclass_error11=accuracy_score(test_class1, aap2)\naclass_error21=cohen_kappa_score(test_class1, aap2)\n\nprint(\"msclass_roc_curve\",aclass_error1)\nprint(\"mclass_accuracy_scorer\",aclass_error11)\nprint(\"mclass_cohen_kappa_score\",aclass_error21)\n\n\naclass_pear1= precision_score(test_class1, aap2)\naclass_spear1= average_precision_score(test_class1, ap22)\nprint(\"mclass_precision_score\",aclass_pear1)\nprint(\"mclass_average_precision_score\",aclass_spear1)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-13T10:34:26.602055Z","iopub.execute_input":"2023-07-13T10:34:26.602501Z","iopub.status.idle":"2023-07-13T13:29:54.008256Z","shell.execute_reply.started":"2023-07-13T10:34:26.602383Z","shell.execute_reply":"2023-07-13T13:29:54.007216Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Model: \"model_2\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n input2 (InputLayer)            [(None, 394)]        0           []                               \n                                                                                                  \n input1 (InputLayer)            [(None, 394)]        0           []                               \n                                                                                                  \n dense_3 (Dense)                (None, 1024)         404480      ['input2[0][0]']                 \n                                                                                                  \n dense (Dense)                  (None, 1024)         404480      ['input1[0][0]']                 \n                                                                                                  \n dropout_2 (Dropout)            (None, 1024)         0           ['dense_3[0][0]']                \n                                                                                                  \n graph2 (InputLayer)            [(None, 455)]        0           []                               \n                                                                                                  \n dropout (Dropout)              (None, 1024)         0           ['dense[0][0]']                  \n                                                                                                  \n graph1 (InputLayer)            [(None, 455)]        0           []                               \n                                                                                                  \n dense_4 (Dense)                (None, 512)          524800      ['dropout_2[0][0]']              \n                                                                                                  \n dense_11 (Dense)               (None, 258)          117648      ['graph2[0][0]']                 \n                                                                                                  \n dense_1 (Dense)                (None, 512)          524800      ['dropout[0][0]']                \n                                                                                                  \n dense_9 (Dense)                (None, 258)          117648      ['graph1[0][0]']                 \n                                                                                                  \n input_1 (InputLayer)           [(None, 875)]        0           []                               \n                                                                                                  \n ddi (InputLayer)               [(None, 2048)]       0           []                               \n                                                                                                  \n dropout_3 (Dropout)            (None, 512)          0           ['dense_4[0][0]']                \n                                                                                                  \n dropout_7 (Dropout)            (None, 258)          0           ['dense_11[0][0]']               \n                                                                                                  \n dropout_1 (Dropout)            (None, 512)          0           ['dense_1[0][0]']                \n                                                                                                  \n dropout_6 (Dropout)            (None, 258)          0           ['dense_9[0][0]']                \n                                                                                                  \n dense_6 (Dense)                (None, 512)          448512      ['input_1[0][0]']                \n                                                                                                  \n dense_19 (Dense)               (None, 1024)         2098176     ['ddi[0][0]']                    \n                                                                                                  \n dense_5 (Dense)                (None, 256)          131328      ['dropout_3[0][0]']              \n                                                                                                  \n dense_12 (Dense)               (None, 128)          33152       ['dropout_7[0][0]']              \n                                                                                                  \n dense_2 (Dense)                (None, 256)          131328      ['dropout_1[0][0]']              \n                                                                                                  \n dense_10 (Dense)               (None, 128)          33152       ['dropout_6[0][0]']              \n                                                                                                  \n dropout_4 (Dropout)            (None, 512)          0           ['dense_6[0][0]']                \n                                                                                                  \n dropout_12 (Dropout)           (None, 1024)         0           ['dense_19[0][0]']               \n                                                                                                  \n concatenate_1 (Concatenate)    (None, 384)          0           ['dense_5[0][0]',                \n                                                                  'dense_12[0][0]']               \n                                                                                                  \n concatenate (Concatenate)      (None, 384)          0           ['dense_2[0][0]',                \n                                                                  'dense_10[0][0]']               \n                                                                                                  \n dense_7 (Dense)                (None, 265)          135945      ['dropout_4[0][0]']              \n                                                                                                  \n dense_20 (Dense)               (None, 512)          524800      ['dropout_12[0][0]']             \n                                                                                                  \n dense_16 (Dense)               (None, 256)          98560       ['concatenate_1[0][0]']          \n                                                                                                  \n dense_13 (Dense)               (None, 256)          98560       ['concatenate[0][0]']            \n                                                                                                  \n dropout_5 (Dropout)            (None, 265)          0           ['dense_7[0][0]']                \n                                                                                                  \n dropout_13 (Dropout)           (None, 512)          0           ['dense_20[0][0]']               \n                                                                                                  \n dropout_10 (Dropout)           (None, 256)          0           ['dense_16[0][0]']               \n                                                                                                  \n dropout_8 (Dropout)            (None, 256)          0           ['dense_13[0][0]']               \n                                                                                                  \n dense_8 (Dense)                (None, 128)          34048       ['dropout_5[0][0]']              \n                                                                                                  \n dense_21 (Dense)               (None, 256)          131328      ['dropout_13[0][0]']             \n                                                                                                  \n dense_17 (Dense)               (None, 512)          131584      ['dropout_10[0][0]']             \n                                                                                                  \n dense_14 (Dense)               (None, 512)          131584      ['dropout_8[0][0]']              \n                                                                                                  \n multi_head_attention (MultiHea  (None, None, 128)   98816       ['dense_8[0][0]',                \n dAttention)                                                      'dense_21[0][0]',               \n                                                                  'dense_21[0][0]']               \n                                                                                                  \n dropout_11 (Dropout)           (None, 512)          0           ['dense_17[0][0]']               \n                                                                                                  \n dropout_9 (Dropout)            (None, 512)          0           ['dense_14[0][0]']               \n                                                                                                  \n reshape (Reshape)              (None, 128)          0           ['multi_head_attention[0][0]']   \n                                                                                                  \n dense_18 (Dense)               (None, 256)          131328      ['dropout_11[0][0]']             \n                                                                                                  \n dense_15 (Dense)               (None, 256)          131328      ['dropout_9[0][0]']              \n                                                                                                  \n concatenate_2 (Concatenate)    (None, 256)          0           ['reshape[0][0]',                \n                                                                  'dense_8[0][0]']                \n                                                                                                  \n concatenate_3 (Concatenate)    (None, 768)          0           ['dense_18[0][0]',               \n                                                                  'dense_15[0][0]',               \n                                                                  'concatenate_2[0][0]']          \n                                                                                                  \n batch_normalization (BatchNorm  (None, 768)         3072        ['concatenate_3[0][0]']          \n alization)                                                                                       \n                                                                                                  \n multi_head_attention_1 (MultiH  (None, None, 768)   2362368     ['batch_normalization[0][0]',    \n eadAttention)                                                    'batch_normalization[0][0]',    \n                                                                  'batch_normalization[0][0]']    \n                                                                                                  \n multi_head_attention_2 (MultiH  (None, None, 768)   2362368     ['batch_normalization[0][0]',    \n eadAttention)                                                    'batch_normalization[0][0]',    \n                                                                  'batch_normalization[0][0]']    \n                                                                                                  \n reshape_1 (Reshape)            (None, 768)          0           ['multi_head_attention_1[0][0]'] \n                                                                                                  \n reshape_2 (Reshape)            (None, 768)          0           ['multi_head_attention_2[0][0]'] \n                                                                                                  \n concatenate_4 (Concatenate)    (None, 1536)         0           ['reshape_1[0][0]',              \n                                                                  'batch_normalization[0][0]']    \n                                                                                                  \n concatenate_5 (Concatenate)    (None, 1536)         0           ['reshape_2[0][0]',              \n                                                                  'batch_normalization[0][0]']    \n                                                                                                  \n cross_stitch (CrossStitch)     ((None, 1536),       4           ['concatenate_4[0][0]',          \n                                 (None, 1536))                    'concatenate_5[0][0]']          \n                                                                                                  \n dense_34 (Dense)               (None, 2048)         3147776     ['cross_stitch[0][0]']           \n                                                                                                  \n dense_35 (Dense)               (None, 2048)         3147776     ['cross_stitch[0][1]']           \n                                                                                                  \n cross_stitch_1 (CrossStitch)   ((None, 2048),       4           ['dense_34[0][0]',               \n                                 (None, 2048))                    'dense_35[0][0]']               \n                                                                                                  \n concatenate_6 (Concatenate)    (None, 3584)         0           ['cross_stitch_1[0][0]',         \n                                                                  'concatenate_4[0][0]']          \n                                                                                                  \n fsynergy1 (Dense)              (None, 128)          458880      ['concatenate_6[0][0]']          \n                                                                                                  \n p_re_lu (PReLU)                (None, 128)          128         ['fsynergy1[0][0]']              \n                                                                                                  \n concatenate_7 (Concatenate)    (None, 3584)         0           ['cross_stitch_1[0][1]',         \n                                                                  'concatenate_5[0][0]']          \n                                                                                                  \n fsynergy2 (Dense)              (None, 64)           8256        ['p_re_lu[0][0]']                \n                                                                                                  \n fclass1 (Dense)                (None, 128)          458880      ['concatenate_7[0][0]']          \n                                                                                                  \n p_re_lu_1 (PReLU)              (None, 64)           64          ['fsynergy2[0][0]']              \n                                                                                                  \n fclass2 (Dense)                (None, 64)           8256        ['fclass1[0][0]']                \n                                                                                                  \n synergy (Dense)                (None, 1)            65          ['p_re_lu_1[0][0]']              \n                                                                                                  \n class (Dense)                  (None, 3)            195         ['fclass2[0][0]']                \n                                                                                                  \n==================================================================================================\nTotal params: 18,575,477\nTrainable params: 18,573,941\nNon-trainable params: 1,536\n__________________________________________________________________________________________________\nNone\nEpoch 1/1000\n570/570 [==============================] - 17s 18ms/step - loss: 952.9340 - synergy_loss: 473.4882 - class_loss: 0.9371\nEpoch 2/1000\n570/570 [==============================] - 11s 18ms/step - loss: 864.2462 - synergy_loss: 414.2113 - class_loss: 0.8409\nEpoch 3/1000\n570/570 [==============================] - 11s 19ms/step - loss: 825.9214 - synergy_loss: 402.2000 - class_loss: 0.8153\nEpoch 4/1000\n570/570 [==============================] - 10s 18ms/step - loss: 796.0976 - synergy_loss: 395.9525 - class_loss: 0.8011\nEpoch 5/1000\n570/570 [==============================] - 10s 18ms/step - loss: 770.5201 - synergy_loss: 390.9823 - class_loss: 0.7929\nEpoch 6/1000\n570/570 [==============================] - 11s 19ms/step - loss: 746.9644 - synergy_loss: 385.1856 - class_loss: 0.7859\nEpoch 7/1000\n570/570 [==============================] - 10s 18ms/step - loss: 725.4890 - synergy_loss: 378.5342 - class_loss: 0.7786\nEpoch 8/1000\n570/570 [==============================] - 10s 18ms/step - loss: 709.5057 - synergy_loss: 374.7067 - class_loss: 0.7761\nEpoch 9/1000\n570/570 [==============================] - 11s 19ms/step - loss: 692.3932 - synergy_loss: 367.1859 - class_loss: 0.7709\nEpoch 10/1000\n570/570 [==============================] - 11s 19ms/step - loss: 677.8642 - synergy_loss: 360.0767 - class_loss: 0.7669\nEpoch 11/1000\n570/570 [==============================] - 10s 18ms/step - loss: 664.8287 - synergy_loss: 352.6577 - class_loss: 0.7618\nEpoch 12/1000\n570/570 [==============================] - 11s 19ms/step - loss: 654.5427 - synergy_loss: 346.3633 - class_loss: 0.7604\nEpoch 13/1000\n570/570 [==============================] - 11s 18ms/step - loss: 645.8317 - synergy_loss: 340.4170 - class_loss: 0.7560\nEpoch 14/1000\n570/570 [==============================] - 10s 18ms/step - loss: 640.7554 - synergy_loss: 337.2737 - class_loss: 0.7535\nEpoch 15/1000\n570/570 [==============================] - 11s 19ms/step - loss: 634.1328 - synergy_loss: 331.8553 - class_loss: 0.7498\nEpoch 16/1000\n570/570 [==============================] - 10s 18ms/step - loss: 629.0562 - synergy_loss: 327.5668 - class_loss: 0.7443\nEpoch 17/1000\n570/570 [==============================] - 10s 18ms/step - loss: 626.1959 - synergy_loss: 325.2150 - class_loss: 0.7422\nEpoch 18/1000\n570/570 [==============================] - 11s 19ms/step - loss: 622.1831 - synergy_loss: 321.6899 - class_loss: 0.7398\nEpoch 19/1000\n570/570 [==============================] - 10s 18ms/step - loss: 618.0851 - synergy_loss: 318.0221 - class_loss: 0.7356\nEpoch 20/1000\n570/570 [==============================] - 10s 18ms/step - loss: 615.4831 - synergy_loss: 315.8391 - class_loss: 0.7332\nEpoch 21/1000\n570/570 [==============================] - 11s 20ms/step - loss: 612.1180 - synergy_loss: 312.8023 - class_loss: 0.7301\nEpoch 22/1000\n570/570 [==============================] - 11s 19ms/step - loss: 609.1364 - synergy_loss: 310.1665 - class_loss: 0.7275\nEpoch 23/1000\n570/570 [==============================] - 11s 19ms/step - loss: 605.4861 - synergy_loss: 306.7925 - class_loss: 0.7245\nEpoch 24/1000\n570/570 [==============================] - 11s 19ms/step - loss: 601.3288 - synergy_loss: 302.9069 - class_loss: 0.7233\nEpoch 25/1000\n570/570 [==============================] - 10s 18ms/step - loss: 596.9879 - synergy_loss: 298.8194 - class_loss: 0.7188\nEpoch 26/1000\n570/570 [==============================] - 11s 18ms/step - loss: 595.5022 - synergy_loss: 297.5671 - class_loss: 0.7174\nEpoch 27/1000\n570/570 [==============================] - 11s 19ms/step - loss: 589.9712 - synergy_loss: 292.2550 - class_loss: 0.7151\nEpoch 28/1000\n570/570 [==============================] - 10s 18ms/step - loss: 590.8456 - synergy_loss: 293.4179 - class_loss: 0.7138\nEpoch 29/1000\n570/570 [==============================] - 10s 18ms/step - loss: 583.1375 - synergy_loss: 285.8770 - class_loss: 0.7090\nEpoch 30/1000\n570/570 [==============================] - 11s 19ms/step - loss: 582.0752 - synergy_loss: 285.1075 - class_loss: 0.7075\nEpoch 31/1000\n570/570 [==============================] - 10s 18ms/step - loss: 578.9772 - synergy_loss: 282.1822 - class_loss: 0.7062\nEpoch 32/1000\n570/570 [==============================] - 11s 18ms/step - loss: 575.0172 - synergy_loss: 278.4602 - class_loss: 0.7021\nEpoch 33/1000\n570/570 [==============================] - 11s 19ms/step - loss: 571.1241 - synergy_loss: 274.7558 - class_loss: 0.6988\nEpoch 34/1000\n570/570 [==============================] - 11s 18ms/step - loss: 570.4962 - synergy_loss: 274.4068 - class_loss: 0.6985\nEpoch 35/1000\n570/570 [==============================] - 10s 18ms/step - loss: 565.5714 - synergy_loss: 269.5923 - class_loss: 0.6917\nEpoch 36/1000\n570/570 [==============================] - 11s 19ms/step - loss: 566.1783 - synergy_loss: 270.3828 - class_loss: 0.6904\nEpoch 37/1000\n570/570 [==============================] - 10s 18ms/step - loss: 560.1533 - synergy_loss: 264.5468 - class_loss: 0.6872\nEpoch 38/1000\n570/570 [==============================] - 10s 18ms/step - loss: 560.2835 - synergy_loss: 264.8587 - class_loss: 0.6857\nEpoch 39/1000\n570/570 [==============================] - 11s 19ms/step - loss: 556.8672 - synergy_loss: 261.6150 - class_loss: 0.6836\nEpoch 40/1000\n570/570 [==============================] - 11s 18ms/step - loss: 555.3051 - synergy_loss: 260.2440 - class_loss: 0.6780\nEpoch 41/1000\n570/570 [==============================] - 11s 19ms/step - loss: 552.7828 - synergy_loss: 257.8331 - class_loss: 0.6774\nEpoch 42/1000\n570/570 [==============================] - 11s 19ms/step - loss: 546.4820 - synergy_loss: 251.7059 - class_loss: 0.6738\nEpoch 43/1000\n570/570 [==============================] - 11s 19ms/step - loss: 545.7139 - synergy_loss: 251.1406 - class_loss: 0.6704\nEpoch 44/1000\n570/570 [==============================] - 11s 19ms/step - loss: 542.7469 - synergy_loss: 248.3317 - class_loss: 0.6673\nEpoch 45/1000\n570/570 [==============================] - 11s 19ms/step - loss: 540.8992 - synergy_loss: 246.5907 - class_loss: 0.6663\nEpoch 46/1000\n570/570 [==============================] - 10s 18ms/step - loss: 539.6577 - synergy_loss: 245.5066 - class_loss: 0.6653\nEpoch 47/1000\n570/570 [==============================] - 11s 19ms/step - loss: 538.3740 - synergy_loss: 244.4619 - class_loss: 0.6594\nEpoch 48/1000\n570/570 [==============================] - 11s 19ms/step - loss: 536.0189 - synergy_loss: 242.2317 - class_loss: 0.6578\nEpoch 49/1000\n570/570 [==============================] - 11s 19ms/step - loss: 533.1617 - synergy_loss: 239.4764 - class_loss: 0.6562\nEpoch 50/1000\n570/570 [==============================] - 11s 19ms/step - loss: 531.1202 - synergy_loss: 237.6415 - class_loss: 0.6534\nEpoch 51/1000\n570/570 [==============================] - 11s 19ms/step - loss: 531.0262 - synergy_loss: 237.6364 - class_loss: 0.6527\nEpoch 52/1000\n570/570 [==============================] - 11s 19ms/step - loss: 527.1190 - synergy_loss: 233.9325 - class_loss: 0.6474\nEpoch 53/1000\n570/570 [==============================] - 11s 19ms/step - loss: 523.7298 - synergy_loss: 230.6546 - class_loss: 0.6475\nEpoch 54/1000\n570/570 [==============================] - 11s 19ms/step - loss: 521.2439 - synergy_loss: 228.3762 - class_loss: 0.6449\nEpoch 55/1000\n570/570 [==============================] - 11s 19ms/step - loss: 521.2031 - synergy_loss: 228.4379 - class_loss: 0.6420\nEpoch 56/1000\n570/570 [==============================] - 11s 19ms/step - loss: 521.1995 - synergy_loss: 228.5992 - class_loss: 0.6424\nEpoch 57/1000\n570/570 [==============================] - 11s 19ms/step - loss: 517.1656 - synergy_loss: 224.7160 - class_loss: 0.6392\nEpoch 58/1000\n570/570 [==============================] - 11s 19ms/step - loss: 516.8844 - synergy_loss: 224.5884 - class_loss: 0.6367\nEpoch 59/1000\n570/570 [==============================] - 11s 19ms/step - loss: 512.0877 - synergy_loss: 219.9203 - class_loss: 0.6351\nEpoch 60/1000\n570/570 [==============================] - 11s 18ms/step - loss: 513.6941 - synergy_loss: 221.6836 - class_loss: 0.6328\nEpoch 61/1000\n570/570 [==============================] - 11s 19ms/step - loss: 513.2791 - synergy_loss: 221.4162 - class_loss: 0.6304\nEpoch 62/1000\n570/570 [==============================] - 11s 19ms/step - loss: 510.4392 - synergy_loss: 218.7735 - class_loss: 0.6290\nEpoch 63/1000\n570/570 [==============================] - 10s 18ms/step - loss: 506.4892 - synergy_loss: 214.9207 - class_loss: 0.6264\nEpoch 64/1000\n570/570 [==============================] - 11s 18ms/step - loss: 506.8940 - synergy_loss: 215.4712 - class_loss: 0.6261\nEpoch 65/1000\n570/570 [==============================] - 11s 19ms/step - loss: 508.1361 - synergy_loss: 216.9221 - class_loss: 0.6242\nEpoch 66/1000\n570/570 [==============================] - 11s 19ms/step - loss: 504.9428 - synergy_loss: 213.8260 - class_loss: 0.6214\nEpoch 67/1000\n570/570 [==============================] - 11s 18ms/step - loss: 502.0026 - synergy_loss: 211.0606 - class_loss: 0.6204\nEpoch 68/1000\n570/570 [==============================] - 11s 19ms/step - loss: 497.6014 - synergy_loss: 206.7887 - class_loss: 0.6189\nEpoch 69/1000\n570/570 [==============================] - 11s 19ms/step - loss: 497.9096 - synergy_loss: 207.2532 - class_loss: 0.6164\nEpoch 70/1000\n570/570 [==============================] - 10s 18ms/step - loss: 494.0768 - synergy_loss: 203.5619 - class_loss: 0.6120\nEpoch 71/1000\n570/570 [==============================] - 11s 19ms/step - loss: 492.1310 - synergy_loss: 201.7541 - class_loss: 0.6127\nEpoch 72/1000\n570/570 [==============================] - 10s 18ms/step - loss: 490.2341 - synergy_loss: 200.0172 - class_loss: 0.6102\nEpoch 73/1000\n570/570 [==============================] - 11s 18ms/step - loss: 489.7850 - synergy_loss: 199.6947 - class_loss: 0.6077\nEpoch 74/1000\n570/570 [==============================] - 11s 19ms/step - loss: 488.8756 - synergy_loss: 198.9506 - class_loss: 0.6096\nEpoch 75/1000\n570/570 [==============================] - 10s 18ms/step - loss: 488.3217 - synergy_loss: 198.5121 - class_loss: 0.6064\nEpoch 76/1000\n570/570 [==============================] - 10s 18ms/step - loss: 487.2825 - synergy_loss: 197.6228 - class_loss: 0.6048\nEpoch 77/1000\n570/570 [==============================] - 11s 20ms/step - loss: 484.0945 - synergy_loss: 194.6022 - class_loss: 0.6032\nEpoch 78/1000\n570/570 [==============================] - 10s 18ms/step - loss: 482.5088 - synergy_loss: 193.1294 - class_loss: 0.6015\nEpoch 79/1000\n570/570 [==============================] - 10s 18ms/step - loss: 481.5943 - synergy_loss: 192.3547 - class_loss: 0.6010\nEpoch 80/1000\n570/570 [==============================] - 11s 19ms/step - loss: 481.2176 - synergy_loss: 192.1356 - class_loss: 0.5995\nEpoch 81/1000\n570/570 [==============================] - 11s 19ms/step - loss: 477.4383 - synergy_loss: 188.4333 - class_loss: 0.5972\nEpoch 82/1000\n570/570 [==============================] - 11s 19ms/step - loss: 476.3752 - synergy_loss: 187.5580 - class_loss: 0.5927\nEpoch 83/1000\n570/570 [==============================] - 11s 19ms/step - loss: 474.7922 - synergy_loss: 186.1195 - class_loss: 0.5943\nEpoch 84/1000\n570/570 [==============================] - 11s 19ms/step - loss: 474.1151 - synergy_loss: 185.5771 - class_loss: 0.5933\nEpoch 85/1000\n570/570 [==============================] - 11s 18ms/step - loss: 469.9749 - synergy_loss: 181.5651 - class_loss: 0.5899\nEpoch 86/1000\n570/570 [==============================] - 11s 20ms/step - loss: 468.5207 - synergy_loss: 180.2574 - class_loss: 0.5874\nEpoch 87/1000\n570/570 [==============================] - 11s 19ms/step - loss: 470.4775 - synergy_loss: 182.3695 - class_loss: 0.5917\nEpoch 88/1000\n570/570 [==============================] - 10s 18ms/step - loss: 469.2130 - synergy_loss: 181.2608 - class_loss: 0.5893\nEpoch 89/1000\n570/570 [==============================] - 11s 20ms/step - loss: 463.0631 - synergy_loss: 175.2367 - class_loss: 0.5856\nEpoch 90/1000\n570/570 [==============================] - 11s 19ms/step - loss: 465.6359 - synergy_loss: 177.9818 - class_loss: 0.5877\nEpoch 91/1000\n570/570 [==============================] - 11s 18ms/step - loss: 462.8155 - synergy_loss: 175.2711 - class_loss: 0.5829\nEpoch 92/1000\n570/570 [==============================] - 11s 20ms/step - loss: 462.1248 - synergy_loss: 174.7431 - class_loss: 0.5840\nEpoch 93/1000\n570/570 [==============================] - 10s 18ms/step - loss: 457.5695 - synergy_loss: 170.3380 - class_loss: 0.5817\nEpoch 94/1000\n570/570 [==============================] - 10s 18ms/step - loss: 456.7836 - synergy_loss: 169.6784 - class_loss: 0.5792\nEpoch 95/1000\n570/570 [==============================] - 11s 19ms/step - loss: 457.5075 - synergy_loss: 170.5555 - class_loss: 0.5766\nEpoch 96/1000\n570/570 [==============================] - 10s 18ms/step - loss: 453.3807 - synergy_loss: 166.5649 - class_loss: 0.5771\nEpoch 97/1000\n570/570 [==============================] - 10s 18ms/step - loss: 454.2141 - synergy_loss: 167.5339 - class_loss: 0.5757\nEpoch 98/1000\n570/570 [==============================] - 11s 19ms/step - loss: 453.6631 - synergy_loss: 167.1344 - class_loss: 0.5768\nEpoch 99/1000\n570/570 [==============================] - 10s 18ms/step - loss: 451.3433 - synergy_loss: 164.9644 - class_loss: 0.5755\nEpoch 100/1000\n570/570 [==============================] - 11s 19ms/step - loss: 449.4990 - synergy_loss: 163.2403 - class_loss: 0.5741\nEpoch 101/1000\n570/570 [==============================] - 11s 19ms/step - loss: 451.0526 - synergy_loss: 164.9533 - class_loss: 0.5705\nEpoch 102/1000\n570/570 [==============================] - 10s 18ms/step - loss: 445.7826 - synergy_loss: 159.7888 - class_loss: 0.5662\nEpoch 103/1000\n570/570 [==============================] - 10s 18ms/step - loss: 445.0882 - synergy_loss: 159.2184 - class_loss: 0.5693\nEpoch 104/1000\n570/570 [==============================] - 11s 19ms/step - loss: 447.2255 - synergy_loss: 161.5550 - class_loss: 0.5675\nEpoch 105/1000\n570/570 [==============================] - 10s 18ms/step - loss: 440.1221 - synergy_loss: 154.5846 - class_loss: 0.5646\nEpoch 106/1000\n570/570 [==============================] - 10s 18ms/step - loss: 440.9599 - synergy_loss: 155.5650 - class_loss: 0.5657\nEpoch 107/1000\n570/570 [==============================] - 11s 19ms/step - loss: 441.3651 - synergy_loss: 156.1301 - class_loss: 0.5653\nEpoch 108/1000\n570/570 [==============================] - 10s 18ms/step - loss: 437.6741 - synergy_loss: 152.5833 - class_loss: 0.5618\nEpoch 109/1000\n570/570 [==============================] - 10s 18ms/step - loss: 435.5127 - synergy_loss: 150.5396 - class_loss: 0.5605\nEpoch 110/1000\n570/570 [==============================] - 11s 19ms/step - loss: 435.9365 - synergy_loss: 151.1071 - class_loss: 0.5611\nEpoch 111/1000\n570/570 [==============================] - 11s 19ms/step - loss: 433.0804 - synergy_loss: 148.3851 - class_loss: 0.5584\nEpoch 112/1000\n570/570 [==============================] - 10s 18ms/step - loss: 430.7822 - synergy_loss: 146.2288 - class_loss: 0.5573\nEpoch 113/1000\n570/570 [==============================] - 11s 19ms/step - loss: 429.5801 - synergy_loss: 145.1648 - class_loss: 0.5553\nEpoch 114/1000\n570/570 [==============================] - 10s 18ms/step - loss: 431.6001 - synergy_loss: 147.3288 - class_loss: 0.5547\nEpoch 115/1000\n570/570 [==============================] - 11s 19ms/step - loss: 427.5902 - synergy_loss: 143.4800 - class_loss: 0.5522\nEpoch 116/1000\n570/570 [==============================] - 11s 19ms/step - loss: 427.1429 - synergy_loss: 143.1627 - class_loss: 0.5516\nEpoch 117/1000\n570/570 [==============================] - 10s 18ms/step - loss: 427.4999 - synergy_loss: 143.6607 - class_loss: 0.5495\nEpoch 118/1000\n570/570 [==============================] - 10s 18ms/step - loss: 423.8264 - synergy_loss: 140.1489 - class_loss: 0.5489\nEpoch 119/1000\n570/570 [==============================] - 11s 19ms/step - loss: 424.2098 - synergy_loss: 140.7228 - class_loss: 0.5468\nEpoch 120/1000\n570/570 [==============================] - 10s 18ms/step - loss: 422.6655 - synergy_loss: 139.3397 - class_loss: 0.5466\nEpoch 121/1000\n570/570 [==============================] - 10s 18ms/step - loss: 421.0917 - synergy_loss: 137.9015 - class_loss: 0.5463\nEpoch 122/1000\n570/570 [==============================] - 11s 19ms/step - loss: 418.1171 - synergy_loss: 135.0626 - class_loss: 0.5422\nEpoch 123/1000\n570/570 [==============================] - 11s 19ms/step - loss: 420.8504 - synergy_loss: 137.9544 - class_loss: 0.5453\nEpoch 124/1000\n570/570 [==============================] - 10s 18ms/step - loss: 416.8621 - synergy_loss: 134.0932 - class_loss: 0.5437\nEpoch 125/1000\n570/570 [==============================] - 11s 19ms/step - loss: 419.3046 - synergy_loss: 136.6976 - class_loss: 0.5404\nEpoch 126/1000\n570/570 [==============================] - 10s 18ms/step - loss: 418.6422 - synergy_loss: 136.1556 - class_loss: 0.5410\nEpoch 127/1000\n570/570 [==============================] - 10s 18ms/step - loss: 415.3609 - synergy_loss: 132.9929 - class_loss: 0.5410\nEpoch 128/1000\n570/570 [==============================] - 11s 19ms/step - loss: 412.7937 - synergy_loss: 130.5821 - class_loss: 0.5366\nEpoch 129/1000\n570/570 [==============================] - 10s 18ms/step - loss: 412.3169 - synergy_loss: 130.2680 - class_loss: 0.5359\nEpoch 130/1000\n570/570 [==============================] - 11s 18ms/step - loss: 411.3988 - synergy_loss: 129.4851 - class_loss: 0.5376\nEpoch 131/1000\n570/570 [==============================] - 11s 19ms/step - loss: 409.4800 - synergy_loss: 127.6989 - class_loss: 0.5341\nEpoch 132/1000\n570/570 [==============================] - 10s 18ms/step - loss: 410.0929 - synergy_loss: 128.4523 - class_loss: 0.5349\nEpoch 133/1000\n570/570 [==============================] - 10s 18ms/step - loss: 408.0406 - synergy_loss: 126.5611 - class_loss: 0.5317\nEpoch 134/1000\n570/570 [==============================] - 11s 19ms/step - loss: 405.7451 - synergy_loss: 124.4059 - class_loss: 0.5347\nEpoch 135/1000\n570/570 [==============================] - 10s 18ms/step - loss: 405.3060 - synergy_loss: 124.1115 - class_loss: 0.5301\nEpoch 136/1000\n570/570 [==============================] - 10s 18ms/step - loss: 406.4818 - synergy_loss: 125.4764 - class_loss: 0.5304\nEpoch 137/1000\n570/570 [==============================] - 11s 19ms/step - loss: 404.7577 - synergy_loss: 123.8941 - class_loss: 0.5279\nEpoch 138/1000\n570/570 [==============================] - 10s 18ms/step - loss: 404.3987 - synergy_loss: 123.6736 - class_loss: 0.5283\nEpoch 139/1000\n570/570 [==============================] - 11s 19ms/step - loss: 401.4065 - synergy_loss: 120.8321 - class_loss: 0.5226\nEpoch 140/1000\n570/570 [==============================] - 11s 19ms/step - loss: 399.8676 - synergy_loss: 119.4281 - class_loss: 0.5239\nEpoch 141/1000\n570/570 [==============================] - 10s 18ms/step - loss: 401.3837 - synergy_loss: 121.0775 - class_loss: 0.5213\nEpoch 142/1000\n570/570 [==============================] - 10s 18ms/step - loss: 400.4106 - synergy_loss: 120.2357 - class_loss: 0.5220\nEpoch 143/1000\n570/570 [==============================] - 11s 19ms/step - loss: 399.6646 - synergy_loss: 119.6330 - class_loss: 0.5222\nEpoch 144/1000\n570/570 [==============================] - 10s 18ms/step - loss: 397.9317 - synergy_loss: 118.0495 - class_loss: 0.5239\nEpoch 145/1000\n570/570 [==============================] - 11s 19ms/step - loss: 396.1161 - synergy_loss: 116.3697 - class_loss: 0.5200\nEpoch 146/1000\n570/570 [==============================] - 11s 19ms/step - loss: 394.5699 - synergy_loss: 114.9881 - class_loss: 0.5198\nEpoch 147/1000\n570/570 [==============================] - 11s 19ms/step - loss: 394.4899 - synergy_loss: 115.0986 - class_loss: 0.5168\nEpoch 148/1000\n570/570 [==============================] - 10s 18ms/step - loss: 393.9991 - synergy_loss: 114.7294 - class_loss: 0.5179\nEpoch 149/1000\n570/570 [==============================] - 11s 19ms/step - loss: 395.5121 - synergy_loss: 116.4072 - class_loss: 0.5166\nEpoch 150/1000\n570/570 [==============================] - 10s 18ms/step - loss: 391.3135 - synergy_loss: 112.3465 - class_loss: 0.5139\nEpoch 151/1000\n570/570 [==============================] - 10s 18ms/step - loss: 390.9738 - synergy_loss: 112.1775 - class_loss: 0.5155\nEpoch 152/1000\n570/570 [==============================] - 11s 19ms/step - loss: 389.1140 - synergy_loss: 110.4554 - class_loss: 0.5145\nEpoch 153/1000\n570/570 [==============================] - 11s 19ms/step - loss: 389.3729 - synergy_loss: 110.8739 - class_loss: 0.5103\nEpoch 154/1000\n570/570 [==============================] - 10s 18ms/step - loss: 387.5558 - synergy_loss: 109.1850 - class_loss: 0.5118\nEpoch 155/1000\n570/570 [==============================] - 11s 19ms/step - loss: 386.2736 - synergy_loss: 108.0385 - class_loss: 0.5103\nEpoch 156/1000\n570/570 [==============================] - 10s 18ms/step - loss: 386.8047 - synergy_loss: 108.7270 - class_loss: 0.5090\nEpoch 157/1000\n570/570 [==============================] - 10s 18ms/step - loss: 384.7148 - synergy_loss: 106.7625 - class_loss: 0.5097\nEpoch 158/1000\n570/570 [==============================] - 11s 19ms/step - loss: 385.5997 - synergy_loss: 107.7930 - class_loss: 0.5089\nEpoch 159/1000\n570/570 [==============================] - 10s 18ms/step - loss: 383.9472 - synergy_loss: 106.3107 - class_loss: 0.5053\nEpoch 160/1000\n570/570 [==============================] - 10s 18ms/step - loss: 383.0796 - synergy_loss: 105.6116 - class_loss: 0.5021\nEpoch 161/1000\n570/570 [==============================] - 11s 19ms/step - loss: 381.7283 - synergy_loss: 104.4066 - class_loss: 0.5030\nEpoch 162/1000\n570/570 [==============================] - 10s 18ms/step - loss: 382.3353 - synergy_loss: 105.1839 - class_loss: 0.5044\nEpoch 163/1000\n570/570 [==============================] - 10s 18ms/step - loss: 379.6448 - synergy_loss: 102.6116 - class_loss: 0.5030\nEpoch 164/1000\n570/570 [==============================] - 11s 19ms/step - loss: 379.7926 - synergy_loss: 102.9162 - class_loss: 0.4998\nEpoch 165/1000\n570/570 [==============================] - 10s 18ms/step - loss: 380.8604 - synergy_loss: 104.1461 - class_loss: 0.5012\nEpoch 166/1000\n570/570 [==============================] - 10s 18ms/step - loss: 378.1129 - synergy_loss: 101.5274 - class_loss: 0.4984\nEpoch 167/1000\n570/570 [==============================] - 11s 19ms/step - loss: 375.8778 - synergy_loss: 99.4799 - class_loss: 0.4974\nEpoch 168/1000\n570/570 [==============================] - 11s 18ms/step - loss: 376.3100 - synergy_loss: 100.0485 - class_loss: 0.4975\nEpoch 169/1000\n570/570 [==============================] - 11s 18ms/step - loss: 375.5526 - synergy_loss: 99.4112 - class_loss: 0.4969\nEpoch 170/1000\n570/570 [==============================] - 11s 19ms/step - loss: 374.9138 - synergy_loss: 98.9494 - class_loss: 0.4956\nEpoch 171/1000\n570/570 [==============================] - 10s 18ms/step - loss: 374.4586 - synergy_loss: 98.6496 - class_loss: 0.4933\nEpoch 172/1000\n570/570 [==============================] - 10s 18ms/step - loss: 373.8616 - synergy_loss: 98.2198 - class_loss: 0.4932\nEpoch 173/1000\n570/570 [==============================] - 11s 19ms/step - loss: 372.5574 - synergy_loss: 97.0581 - class_loss: 0.4945\nEpoch 174/1000\n570/570 [==============================] - 10s 18ms/step - loss: 372.4361 - synergy_loss: 97.1050 - class_loss: 0.4906\nEpoch 175/1000\n570/570 [==============================] - 10s 18ms/step - loss: 370.7214 - synergy_loss: 95.5381 - class_loss: 0.4872\nEpoch 176/1000\n570/570 [==============================] - 11s 19ms/step - loss: 371.1776 - synergy_loss: 96.1434 - class_loss: 0.4900\nEpoch 177/1000\n570/570 [==============================] - 10s 18ms/step - loss: 370.5230 - synergy_loss: 95.5932 - class_loss: 0.4915\nEpoch 178/1000\n570/570 [==============================] - 10s 18ms/step - loss: 367.6059 - synergy_loss: 92.8375 - class_loss: 0.4859\nEpoch 179/1000\n570/570 [==============================] - 11s 19ms/step - loss: 367.7464 - synergy_loss: 93.1337 - class_loss: 0.4872\nEpoch 180/1000\n570/570 [==============================] - 10s 18ms/step - loss: 366.8995 - synergy_loss: 92.4541 - class_loss: 0.4861\nEpoch 181/1000\n570/570 [==============================] - 10s 18ms/step - loss: 364.9962 - synergy_loss: 90.6587 - class_loss: 0.4836\nEpoch 182/1000\n570/570 [==============================] - 11s 19ms/step - loss: 365.1875 - synergy_loss: 91.0253 - class_loss: 0.4889\nEpoch 183/1000\n570/570 [==============================] - 10s 18ms/step - loss: 365.0310 - synergy_loss: 91.0626 - class_loss: 0.4825\nEpoch 184/1000\n570/570 [==============================] - 10s 18ms/step - loss: 364.5674 - synergy_loss: 90.7439 - class_loss: 0.4806\nEpoch 185/1000\n570/570 [==============================] - 11s 19ms/step - loss: 362.5398 - synergy_loss: 88.8804 - class_loss: 0.4830\nEpoch 186/1000\n570/570 [==============================] - 10s 18ms/step - loss: 364.6266 - synergy_loss: 91.1166 - class_loss: 0.4804\nEpoch 187/1000\n570/570 [==============================] - 10s 18ms/step - loss: 360.8193 - synergy_loss: 87.4746 - class_loss: 0.4779\nEpoch 188/1000\n570/570 [==============================] - 11s 19ms/step - loss: 361.4840 - synergy_loss: 88.2896 - class_loss: 0.4760\nEpoch 189/1000\n570/570 [==============================] - 10s 18ms/step - loss: 358.7408 - synergy_loss: 85.7256 - class_loss: 0.4765\nEpoch 190/1000\n570/570 [==============================] - 10s 18ms/step - loss: 358.7112 - synergy_loss: 85.8570 - class_loss: 0.4726\nEpoch 191/1000\n570/570 [==============================] - 11s 20ms/step - loss: 358.8691 - synergy_loss: 86.1672 - class_loss: 0.4762\nEpoch 192/1000\n570/570 [==============================] - 10s 18ms/step - loss: 360.1877 - synergy_loss: 87.6835 - class_loss: 0.4736\nEpoch 193/1000\n570/570 [==============================] - 11s 19ms/step - loss: 359.2576 - synergy_loss: 86.9096 - class_loss: 0.4731\nEpoch 194/1000\n570/570 [==============================] - 11s 19ms/step - loss: 356.1257 - synergy_loss: 83.9133 - class_loss: 0.4737\nEpoch 195/1000\n570/570 [==============================] - 10s 18ms/step - loss: 356.0830 - synergy_loss: 84.0181 - class_loss: 0.4715\nEpoch 196/1000\n570/570 [==============================] - 10s 18ms/step - loss: 354.8589 - synergy_loss: 82.9526 - class_loss: 0.4688\nEpoch 197/1000\n570/570 [==============================] - 11s 19ms/step - loss: 354.7369 - synergy_loss: 83.0234 - class_loss: 0.4710\nEpoch 198/1000\n570/570 [==============================] - 10s 18ms/step - loss: 353.4532 - synergy_loss: 81.9027 - class_loss: 0.4685\nEpoch 199/1000\n570/570 [==============================] - 10s 18ms/step - loss: 352.9425 - synergy_loss: 81.5593 - class_loss: 0.4674\nEpoch 200/1000\n570/570 [==============================] - 11s 19ms/step - loss: 352.9087 - synergy_loss: 81.6839 - class_loss: 0.4698\nEpoch 201/1000\n570/570 [==============================] - 10s 18ms/step - loss: 352.9754 - synergy_loss: 81.8931 - class_loss: 0.4678\nEpoch 202/1000\n570/570 [==============================] - 10s 18ms/step - loss: 351.6231 - synergy_loss: 80.7022 - class_loss: 0.4619\nEpoch 203/1000\n570/570 [==============================] - 11s 19ms/step - loss: 350.7686 - synergy_loss: 79.9762 - class_loss: 0.4671\nEpoch 204/1000\n570/570 [==============================] - 10s 18ms/step - loss: 350.9767 - synergy_loss: 80.3736 - class_loss: 0.4633\nEpoch 205/1000\n570/570 [==============================] - 10s 18ms/step - loss: 348.7773 - synergy_loss: 78.3452 - class_loss: 0.4633\nEpoch 206/1000\n570/570 [==============================] - 11s 19ms/step - loss: 348.1123 - synergy_loss: 77.8206 - class_loss: 0.4582\nEpoch 207/1000\n570/570 [==============================] - 10s 18ms/step - loss: 348.3556 - synergy_loss: 78.2362 - class_loss: 0.4618\nEpoch 208/1000\n570/570 [==============================] - 10s 18ms/step - loss: 346.9525 - synergy_loss: 77.0103 - class_loss: 0.4578\nEpoch 209/1000\n570/570 [==============================] - 11s 19ms/step - loss: 346.0886 - synergy_loss: 76.3271 - class_loss: 0.4576\nEpoch 210/1000\n570/570 [==============================] - 10s 18ms/step - loss: 345.9589 - synergy_loss: 76.3730 - class_loss: 0.4560\nEpoch 211/1000\n570/570 [==============================] - 10s 18ms/step - loss: 346.8354 - synergy_loss: 77.4260 - class_loss: 0.4570\nEpoch 212/1000\n570/570 [==============================] - 11s 19ms/step - loss: 344.4662 - synergy_loss: 75.2425 - class_loss: 0.4544\nEpoch 213/1000\n570/570 [==============================] - 10s 18ms/step - loss: 345.7130 - synergy_loss: 76.6589 - class_loss: 0.4582\nEpoch 214/1000\n570/570 [==============================] - 11s 18ms/step - loss: 343.6375 - synergy_loss: 74.7453 - class_loss: 0.4554\nEpoch 215/1000\n570/570 [==============================] - 11s 19ms/step - loss: 342.5909 - synergy_loss: 73.8574 - class_loss: 0.4520\nEpoch 216/1000\n570/570 [==============================] - 10s 18ms/step - loss: 342.6655 - synergy_loss: 74.0916 - class_loss: 0.4517\nEpoch 217/1000\n570/570 [==============================] - 10s 18ms/step - loss: 342.8640 - synergy_loss: 74.4432 - class_loss: 0.4505\nEpoch 218/1000\n570/570 [==============================] - 11s 19ms/step - loss: 340.5441 - synergy_loss: 72.2984 - class_loss: 0.4498\nEpoch 219/1000\n570/570 [==============================] - 10s 18ms/step - loss: 340.5704 - synergy_loss: 72.4696 - class_loss: 0.4515\nEpoch 220/1000\n570/570 [==============================] - 10s 18ms/step - loss: 339.6667 - synergy_loss: 71.7531 - class_loss: 0.4496\nEpoch 221/1000\n570/570 [==============================] - 11s 19ms/step - loss: 340.1843 - synergy_loss: 72.4101 - class_loss: 0.4518\nEpoch 222/1000\n570/570 [==============================] - 10s 18ms/step - loss: 338.4843 - synergy_loss: 70.9160 - class_loss: 0.4430\nEpoch 223/1000\n570/570 [==============================] - 10s 18ms/step - loss: 338.4242 - synergy_loss: 71.0179 - class_loss: 0.4484\nEpoch 224/1000\n570/570 [==============================] - 11s 19ms/step - loss: 337.1993 - synergy_loss: 69.9748 - class_loss: 0.4409\nEpoch 225/1000\n570/570 [==============================] - 10s 18ms/step - loss: 336.3791 - synergy_loss: 69.3563 - class_loss: 0.4455\nEpoch 226/1000\n570/570 [==============================] - 10s 18ms/step - loss: 336.0985 - synergy_loss: 69.2555 - class_loss: 0.4427\nEpoch 227/1000\n570/570 [==============================] - 11s 19ms/step - loss: 335.7373 - synergy_loss: 69.0495 - class_loss: 0.4441\nEpoch 228/1000\n570/570 [==============================] - 10s 18ms/step - loss: 335.9392 - synergy_loss: 69.4342 - class_loss: 0.4397\nEpoch 229/1000\n570/570 [==============================] - 10s 18ms/step - loss: 334.2050 - synergy_loss: 67.8711 - class_loss: 0.4423\nEpoch 230/1000\n570/570 [==============================] - 11s 19ms/step - loss: 334.2437 - synergy_loss: 68.1200 - class_loss: 0.4418\nEpoch 231/1000\n570/570 [==============================] - 10s 18ms/step - loss: 334.0308 - synergy_loss: 68.0541 - class_loss: 0.4413\nEpoch 232/1000\n570/570 [==============================] - 10s 18ms/step - loss: 333.8535 - synergy_loss: 68.0422 - class_loss: 0.4394\nEpoch 233/1000\n570/570 [==============================] - 10s 18ms/step - loss: 331.3462 - synergy_loss: 65.7211 - class_loss: 0.4371\nEpoch 234/1000\n570/570 [==============================] - 10s 18ms/step - loss: 333.0652 - synergy_loss: 67.5961 - class_loss: 0.4361\nEpoch 235/1000\n570/570 [==============================] - 10s 18ms/step - loss: 331.2326 - synergy_loss: 65.9575 - class_loss: 0.4364\nEpoch 236/1000\n570/570 [==============================] - 10s 18ms/step - loss: 330.4641 - synergy_loss: 65.3439 - class_loss: 0.4334\nEpoch 237/1000\n570/570 [==============================] - 11s 19ms/step - loss: 330.2880 - synergy_loss: 65.3414 - class_loss: 0.4325\nEpoch 238/1000\n570/570 [==============================] - 10s 18ms/step - loss: 330.6082 - synergy_loss: 65.8267 - class_loss: 0.4344\nEpoch 239/1000\n570/570 [==============================] - 10s 18ms/step - loss: 328.5121 - synergy_loss: 63.8945 - class_loss: 0.4326\nEpoch 240/1000\n570/570 [==============================] - 11s 19ms/step - loss: 329.0589 - synergy_loss: 64.6300 - class_loss: 0.4323\nEpoch 241/1000\n570/570 [==============================] - 10s 18ms/step - loss: 328.5237 - synergy_loss: 64.2735 - class_loss: 0.4304\nEpoch 242/1000\n570/570 [==============================] - 10s 18ms/step - loss: 327.4099 - synergy_loss: 63.3467 - class_loss: 0.4283\nEpoch 243/1000\n570/570 [==============================] - 11s 19ms/step - loss: 328.2751 - synergy_loss: 64.3894 - class_loss: 0.4316\nEpoch 244/1000\n570/570 [==============================] - 10s 18ms/step - loss: 326.6552 - synergy_loss: 62.9492 - class_loss: 0.4270\nEpoch 245/1000\n570/570 [==============================] - 10s 18ms/step - loss: 326.5830 - synergy_loss: 63.0642 - class_loss: 0.4287\nEpoch 246/1000\n570/570 [==============================] - 11s 19ms/step - loss: 325.4407 - synergy_loss: 62.0907 - class_loss: 0.4264\nEpoch 247/1000\n570/570 [==============================] - 10s 18ms/step - loss: 324.8560 - synergy_loss: 61.6952 - class_loss: 0.4248\nEpoch 248/1000\n570/570 [==============================] - 11s 19ms/step - loss: 326.0103 - synergy_loss: 63.0355 - class_loss: 0.4257\nEpoch 249/1000\n570/570 [==============================] - 11s 19ms/step - loss: 323.5343 - synergy_loss: 60.7153 - class_loss: 0.4244\nEpoch 250/1000\n570/570 [==============================] - 10s 18ms/step - loss: 322.7129 - synergy_loss: 60.0977 - class_loss: 0.4207\nEpoch 251/1000\n570/570 [==============================] - 10s 18ms/step - loss: 323.5593 - synergy_loss: 61.1262 - class_loss: 0.4225\nEpoch 252/1000\n570/570 [==============================] - 11s 19ms/step - loss: 322.8375 - synergy_loss: 60.5617 - class_loss: 0.4233\nEpoch 253/1000\n570/570 [==============================] - 10s 18ms/step - loss: 321.5604 - synergy_loss: 59.4831 - class_loss: 0.4192\nEpoch 254/1000\n570/570 [==============================] - 10s 18ms/step - loss: 322.0767 - synergy_loss: 60.1916 - class_loss: 0.4208\nEpoch 255/1000\n570/570 [==============================] - 11s 19ms/step - loss: 321.5592 - synergy_loss: 59.8575 - class_loss: 0.4194\nEpoch 256/1000\n570/570 [==============================] - 10s 18ms/step - loss: 320.7860 - synergy_loss: 59.2680 - class_loss: 0.4192\nEpoch 257/1000\n570/570 [==============================] - 10s 18ms/step - loss: 319.6958 - synergy_loss: 58.3820 - class_loss: 0.4142\nEpoch 258/1000\n570/570 [==============================] - 11s 19ms/step - loss: 319.2968 - synergy_loss: 58.1721 - class_loss: 0.4150\nEpoch 259/1000\n570/570 [==============================] - 10s 18ms/step - loss: 319.0730 - synergy_loss: 58.1140 - class_loss: 0.4130\nEpoch 260/1000\n570/570 [==============================] - 11s 18ms/step - loss: 317.7436 - synergy_loss: 56.9943 - class_loss: 0.4136\nEpoch 261/1000\n570/570 [==============================] - 11s 19ms/step - loss: 318.3984 - synergy_loss: 57.8417 - class_loss: 0.4142\nEpoch 262/1000\n570/570 [==============================] - 10s 18ms/step - loss: 318.4549 - synergy_loss: 58.0756 - class_loss: 0.4125\nEpoch 263/1000\n570/570 [==============================] - 11s 18ms/step - loss: 316.8428 - synergy_loss: 56.6565 - class_loss: 0.4115\nEpoch 264/1000\n570/570 [==============================] - 11s 19ms/step - loss: 316.4507 - synergy_loss: 56.4544 - class_loss: 0.4090\nEpoch 265/1000\n570/570 [==============================] - 10s 18ms/step - loss: 316.3367 - synergy_loss: 56.5291 - class_loss: 0.4102\nEpoch 266/1000\n570/570 [==============================] - 10s 18ms/step - loss: 314.4238 - synergy_loss: 54.8005 - class_loss: 0.4072\nEpoch 267/1000\n570/570 [==============================] - 11s 19ms/step - loss: 314.6135 - synergy_loss: 55.1911 - class_loss: 0.4047\nEpoch 268/1000\n570/570 [==============================] - 10s 18ms/step - loss: 313.5151 - synergy_loss: 54.2735 - class_loss: 0.4064\nEpoch 269/1000\n570/570 [==============================] - 11s 18ms/step - loss: 314.1709 - synergy_loss: 55.1295 - class_loss: 0.4054\nEpoch 270/1000\n570/570 [==============================] - 11s 19ms/step - loss: 314.7510 - synergy_loss: 55.8989 - class_loss: 0.4064\nEpoch 271/1000\n570/570 [==============================] - 11s 19ms/step - loss: 314.6162 - synergy_loss: 55.9479 - class_loss: 0.4069\nEpoch 272/1000\n570/570 [==============================] - 10s 18ms/step - loss: 312.5024 - synergy_loss: 54.0135 - class_loss: 0.4059\nEpoch 273/1000\n570/570 [==============================] - 11s 19ms/step - loss: 312.0475 - synergy_loss: 53.7537 - class_loss: 0.3996\nEpoch 274/1000\n570/570 [==============================] - 10s 18ms/step - loss: 311.6312 - synergy_loss: 53.5278 - class_loss: 0.4016\nEpoch 275/1000\n570/570 [==============================] - 10s 18ms/step - loss: 311.6022 - synergy_loss: 53.6892 - class_loss: 0.4047\nEpoch 276/1000\n570/570 [==============================] - 11s 19ms/step - loss: 310.9515 - synergy_loss: 53.2359 - class_loss: 0.4014\nEpoch 277/1000\n570/570 [==============================] - 10s 18ms/step - loss: 310.3286 - synergy_loss: 52.8155 - class_loss: 0.4017\nEpoch 278/1000\n570/570 [==============================] - 10s 18ms/step - loss: 308.8872 - synergy_loss: 51.5674 - class_loss: 0.3990\nEpoch 279/1000\n570/570 [==============================] - 11s 19ms/step - loss: 309.6294 - synergy_loss: 52.5089 - class_loss: 0.3997\nEpoch 280/1000\n570/570 [==============================] - 10s 18ms/step - loss: 308.6108 - synergy_loss: 51.6943 - class_loss: 0.3956\nEpoch 281/1000\n570/570 [==============================] - 10s 18ms/step - loss: 308.9297 - synergy_loss: 52.2091 - class_loss: 0.3947\nEpoch 282/1000\n570/570 [==============================] - 11s 19ms/step - loss: 308.5562 - synergy_loss: 52.0252 - class_loss: 0.3933\nEpoch 283/1000\n570/570 [==============================] - 10s 18ms/step - loss: 307.6376 - synergy_loss: 51.3095 - class_loss: 0.3949\nEpoch 284/1000\n570/570 [==============================] - 11s 19ms/step - loss: 306.8894 - synergy_loss: 50.7435 - class_loss: 0.3943\nEpoch 285/1000\n570/570 [==============================] - 11s 19ms/step - loss: 307.1335 - synergy_loss: 51.1894 - class_loss: 0.3916\nEpoch 286/1000\n570/570 [==============================] - 11s 18ms/step - loss: 307.4927 - synergy_loss: 51.7415 - class_loss: 0.3906\nEpoch 287/1000\n570/570 [==============================] - 10s 18ms/step - loss: 306.1167 - synergy_loss: 50.5802 - class_loss: 0.3902\nEpoch 288/1000\n570/570 [==============================] - 11s 19ms/step - loss: 305.5996 - synergy_loss: 50.2405 - class_loss: 0.3896\nEpoch 289/1000\n570/570 [==============================] - 10s 18ms/step - loss: 304.4644 - synergy_loss: 49.3033 - class_loss: 0.3911\nEpoch 290/1000\n570/570 [==============================] - 10s 18ms/step - loss: 304.3401 - synergy_loss: 49.3619 - class_loss: 0.3871\nEpoch 291/1000\n570/570 [==============================] - 11s 19ms/step - loss: 304.2250 - synergy_loss: 49.4519 - class_loss: 0.3878\nEpoch 292/1000\n570/570 [==============================] - 10s 18ms/step - loss: 303.4205 - synergy_loss: 48.8658 - class_loss: 0.3854\nEpoch 293/1000\n570/570 [==============================] - 10s 18ms/step - loss: 303.5406 - synergy_loss: 49.1899 - class_loss: 0.3849\nEpoch 294/1000\n570/570 [==============================] - 11s 19ms/step - loss: 302.7891 - synergy_loss: 48.6308 - class_loss: 0.3852\nEpoch 295/1000\n570/570 [==============================] - 10s 18ms/step - loss: 301.4613 - synergy_loss: 47.5042 - class_loss: 0.3838\nEpoch 296/1000\n570/570 [==============================] - 10s 18ms/step - loss: 302.4849 - synergy_loss: 48.7173 - class_loss: 0.3816\nEpoch 297/1000\n570/570 [==============================] - 11s 18ms/step - loss: 301.6102 - synergy_loss: 48.0545 - class_loss: 0.3829\nEpoch 298/1000\n570/570 [==============================] - 10s 18ms/step - loss: 302.0784 - synergy_loss: 48.7122 - class_loss: 0.3840\nEpoch 299/1000\n570/570 [==============================] - 10s 18ms/step - loss: 301.0424 - synergy_loss: 47.8817 - class_loss: 0.3820\nEpoch 300/1000\n570/570 [==============================] - 11s 19ms/step - loss: 300.5740 - synergy_loss: 47.6081 - class_loss: 0.3805\nEpoch 301/1000\n570/570 [==============================] - 10s 18ms/step - loss: 301.0179 - synergy_loss: 48.2498 - class_loss: 0.3778\nEpoch 302/1000\n570/570 [==============================] - 10s 18ms/step - loss: 298.9694 - synergy_loss: 46.4042 - class_loss: 0.3767\nEpoch 303/1000\n570/570 [==============================] - 11s 19ms/step - loss: 300.2005 - synergy_loss: 47.8399 - class_loss: 0.3794\nEpoch 304/1000\n570/570 [==============================] - 10s 18ms/step - loss: 299.8437 - synergy_loss: 47.6891 - class_loss: 0.3742\nEpoch 305/1000\n570/570 [==============================] - 10s 18ms/step - loss: 298.8770 - synergy_loss: 46.9002 - class_loss: 0.3747\nEpoch 306/1000\n570/570 [==============================] - 11s 19ms/step - loss: 298.2572 - synergy_loss: 46.4962 - class_loss: 0.3742\nEpoch 307/1000\n570/570 [==============================] - 10s 18ms/step - loss: 297.6664 - synergy_loss: 46.1089 - class_loss: 0.3713\nEpoch 308/1000\n570/570 [==============================] - 11s 19ms/step - loss: 297.8948 - synergy_loss: 46.5279 - class_loss: 0.3705\nEpoch 309/1000\n570/570 [==============================] - 11s 19ms/step - loss: 297.2960 - synergy_loss: 46.1095 - class_loss: 0.3698\nEpoch 310/1000\n570/570 [==============================] - 10s 18ms/step - loss: 296.0968 - synergy_loss: 45.1065 - class_loss: 0.3726\nEpoch 311/1000\n570/570 [==============================] - 10s 18ms/step - loss: 295.8395 - synergy_loss: 45.0454 - class_loss: 0.3732\nEpoch 312/1000\n570/570 [==============================] - 11s 19ms/step - loss: 296.2623 - synergy_loss: 45.6844 - class_loss: 0.3674\nEpoch 313/1000\n570/570 [==============================] - 10s 18ms/step - loss: 296.3019 - synergy_loss: 45.9217 - class_loss: 0.3704\nEpoch 314/1000\n570/570 [==============================] - 10s 18ms/step - loss: 294.8065 - synergy_loss: 44.6244 - class_loss: 0.3696\nEpoch 315/1000\n570/570 [==============================] - 11s 19ms/step - loss: 295.2204 - synergy_loss: 45.2675 - class_loss: 0.3657\nEpoch 316/1000\n570/570 [==============================] - 10s 18ms/step - loss: 294.2182 - synergy_loss: 44.4686 - class_loss: 0.3652\nEpoch 317/1000\n570/570 [==============================] - 11s 19ms/step - loss: 292.9961 - synergy_loss: 43.4484 - class_loss: 0.3627\nEpoch 318/1000\n570/570 [==============================] - 11s 19ms/step - loss: 293.3085 - synergy_loss: 43.9692 - class_loss: 0.3644\nEpoch 319/1000\n570/570 [==============================] - 10s 18ms/step - loss: 292.4745 - synergy_loss: 43.3488 - class_loss: 0.3637\nEpoch 320/1000\n570/570 [==============================] - 10s 18ms/step - loss: 293.2843 - synergy_loss: 44.3696 - class_loss: 0.3618\nEpoch 321/1000\n570/570 [==============================] - 11s 19ms/step - loss: 293.0844 - synergy_loss: 44.3483 - class_loss: 0.3631\nEpoch 322/1000\n570/570 [==============================] - 10s 18ms/step - loss: 292.1240 - synergy_loss: 43.5771 - class_loss: 0.3609\nEpoch 323/1000\n570/570 [==============================] - 10s 18ms/step - loss: 292.3839 - synergy_loss: 44.0384 - class_loss: 0.3591\nEpoch 324/1000\n570/570 [==============================] - 11s 19ms/step - loss: 290.9262 - synergy_loss: 42.7655 - class_loss: 0.3595\nEpoch 325/1000\n570/570 [==============================] - 10s 18ms/step - loss: 290.3145 - synergy_loss: 42.3801 - class_loss: 0.3579\nEpoch 326/1000\n570/570 [==============================] - 10s 18ms/step - loss: 290.0011 - synergy_loss: 42.2780 - class_loss: 0.3572\nEpoch 327/1000\n570/570 [==============================] - 11s 19ms/step - loss: 289.5673 - synergy_loss: 42.0521 - class_loss: 0.3546\nEpoch 328/1000\n570/570 [==============================] - 10s 18ms/step - loss: 289.5657 - synergy_loss: 42.2796 - class_loss: 0.3547\nEpoch 329/1000\n570/570 [==============================] - 10s 18ms/step - loss: 289.0441 - synergy_loss: 41.9600 - class_loss: 0.3544\nEpoch 330/1000\n570/570 [==============================] - 11s 19ms/step - loss: 289.7301 - synergy_loss: 42.8745 - class_loss: 0.3530\nEpoch 331/1000\n570/570 [==============================] - 10s 18ms/step - loss: 289.0297 - synergy_loss: 42.3638 - class_loss: 0.3553\nEpoch 332/1000\n570/570 [==============================] - 10s 18ms/step - loss: 288.4017 - synergy_loss: 41.9343 - class_loss: 0.3541\nEpoch 333/1000\n570/570 [==============================] - 11s 19ms/step - loss: 287.7341 - synergy_loss: 41.4615 - class_loss: 0.3520\nEpoch 334/1000\n570/570 [==============================] - 10s 18ms/step - loss: 288.3140 - synergy_loss: 42.2544 - class_loss: 0.3506\nEpoch 335/1000\n570/570 [==============================] - 10s 18ms/step - loss: 286.1078 - synergy_loss: 40.2589 - class_loss: 0.3485\nEpoch 336/1000\n570/570 [==============================] - 11s 19ms/step - loss: 287.0619 - synergy_loss: 41.4470 - class_loss: 0.3502\nEpoch 337/1000\n570/570 [==============================] - 10s 18ms/step - loss: 285.9025 - synergy_loss: 40.4888 - class_loss: 0.3475\nEpoch 338/1000\n570/570 [==============================] - 10s 18ms/step - loss: 286.4512 - synergy_loss: 41.2446 - class_loss: 0.3487\nEpoch 339/1000\n570/570 [==============================] - 11s 19ms/step - loss: 285.0570 - synergy_loss: 40.0567 - class_loss: 0.3436\nEpoch 340/1000\n570/570 [==============================] - 11s 19ms/step - loss: 284.7754 - synergy_loss: 39.9788 - class_loss: 0.3459\nEpoch 341/1000\n570/570 [==============================] - 10s 18ms/step - loss: 284.5780 - synergy_loss: 40.0139 - class_loss: 0.3451\nEpoch 342/1000\n570/570 [==============================] - 11s 19ms/step - loss: 284.6618 - synergy_loss: 40.3019 - class_loss: 0.3471\nEpoch 343/1000\n570/570 [==============================] - 10s 18ms/step - loss: 283.5036 - synergy_loss: 39.3552 - class_loss: 0.3444\nEpoch 344/1000\n570/570 [==============================] - 10s 18ms/step - loss: 284.2060 - synergy_loss: 40.2666 - class_loss: 0.3446\nEpoch 345/1000\n570/570 [==============================] - 11s 19ms/step - loss: 283.8544 - synergy_loss: 40.1368 - class_loss: 0.3437\nEpoch 346/1000\n570/570 [==============================] - 10s 18ms/step - loss: 283.4501 - synergy_loss: 39.9467 - class_loss: 0.3403\nEpoch 347/1000\n570/570 [==============================] - 11s 19ms/step - loss: 283.1911 - synergy_loss: 39.9169 - class_loss: 0.3410\nEpoch 348/1000\n570/570 [==============================] - 11s 19ms/step - loss: 281.8908 - synergy_loss: 38.8305 - class_loss: 0.3368\nEpoch 349/1000\n570/570 [==============================] - 10s 18ms/step - loss: 281.7514 - synergy_loss: 38.8948 - class_loss: 0.3391\nEpoch 350/1000\n570/570 [==============================] - 10s 18ms/step - loss: 281.5862 - synergy_loss: 38.9619 - class_loss: 0.3406\nEpoch 351/1000\n570/570 [==============================] - 11s 19ms/step - loss: 281.0734 - synergy_loss: 38.6533 - class_loss: 0.3392\nEpoch 352/1000\n570/570 [==============================] - 10s 18ms/step - loss: 281.0868 - synergy_loss: 38.8846 - class_loss: 0.3364\nEpoch 353/1000\n570/570 [==============================] - 10s 18ms/step - loss: 280.5944 - synergy_loss: 38.6066 - class_loss: 0.3364\nEpoch 354/1000\n570/570 [==============================] - 11s 19ms/step - loss: 280.0557 - synergy_loss: 38.2836 - class_loss: 0.3307\nEpoch 355/1000\n570/570 [==============================] - 10s 18ms/step - loss: 279.5385 - synergy_loss: 37.9832 - class_loss: 0.3339\nEpoch 356/1000\n570/570 [==============================] - 10s 18ms/step - loss: 279.7907 - synergy_loss: 38.4468 - class_loss: 0.3352\nEpoch 357/1000\n570/570 [==============================] - 11s 19ms/step - loss: 278.8987 - synergy_loss: 37.7648 - class_loss: 0.3290\nEpoch 358/1000\n570/570 [==============================] - 10s 18ms/step - loss: 278.5573 - synergy_loss: 37.6424 - class_loss: 0.3316\nEpoch 359/1000\n570/570 [==============================] - 11s 18ms/step - loss: 277.8781 - synergy_loss: 37.1912 - class_loss: 0.3300\nEpoch 360/1000\n570/570 [==============================] - 11s 19ms/step - loss: 277.3503 - synergy_loss: 36.8953 - class_loss: 0.3299\nEpoch 361/1000\n570/570 [==============================] - 10s 18ms/step - loss: 277.5276 - synergy_loss: 37.2785 - class_loss: 0.3263\nEpoch 362/1000\n570/570 [==============================] - 10s 18ms/step - loss: 277.3671 - synergy_loss: 37.3140 - class_loss: 0.3310\nEpoch 363/1000\n570/570 [==============================] - 11s 19ms/step - loss: 276.2038 - synergy_loss: 36.3912 - class_loss: 0.3272\nEpoch 364/1000\n570/570 [==============================] - 11s 18ms/step - loss: 276.8459 - synergy_loss: 37.2340 - class_loss: 0.3276\nEpoch 365/1000\n570/570 [==============================] - 11s 19ms/step - loss: 276.2424 - synergy_loss: 36.8485 - class_loss: 0.3214\nEpoch 366/1000\n570/570 [==============================] - 11s 19ms/step - loss: 276.2402 - synergy_loss: 37.0661 - class_loss: 0.3260\nEpoch 367/1000\n570/570 [==============================] - 10s 18ms/step - loss: 276.0488 - synergy_loss: 37.0819 - class_loss: 0.3226\nEpoch 368/1000\n570/570 [==============================] - 10s 18ms/step - loss: 275.2702 - synergy_loss: 36.5227 - class_loss: 0.3225\nEpoch 369/1000\n570/570 [==============================] - 11s 19ms/step - loss: 274.4056 - synergy_loss: 35.8664 - class_loss: 0.3250\nEpoch 370/1000\n570/570 [==============================] - 10s 18ms/step - loss: 275.0805 - synergy_loss: 36.7576 - class_loss: 0.3209\nEpoch 371/1000\n570/570 [==============================] - 10s 18ms/step - loss: 274.1924 - synergy_loss: 36.0912 - class_loss: 0.3176\nEpoch 372/1000\n570/570 [==============================] - 11s 19ms/step - loss: 273.3338 - synergy_loss: 35.4518 - class_loss: 0.3191\nEpoch 373/1000\n570/570 [==============================] - 10s 18ms/step - loss: 274.0980 - synergy_loss: 36.4205 - class_loss: 0.3212\nEpoch 374/1000\n570/570 [==============================] - 11s 19ms/step - loss: 273.2035 - synergy_loss: 35.7690 - class_loss: 0.3151\nEpoch 375/1000\n570/570 [==============================] - 11s 19ms/step - loss: 272.4282 - synergy_loss: 35.2100 - class_loss: 0.3186\nEpoch 376/1000\n570/570 [==============================] - 10s 18ms/step - loss: 272.6812 - synergy_loss: 35.6822 - class_loss: 0.3166\nEpoch 377/1000\n570/570 [==============================] - 10s 18ms/step - loss: 271.7659 - synergy_loss: 34.9951 - class_loss: 0.3142\nEpoch 378/1000\n570/570 [==============================] - 11s 19ms/step - loss: 271.6804 - synergy_loss: 35.1458 - class_loss: 0.3117\nEpoch 379/1000\n570/570 [==============================] - 10s 18ms/step - loss: 271.4246 - synergy_loss: 35.1036 - class_loss: 0.3174\nEpoch 380/1000\n570/570 [==============================] - 10s 18ms/step - loss: 271.3350 - synergy_loss: 35.2219 - class_loss: 0.3163\nEpoch 381/1000\n570/570 [==============================] - 11s 19ms/step - loss: 271.1371 - synergy_loss: 35.2461 - class_loss: 0.3111\nEpoch 382/1000\n570/570 [==============================] - 10s 18ms/step - loss: 271.2621 - synergy_loss: 35.6021 - class_loss: 0.3114\nEpoch 383/1000\n570/570 [==============================] - 10s 18ms/step - loss: 270.8938 - synergy_loss: 35.4474 - class_loss: 0.3100\nEpoch 384/1000\n570/570 [==============================] - 11s 19ms/step - loss: 270.9854 - synergy_loss: 35.7512 - class_loss: 0.3106\nEpoch 385/1000\n570/570 [==============================] - 11s 19ms/step - loss: 269.7884 - synergy_loss: 34.7613 - class_loss: 0.3074\nEpoch 386/1000\n570/570 [==============================] - 10s 18ms/step - loss: 268.7180 - synergy_loss: 33.8994 - class_loss: 0.3028\nEpoch 387/1000\n570/570 [==============================] - 11s 19ms/step - loss: 268.7772 - synergy_loss: 34.1972 - class_loss: 0.3026\nEpoch 388/1000\n570/570 [==============================] - 10s 18ms/step - loss: 269.2056 - synergy_loss: 34.8263 - class_loss: 0.3066\nEpoch 389/1000\n570/570 [==============================] - 10s 18ms/step - loss: 268.4340 - synergy_loss: 34.2739 - class_loss: 0.3074\nEpoch 390/1000\n570/570 [==============================] - 11s 19ms/step - loss: 267.9853 - synergy_loss: 34.0578 - class_loss: 0.3061\nEpoch 391/1000\n570/570 [==============================] - 11s 19ms/step - loss: 268.0349 - synergy_loss: 34.3358 - class_loss: 0.3007\nEpoch 392/1000\n570/570 [==============================] - 10s 18ms/step - loss: 267.2499 - synergy_loss: 33.7535 - class_loss: 0.3058\nEpoch 393/1000\n570/570 [==============================] - 11s 19ms/step - loss: 266.8487 - synergy_loss: 33.5766 - class_loss: 0.3019\nEpoch 394/1000\n570/570 [==============================] - 10s 18ms/step - loss: 265.7755 - synergy_loss: 32.7203 - class_loss: 0.3028\nEpoch 395/1000\n570/570 [==============================] - 10s 18ms/step - loss: 267.0250 - synergy_loss: 34.2140 - class_loss: 0.2999\nEpoch 396/1000\n570/570 [==============================] - 11s 19ms/step - loss: 267.2044 - synergy_loss: 34.5733 - class_loss: 0.3019\nEpoch 397/1000\n570/570 [==============================] - 10s 18ms/step - loss: 265.7408 - synergy_loss: 33.3191 - class_loss: 0.3002\nEpoch 398/1000\n570/570 [==============================] - 10s 18ms/step - loss: 264.9764 - synergy_loss: 32.7778 - class_loss: 0.2970\nEpoch 399/1000\n570/570 [==============================] - 11s 19ms/step - loss: 265.3409 - synergy_loss: 33.3661 - class_loss: 0.2989\nEpoch 400/1000\n570/570 [==============================] - 10s 18ms/step - loss: 264.7024 - synergy_loss: 32.9287 - class_loss: 0.3002\nEpoch 401/1000\n570/570 [==============================] - 10s 18ms/step - loss: 264.5244 - synergy_loss: 32.9885 - class_loss: 0.2954\nEpoch 402/1000\n570/570 [==============================] - 11s 19ms/step - loss: 264.3578 - synergy_loss: 33.0338 - class_loss: 0.2966\nEpoch 403/1000\n570/570 [==============================] - 11s 19ms/step - loss: 264.2398 - synergy_loss: 33.1416 - class_loss: 0.2973\nEpoch 404/1000\n570/570 [==============================] - 11s 19ms/step - loss: 263.8273 - synergy_loss: 32.9553 - class_loss: 0.2903\nEpoch 405/1000\n570/570 [==============================] - 12s 20ms/step - loss: 263.4734 - synergy_loss: 32.8348 - class_loss: 0.2910\nEpoch 406/1000\n570/570 [==============================] - 11s 20ms/step - loss: 263.3570 - synergy_loss: 32.9376 - class_loss: 0.2936\nEpoch 407/1000\n570/570 [==============================] - 11s 20ms/step - loss: 261.9743 - synergy_loss: 31.7743 - class_loss: 0.2922\nEpoch 408/1000\n570/570 [==============================] - 11s 20ms/step - loss: 261.9977 - synergy_loss: 32.0174 - class_loss: 0.2922\nEpoch 409/1000\n570/570 [==============================] - 11s 19ms/step - loss: 262.4016 - synergy_loss: 32.6583 - class_loss: 0.2931\nEpoch 410/1000\n570/570 [==============================] - 11s 19ms/step - loss: 262.6879 - synergy_loss: 33.1730 - class_loss: 0.2878\nEpoch 411/1000\n570/570 [==============================] - 11s 20ms/step - loss: 261.3324 - synergy_loss: 32.0425 - class_loss: 0.2870\nEpoch 412/1000\n570/570 [==============================] - 11s 20ms/step - loss: 262.0722 - synergy_loss: 32.9986 - class_loss: 0.2860\nEpoch 413/1000\n570/570 [==============================] - 11s 19ms/step - loss: 260.3741 - synergy_loss: 31.5214 - class_loss: 0.2851\nEpoch 414/1000\n570/570 [==============================] - 11s 19ms/step - loss: 260.1849 - synergy_loss: 31.5568 - class_loss: 0.2855\nEpoch 415/1000\n570/570 [==============================] - 10s 18ms/step - loss: 260.1443 - synergy_loss: 31.7425 - class_loss: 0.2858\nEpoch 416/1000\n570/570 [==============================] - 10s 18ms/step - loss: 259.1024 - synergy_loss: 30.9301 - class_loss: 0.2836\nEpoch 417/1000\n570/570 [==============================] - 11s 18ms/step - loss: 260.2732 - synergy_loss: 32.3395 - class_loss: 0.2815\nEpoch 418/1000\n570/570 [==============================] - 10s 18ms/step - loss: 259.2079 - synergy_loss: 31.4943 - class_loss: 0.2849\nEpoch 419/1000\n570/570 [==============================] - 10s 18ms/step - loss: 259.1475 - synergy_loss: 31.6567 - class_loss: 0.2853\nEpoch 420/1000\n570/570 [==============================] - 11s 19ms/step - loss: 258.6603 - synergy_loss: 31.3903 - class_loss: 0.2808\nEpoch 421/1000\n570/570 [==============================] - 10s 18ms/step - loss: 258.6936 - synergy_loss: 31.6453 - class_loss: 0.2795\nEpoch 422/1000\n570/570 [==============================] - 10s 18ms/step - loss: 257.5437 - synergy_loss: 30.7309 - class_loss: 0.2792\nEpoch 423/1000\n570/570 [==============================] - 11s 19ms/step - loss: 258.1201 - synergy_loss: 31.5247 - class_loss: 0.2816\nEpoch 424/1000\n570/570 [==============================] - 10s 18ms/step - loss: 256.9713 - synergy_loss: 30.6057 - class_loss: 0.2782\nEpoch 425/1000\n570/570 [==============================] - 10s 18ms/step - loss: 257.4410 - synergy_loss: 31.3126 - class_loss: 0.2765\nEpoch 426/1000\n570/570 [==============================] - 11s 19ms/step - loss: 257.1835 - synergy_loss: 31.2724 - class_loss: 0.2799\nEpoch 427/1000\n570/570 [==============================] - 10s 18ms/step - loss: 256.4564 - synergy_loss: 30.7636 - class_loss: 0.2748\nEpoch 428/1000\n570/570 [==============================] - 10s 18ms/step - loss: 256.0380 - synergy_loss: 30.5753 - class_loss: 0.2748\nEpoch 429/1000\n570/570 [==============================] - 11s 19ms/step - loss: 255.6748 - synergy_loss: 30.4399 - class_loss: 0.2730\nEpoch 430/1000\n570/570 [==============================] - 10s 18ms/step - loss: 255.4092 - synergy_loss: 30.4223 - class_loss: 0.2754\nEpoch 431/1000\n570/570 [==============================] - 11s 18ms/step - loss: 255.2948 - synergy_loss: 30.5315 - class_loss: 0.2745\nEpoch 432/1000\n570/570 [==============================] - 11s 19ms/step - loss: 254.9953 - synergy_loss: 30.4638 - class_loss: 0.2717\nEpoch 433/1000\n570/570 [==============================] - 10s 18ms/step - loss: 253.9228 - synergy_loss: 29.6136 - class_loss: 0.2720\nEpoch 434/1000\n570/570 [==============================] - 10s 18ms/step - loss: 254.2870 - synergy_loss: 30.2179 - class_loss: 0.2726\nEpoch 435/1000\n570/570 [==============================] - 11s 19ms/step - loss: 253.7749 - synergy_loss: 29.9351 - class_loss: 0.2687\nEpoch 436/1000\n570/570 [==============================] - 10s 18ms/step - loss: 254.0548 - synergy_loss: 30.4416 - class_loss: 0.2686\nEpoch 437/1000\n570/570 [==============================] - 10s 18ms/step - loss: 253.0548 - synergy_loss: 29.6633 - class_loss: 0.2694\nEpoch 438/1000\n570/570 [==============================] - 11s 19ms/step - loss: 252.8546 - synergy_loss: 29.6847 - class_loss: 0.2730\nEpoch 439/1000\n570/570 [==============================] - 10s 18ms/step - loss: 252.2454 - synergy_loss: 29.3235 - class_loss: 0.2722\nEpoch 440/1000\n570/570 [==============================] - 10s 18ms/step - loss: 252.4043 - synergy_loss: 29.7263 - class_loss: 0.2626\nEpoch 441/1000\n570/570 [==============================] - 11s 18ms/step - loss: 252.2853 - synergy_loss: 29.8369 - class_loss: 0.2679\nEpoch 442/1000\n570/570 [==============================] - 10s 18ms/step - loss: 251.3058 - synergy_loss: 29.0788 - class_loss: 0.2698\nEpoch 443/1000\n570/570 [==============================] - 10s 18ms/step - loss: 251.3352 - synergy_loss: 29.3636 - class_loss: 0.2640\nEpoch 444/1000\n570/570 [==============================] - 11s 18ms/step - loss: 252.0703 - synergy_loss: 30.3179 - class_loss: 0.2636\nEpoch 445/1000\n570/570 [==============================] - 10s 18ms/step - loss: 250.7179 - synergy_loss: 29.2224 - class_loss: 0.2593\nEpoch 446/1000\n570/570 [==============================] - 10s 18ms/step - loss: 250.1444 - synergy_loss: 28.8673 - class_loss: 0.2642\nEpoch 447/1000\n570/570 [==============================] - 11s 19ms/step - loss: 250.4593 - synergy_loss: 29.4181 - class_loss: 0.2609\nEpoch 448/1000\n570/570 [==============================] - 10s 18ms/step - loss: 250.3056 - synergy_loss: 29.4891 - class_loss: 0.2614\nEpoch 449/1000\n570/570 [==============================] - 10s 18ms/step - loss: 250.0255 - synergy_loss: 29.4505 - class_loss: 0.2585\nEpoch 450/1000\n570/570 [==============================] - 11s 19ms/step - loss: 249.8335 - synergy_loss: 29.4665 - class_loss: 0.2627\nEpoch 451/1000\n570/570 [==============================] - 10s 18ms/step - loss: 249.7382 - synergy_loss: 29.5697 - class_loss: 0.2618\nEpoch 452/1000\n570/570 [==============================] - 11s 19ms/step - loss: 248.0625 - synergy_loss: 28.1173 - class_loss: 0.2593\nEpoch 453/1000\n570/570 [==============================] - 11s 19ms/step - loss: 248.4130 - synergy_loss: 28.6801 - class_loss: 0.2577\nEpoch 454/1000\n570/570 [==============================] - 11s 18ms/step - loss: 248.5154 - synergy_loss: 29.0309 - class_loss: 0.2516\nEpoch 455/1000\n570/570 [==============================] - 10s 18ms/step - loss: 247.8818 - synergy_loss: 28.6232 - class_loss: 0.2601\nEpoch 456/1000\n570/570 [==============================] - 11s 19ms/step - loss: 248.1145 - synergy_loss: 29.0634 - class_loss: 0.2568\nEpoch 457/1000\n570/570 [==============================] - 11s 19ms/step - loss: 248.4733 - synergy_loss: 29.6005 - class_loss: 0.2550\nEpoch 458/1000\n570/570 [==============================] - 10s 18ms/step - loss: 247.3415 - synergy_loss: 28.6802 - class_loss: 0.2529\nEpoch 459/1000\n570/570 [==============================] - 11s 20ms/step - loss: 247.2397 - synergy_loss: 28.7812 - class_loss: 0.2548\nEpoch 460/1000\n570/570 [==============================] - 10s 18ms/step - loss: 246.9633 - synergy_loss: 28.7163 - class_loss: 0.2542\nEpoch 461/1000\n570/570 [==============================] - 10s 18ms/step - loss: 246.4410 - synergy_loss: 28.4082 - class_loss: 0.2515\nEpoch 462/1000\n570/570 [==============================] - 11s 19ms/step - loss: 245.6811 - synergy_loss: 27.8735 - class_loss: 0.2531\nEpoch 463/1000\n570/570 [==============================] - 10s 18ms/step - loss: 245.8831 - synergy_loss: 28.3036 - class_loss: 0.2523\nEpoch 464/1000\n570/570 [==============================] - 10s 18ms/step - loss: 245.8647 - synergy_loss: 28.5226 - class_loss: 0.2510\nEpoch 465/1000\n570/570 [==============================] - 11s 20ms/step - loss: 245.4709 - synergy_loss: 28.3375 - class_loss: 0.2496\nEpoch 466/1000\n570/570 [==============================] - 10s 18ms/step - loss: 244.6914 - synergy_loss: 27.8038 - class_loss: 0.2470\nEpoch 467/1000\n570/570 [==============================] - 10s 18ms/step - loss: 244.1051 - synergy_loss: 27.4341 - class_loss: 0.2480\nEpoch 468/1000\n570/570 [==============================] - 11s 19ms/step - loss: 244.0908 - synergy_loss: 27.6622 - class_loss: 0.2471\nEpoch 469/1000\n570/570 [==============================] - 10s 18ms/step - loss: 244.9304 - synergy_loss: 28.7438 - class_loss: 0.2460\nEpoch 470/1000\n570/570 [==============================] - 10s 18ms/step - loss: 244.7517 - synergy_loss: 28.7850 - class_loss: 0.2488\nEpoch 471/1000\n570/570 [==============================] - 11s 19ms/step - loss: 243.5577 - synergy_loss: 27.8095 - class_loss: 0.2460\nEpoch 472/1000\n570/570 [==============================] - 11s 19ms/step - loss: 242.7979 - synergy_loss: 27.2681 - class_loss: 0.2482\nEpoch 473/1000\n570/570 [==============================] - 10s 18ms/step - loss: 242.7805 - synergy_loss: 27.4715 - class_loss: 0.2405\nEpoch 474/1000\n570/570 [==============================] - 11s 19ms/step - loss: 242.3376 - synergy_loss: 27.2572 - class_loss: 0.2470\nEpoch 475/1000\n570/570 [==============================] - 10s 18ms/step - loss: 242.3296 - synergy_loss: 27.4811 - class_loss: 0.2413\nEpoch 476/1000\n570/570 [==============================] - 10s 18ms/step - loss: 242.1493 - synergy_loss: 27.5401 - class_loss: 0.2412\nEpoch 477/1000\n570/570 [==============================] - 11s 19ms/step - loss: 241.4673 - synergy_loss: 27.0889 - class_loss: 0.2377\nEpoch 478/1000\n570/570 [==============================] - 10s 18ms/step - loss: 241.5445 - synergy_loss: 27.4103 - class_loss: 0.2384\nEpoch 479/1000\n570/570 [==============================] - 10s 18ms/step - loss: 241.4518 - synergy_loss: 27.5413 - class_loss: 0.2402\nEpoch 480/1000\n570/570 [==============================] - 11s 19ms/step - loss: 241.5891 - synergy_loss: 27.8838 - class_loss: 0.2386\nEpoch 481/1000\n570/570 [==============================] - 10s 18ms/step - loss: 240.6363 - synergy_loss: 27.1730 - class_loss: 0.2374\nEpoch 482/1000\n570/570 [==============================] - 10s 18ms/step - loss: 239.7691 - synergy_loss: 26.5433 - class_loss: 0.2359\nEpoch 483/1000\n570/570 [==============================] - 11s 19ms/step - loss: 240.1484 - synergy_loss: 27.1513 - class_loss: 0.2348\nEpoch 484/1000\n570/570 [==============================] - 10s 18ms/step - loss: 240.0152 - synergy_loss: 27.2451 - class_loss: 0.2375\nEpoch 485/1000\n570/570 [==============================] - 10s 18ms/step - loss: 238.7820 - synergy_loss: 26.2580 - class_loss: 0.2325\nEpoch 486/1000\n570/570 [==============================] - 11s 19ms/step - loss: 239.4358 - synergy_loss: 27.1516 - class_loss: 0.2367\nEpoch 487/1000\n570/570 [==============================] - 10s 18ms/step - loss: 238.4551 - synergy_loss: 26.4257 - class_loss: 0.2297\nEpoch 488/1000\n570/570 [==============================] - 10s 18ms/step - loss: 238.0207 - synergy_loss: 26.2367 - class_loss: 0.2308\nEpoch 489/1000\n570/570 [==============================] - 11s 19ms/step - loss: 238.1470 - synergy_loss: 26.5904 - class_loss: 0.2319\nEpoch 490/1000\n570/570 [==============================] - 10s 18ms/step - loss: 238.1588 - synergy_loss: 26.8487 - class_loss: 0.2305\nEpoch 491/1000\n570/570 [==============================] - 10s 18ms/step - loss: 237.5230 - synergy_loss: 26.4479 - class_loss: 0.2310\nEpoch 492/1000\n570/570 [==============================] - 11s 19ms/step - loss: 237.5393 - synergy_loss: 26.7047 - class_loss: 0.2286\nEpoch 493/1000\n570/570 [==============================] - 10s 18ms/step - loss: 236.8800 - synergy_loss: 26.2775 - class_loss: 0.2333\nEpoch 494/1000\n570/570 [==============================] - 11s 19ms/step - loss: 236.2372 - synergy_loss: 25.8763 - class_loss: 0.2263\nEpoch 495/1000\n570/570 [==============================] - 12s 20ms/step - loss: 236.4495 - synergy_loss: 26.3279 - class_loss: 0.2317\nEpoch 496/1000\n570/570 [==============================] - 11s 19ms/step - loss: 236.6345 - synergy_loss: 26.7463 - class_loss: 0.2304\nEpoch 497/1000\n570/570 [==============================] - 10s 18ms/step - loss: 235.9546 - synergy_loss: 26.2946 - class_loss: 0.2291\nEpoch 498/1000\n570/570 [==============================] - 11s 19ms/step - loss: 235.3418 - synergy_loss: 25.9140 - class_loss: 0.2257\nEpoch 499/1000\n570/570 [==============================] - 11s 18ms/step - loss: 234.8165 - synergy_loss: 25.6351 - class_loss: 0.2255\nEpoch 500/1000\n570/570 [==============================] - 10s 18ms/step - loss: 234.8901 - synergy_loss: 25.9457 - class_loss: 0.2261\nEpoch 501/1000\n570/570 [==============================] - 11s 19ms/step - loss: 235.0753 - synergy_loss: 26.3836 - class_loss: 0.2231\nEpoch 502/1000\n570/570 [==============================] - 10s 18ms/step - loss: 234.7603 - synergy_loss: 26.2866 - class_loss: 0.2231\nEpoch 503/1000\n570/570 [==============================] - 10s 18ms/step - loss: 234.3026 - synergy_loss: 26.0639 - class_loss: 0.2262\nEpoch 504/1000\n570/570 [==============================] - 11s 19ms/step - loss: 233.5754 - synergy_loss: 25.5748 - class_loss: 0.2229\nEpoch 505/1000\n570/570 [==============================] - 10s 18ms/step - loss: 233.5944 - synergy_loss: 25.8255 - class_loss: 0.2225\nEpoch 506/1000\n570/570 [==============================] - 10s 18ms/step - loss: 233.3951 - synergy_loss: 25.8683 - class_loss: 0.2200\nEpoch 507/1000\n570/570 [==============================] - 11s 19ms/step - loss: 232.7382 - synergy_loss: 25.4393 - class_loss: 0.2230\nEpoch 508/1000\n570/570 [==============================] - 10s 18ms/step - loss: 232.5970 - synergy_loss: 25.5320 - class_loss: 0.2207\nEpoch 509/1000\n570/570 [==============================] - 10s 18ms/step - loss: 232.5290 - synergy_loss: 25.6857 - class_loss: 0.2215\nEpoch 510/1000\n570/570 [==============================] - 11s 19ms/step - loss: 231.8835 - synergy_loss: 25.2900 - class_loss: 0.2187\nEpoch 511/1000\n570/570 [==============================] - 10s 18ms/step - loss: 232.5385 - synergy_loss: 26.1842 - class_loss: 0.2165\nEpoch 512/1000\n570/570 [==============================] - 10s 18ms/step - loss: 231.6902 - synergy_loss: 25.5408 - class_loss: 0.2202\nEpoch 513/1000\n570/570 [==============================] - 11s 18ms/step - loss: 231.5699 - synergy_loss: 25.6620 - class_loss: 0.2209\nEpoch 514/1000\n570/570 [==============================] - 10s 18ms/step - loss: 231.1808 - synergy_loss: 25.5175 - class_loss: 0.2188\nEpoch 515/1000\n570/570 [==============================] - 10s 18ms/step - loss: 230.9773 - synergy_loss: 25.5486 - class_loss: 0.2189\nEpoch 516/1000\n570/570 [==============================] - 10s 18ms/step - loss: 230.8493 - synergy_loss: 25.6543 - class_loss: 0.2158\nEpoch 517/1000\n570/570 [==============================] - 11s 18ms/step - loss: 230.0219 - synergy_loss: 25.0559 - class_loss: 0.2150\nEpoch 518/1000\n570/570 [==============================] - 10s 18ms/step - loss: 229.9178 - synergy_loss: 25.1944 - class_loss: 0.2156\nEpoch 519/1000\n570/570 [==============================] - 10s 18ms/step - loss: 229.5175 - synergy_loss: 25.0306 - class_loss: 0.2136\nEpoch 520/1000\n570/570 [==============================] - 11s 19ms/step - loss: 230.2038 - synergy_loss: 25.9511 - class_loss: 0.2109\nEpoch 521/1000\n570/570 [==============================] - 10s 18ms/step - loss: 229.0816 - synergy_loss: 25.0642 - class_loss: 0.2134\nEpoch 522/1000\n570/570 [==============================] - 10s 18ms/step - loss: 228.4377 - synergy_loss: 24.6573 - class_loss: 0.2101\nEpoch 523/1000\n570/570 [==============================] - 11s 19ms/step - loss: 227.9909 - synergy_loss: 24.4499 - class_loss: 0.2125\nEpoch 524/1000\n570/570 [==============================] - 10s 18ms/step - loss: 228.0686 - synergy_loss: 24.7754 - class_loss: 0.2086\nEpoch 525/1000\n570/570 [==============================] - 10s 18ms/step - loss: 227.4877 - synergy_loss: 24.4389 - class_loss: 0.2075\nEpoch 526/1000\n570/570 [==============================] - 11s 19ms/step - loss: 227.3669 - synergy_loss: 24.5699 - class_loss: 0.2050\nEpoch 527/1000\n570/570 [==============================] - 10s 18ms/step - loss: 227.4695 - synergy_loss: 24.9126 - class_loss: 0.2079\nEpoch 528/1000\n570/570 [==============================] - 10s 18ms/step - loss: 227.4895 - synergy_loss: 25.1599 - class_loss: 0.2100\nEpoch 529/1000\n570/570 [==============================] - 11s 19ms/step - loss: 226.9778 - synergy_loss: 24.8916 - class_loss: 0.2065\nEpoch 530/1000\n570/570 [==============================] - 10s 18ms/step - loss: 226.5959 - synergy_loss: 24.7353 - class_loss: 0.2054\nEpoch 531/1000\n570/570 [==============================] - 10s 18ms/step - loss: 226.4713 - synergy_loss: 24.8480 - class_loss: 0.2101\nEpoch 532/1000\n570/570 [==============================] - 11s 19ms/step - loss: 225.9474 - synergy_loss: 24.5614 - class_loss: 0.2049\nEpoch 533/1000\n570/570 [==============================] - 10s 18ms/step - loss: 225.5686 - synergy_loss: 24.4229 - class_loss: 0.2055\nEpoch 534/1000\n570/570 [==============================] - 10s 18ms/step - loss: 225.1235 - synergy_loss: 24.2079 - class_loss: 0.2056\nEpoch 535/1000\n570/570 [==============================] - 11s 19ms/step - loss: 224.7768 - synergy_loss: 24.1110 - class_loss: 0.2039\nEpoch 536/1000\n570/570 [==============================] - 10s 18ms/step - loss: 224.3968 - synergy_loss: 23.9697 - class_loss: 0.2018\nEpoch 537/1000\n570/570 [==============================] - 10s 18ms/step - loss: 224.6266 - synergy_loss: 24.4315 - class_loss: 0.2032\nEpoch 538/1000\n570/570 [==============================] - 11s 19ms/step - loss: 224.7226 - synergy_loss: 24.7674 - class_loss: 0.2038\nEpoch 539/1000\n570/570 [==============================] - 10s 18ms/step - loss: 223.5719 - synergy_loss: 23.8390 - class_loss: 0.2060\nEpoch 540/1000\n570/570 [==============================] - 10s 18ms/step - loss: 223.3616 - synergy_loss: 23.8761 - class_loss: 0.2002\nEpoch 541/1000\n570/570 [==============================] - 11s 19ms/step - loss: 223.9008 - synergy_loss: 24.6543 - class_loss: 0.2006\nEpoch 542/1000\n570/570 [==============================] - 10s 18ms/step - loss: 223.3784 - synergy_loss: 24.3615 - class_loss: 0.2022\nEpoch 543/1000\n570/570 [==============================] - 10s 18ms/step - loss: 222.4473 - synergy_loss: 23.6695 - class_loss: 0.1989\nEpoch 544/1000\n570/570 [==============================] - 11s 19ms/step - loss: 222.6672 - synergy_loss: 24.1237 - class_loss: 0.2040\nEpoch 545/1000\n570/570 [==============================] - 10s 18ms/step - loss: 222.4658 - synergy_loss: 24.1530 - class_loss: 0.1993\nEpoch 546/1000\n570/570 [==============================] - 11s 18ms/step - loss: 221.7025 - synergy_loss: 23.6445 - class_loss: 0.1977\nEpoch 547/1000\n570/570 [==============================] - 11s 19ms/step - loss: 221.5404 - synergy_loss: 23.7190 - class_loss: 0.1974\nEpoch 548/1000\n570/570 [==============================] - 10s 18ms/step - loss: 221.5177 - synergy_loss: 23.9262 - class_loss: 0.1984\nEpoch 549/1000\n570/570 [==============================] - 10s 18ms/step - loss: 221.3989 - synergy_loss: 24.0519 - class_loss: 0.2002\nEpoch 550/1000\n570/570 [==============================] - 11s 19ms/step - loss: 221.0089 - synergy_loss: 23.8965 - class_loss: 0.1938\nEpoch 551/1000\n570/570 [==============================] - 10s 18ms/step - loss: 220.4922 - synergy_loss: 23.6084 - class_loss: 0.1957\nEpoch 552/1000\n570/570 [==============================] - 10s 18ms/step - loss: 220.3240 - synergy_loss: 23.6636 - class_loss: 0.1959\nEpoch 553/1000\n570/570 [==============================] - 11s 19ms/step - loss: 220.0333 - synergy_loss: 23.6150 - class_loss: 0.1943\nEpoch 554/1000\n570/570 [==============================] - 10s 18ms/step - loss: 219.9234 - synergy_loss: 23.7337 - class_loss: 0.1952\nEpoch 555/1000\n570/570 [==============================] - 10s 18ms/step - loss: 219.8323 - synergy_loss: 23.8941 - class_loss: 0.1921\nEpoch 556/1000\n570/570 [==============================] - 11s 19ms/step - loss: 219.4071 - synergy_loss: 23.6925 - class_loss: 0.1967\nEpoch 557/1000\n570/570 [==============================] - 11s 19ms/step - loss: 218.3834 - synergy_loss: 22.9135 - class_loss: 0.1909\nEpoch 558/1000\n570/570 [==============================] - 10s 18ms/step - loss: 218.5354 - synergy_loss: 23.3139 - class_loss: 0.1920\nEpoch 559/1000\n570/570 [==============================] - 11s 19ms/step - loss: 218.0093 - synergy_loss: 23.0150 - class_loss: 0.1926\nEpoch 560/1000\n570/570 [==============================] - 10s 18ms/step - loss: 218.2017 - synergy_loss: 23.4573 - class_loss: 0.1939\nEpoch 561/1000\n570/570 [==============================] - 10s 18ms/step - loss: 218.2286 - synergy_loss: 23.7044 - class_loss: 0.1925\nEpoch 562/1000\n570/570 [==============================] - 11s 19ms/step - loss: 217.7061 - synergy_loss: 23.4256 - class_loss: 0.1887\nEpoch 563/1000\n570/570 [==============================] - 10s 18ms/step - loss: 217.1274 - synergy_loss: 23.1018 - class_loss: 0.1889\nEpoch 564/1000\n570/570 [==============================] - 10s 18ms/step - loss: 217.4707 - synergy_loss: 23.6849 - class_loss: 0.1895\nEpoch 565/1000\n570/570 [==============================] - 11s 19ms/step - loss: 216.5059 - synergy_loss: 22.9494 - class_loss: 0.1895\nEpoch 566/1000\n570/570 [==============================] - 11s 19ms/step - loss: 216.3564 - synergy_loss: 23.0332 - class_loss: 0.1891\nEpoch 567/1000\n570/570 [==============================] - 10s 18ms/step - loss: 216.5838 - synergy_loss: 23.4885 - class_loss: 0.1905\nEpoch 568/1000\n570/570 [==============================] - 12s 20ms/step - loss: 215.6480 - synergy_loss: 22.8070 - class_loss: 0.1828\nEpoch 569/1000\n570/570 [==============================] - 10s 18ms/step - loss: 215.2506 - synergy_loss: 22.6615 - class_loss: 0.1902\nEpoch 570/1000\n570/570 [==============================] - 11s 19ms/step - loss: 215.4408 - synergy_loss: 23.0954 - class_loss: 0.1887\nEpoch 571/1000\n570/570 [==============================] - 11s 19ms/step - loss: 215.3420 - synergy_loss: 23.2213 - class_loss: 0.1871\nEpoch 572/1000\n570/570 [==============================] - 11s 19ms/step - loss: 214.5944 - synergy_loss: 22.7116 - class_loss: 0.1866\nEpoch 573/1000\n570/570 [==============================] - 10s 18ms/step - loss: 214.1959 - synergy_loss: 22.5620 - class_loss: 0.1831\nEpoch 574/1000\n570/570 [==============================] - 11s 19ms/step - loss: 214.6676 - synergy_loss: 23.2635 - class_loss: 0.1862\nEpoch 575/1000\n570/570 [==============================] - 11s 19ms/step - loss: 213.9236 - synergy_loss: 22.7495 - class_loss: 0.1876\nEpoch 576/1000\n570/570 [==============================] - 10s 18ms/step - loss: 214.6130 - synergy_loss: 23.6627 - class_loss: 0.1832\nEpoch 577/1000\n570/570 [==============================] - 11s 19ms/step - loss: 213.2599 - synergy_loss: 22.5342 - class_loss: 0.1836\nEpoch 578/1000\n570/570 [==============================] - 10s 18ms/step - loss: 212.8965 - synergy_loss: 22.4109 - class_loss: 0.1838\nEpoch 579/1000\n570/570 [==============================] - 10s 18ms/step - loss: 212.9034 - synergy_loss: 22.6559 - class_loss: 0.1802\nEpoch 580/1000\n570/570 [==============================] - 11s 19ms/step - loss: 212.9604 - synergy_loss: 22.9536 - class_loss: 0.1795\nEpoch 581/1000\n570/570 [==============================] - 10s 18ms/step - loss: 212.0524 - synergy_loss: 22.2721 - class_loss: 0.1830\nEpoch 582/1000\n570/570 [==============================] - 10s 18ms/step - loss: 212.4011 - synergy_loss: 22.8609 - class_loss: 0.1842\nEpoch 583/1000\n570/570 [==============================] - 11s 19ms/step - loss: 211.5708 - synergy_loss: 22.2771 - class_loss: 0.1805\nEpoch 584/1000\n570/570 [==============================] - 10s 18ms/step - loss: 211.0838 - synergy_loss: 22.0333 - class_loss: 0.1805\nEpoch 585/1000\n570/570 [==============================] - 10s 18ms/step - loss: 211.2276 - synergy_loss: 22.4291 - class_loss: 0.1834\nEpoch 586/1000\n570/570 [==============================] - 11s 19ms/step - loss: 211.3052 - synergy_loss: 22.7324 - class_loss: 0.1776\nEpoch 587/1000\n570/570 [==============================] - 10s 18ms/step - loss: 210.6734 - synergy_loss: 22.3410 - class_loss: 0.1782\nEpoch 588/1000\n570/570 [==============================] - 10s 18ms/step - loss: 210.7883 - synergy_loss: 22.6893 - class_loss: 0.1801\nEpoch 589/1000\n570/570 [==============================] - 11s 19ms/step - loss: 210.6068 - synergy_loss: 22.7316 - class_loss: 0.1779\nEpoch 590/1000\n570/570 [==============================] - 10s 18ms/step - loss: 210.1619 - synergy_loss: 22.5271 - class_loss: 0.1772\nEpoch 591/1000\n570/570 [==============================] - 10s 18ms/step - loss: 209.4310 - synergy_loss: 22.0318 - class_loss: 0.1773\nEpoch 592/1000\n570/570 [==============================] - 11s 19ms/step - loss: 209.2383 - synergy_loss: 22.0671 - class_loss: 0.1776\nEpoch 593/1000\n570/570 [==============================] - 10s 18ms/step - loss: 209.2611 - synergy_loss: 22.3348 - class_loss: 0.1760\nEpoch 594/1000\n570/570 [==============================] - 10s 18ms/step - loss: 209.2331 - synergy_loss: 22.5256 - class_loss: 0.1763\nEpoch 595/1000\n570/570 [==============================] - 11s 19ms/step - loss: 208.4755 - synergy_loss: 22.0075 - class_loss: 0.1774\nEpoch 596/1000\n570/570 [==============================] - 10s 18ms/step - loss: 208.3139 - synergy_loss: 22.0875 - class_loss: 0.1764\nEpoch 597/1000\n570/570 [==============================] - 10s 18ms/step - loss: 208.1247 - synergy_loss: 22.1237 - class_loss: 0.1733\nEpoch 598/1000\n570/570 [==============================] - 11s 19ms/step - loss: 210.1868 - synergy_loss: 24.3476 - class_loss: 0.1740\nEpoch 599/1000\n570/570 [==============================] - 10s 18ms/step - loss: 208.1685 - synergy_loss: 22.5385 - class_loss: 0.1767\nEpoch 600/1000\n570/570 [==============================] - 10s 18ms/step - loss: 207.3723 - synergy_loss: 21.9605 - class_loss: 0.1762\nEpoch 601/1000\n570/570 [==============================] - 11s 19ms/step - loss: 206.3321 - synergy_loss: 21.1475 - class_loss: 0.1738\nEpoch 602/1000\n570/570 [==============================] - 10s 18ms/step - loss: 206.7424 - synergy_loss: 21.7936 - class_loss: 0.1725\nEpoch 603/1000\n570/570 [==============================] - 10s 18ms/step - loss: 206.6676 - synergy_loss: 21.9426 - class_loss: 0.1698\nEpoch 604/1000\n570/570 [==============================] - 11s 19ms/step - loss: 205.7022 - synergy_loss: 21.2148 - class_loss: 0.1687\nEpoch 605/1000\n570/570 [==============================] - 10s 18ms/step - loss: 206.3153 - synergy_loss: 22.0607 - class_loss: 0.1699\nEpoch 606/1000\n570/570 [==============================] - 11s 19ms/step - loss: 205.5932 - synergy_loss: 21.5781 - class_loss: 0.1718\nEpoch 607/1000\n570/570 [==============================] - 11s 20ms/step - loss: 205.5706 - synergy_loss: 21.7896 - class_loss: 0.1710\nEpoch 608/1000\n570/570 [==============================] - 11s 19ms/step - loss: 205.2673 - synergy_loss: 21.7197 - class_loss: 0.1679\nEpoch 609/1000\n570/570 [==============================] - 11s 18ms/step - loss: 204.9292 - synergy_loss: 21.6284 - class_loss: 0.1641\nEpoch 610/1000\n570/570 [==============================] - 11s 20ms/step - loss: 204.8528 - synergy_loss: 21.7904 - class_loss: 0.1717\nEpoch 611/1000\n570/570 [==============================] - 11s 18ms/step - loss: 204.6374 - synergy_loss: 21.8075 - class_loss: 0.1699\nEpoch 612/1000\n570/570 [==============================] - 11s 19ms/step - loss: 204.5876 - synergy_loss: 21.9931 - class_loss: 0.1686\nEpoch 613/1000\n570/570 [==============================] - 11s 19ms/step - loss: 204.2078 - synergy_loss: 21.8599 - class_loss: 0.1668\nEpoch 614/1000\n570/570 [==============================] - 11s 19ms/step - loss: 203.9389 - synergy_loss: 21.8126 - class_loss: 0.1671\nEpoch 615/1000\n570/570 [==============================] - 11s 18ms/step - loss: 203.4584 - synergy_loss: 21.5726 - class_loss: 0.1656\nEpoch 616/1000\n570/570 [==============================] - 11s 19ms/step - loss: 203.1396 - synergy_loss: 21.4901 - class_loss: 0.1676\nEpoch 617/1000\n570/570 [==============================] - 10s 18ms/step - loss: 203.4332 - synergy_loss: 22.0260 - class_loss: 0.1667\nEpoch 618/1000\n570/570 [==============================] - 10s 18ms/step - loss: 203.0481 - synergy_loss: 21.8705 - class_loss: 0.1636\nEpoch 619/1000\n570/570 [==============================] - 11s 19ms/step - loss: 202.1032 - synergy_loss: 21.1499 - class_loss: 0.1660\nEpoch 620/1000\n570/570 [==============================] - 10s 18ms/step - loss: 202.0711 - synergy_loss: 21.3531 - class_loss: 0.1675\nEpoch 621/1000\n570/570 [==============================] - 11s 18ms/step - loss: 201.5799 - synergy_loss: 21.1169 - class_loss: 0.1655\nEpoch 622/1000\n570/570 [==============================] - 11s 19ms/step - loss: 201.4716 - synergy_loss: 21.2446 - class_loss: 0.1632\nEpoch 623/1000\n570/570 [==============================] - 10s 18ms/step - loss: 202.6803 - synergy_loss: 22.6656 - class_loss: 0.1654\nEpoch 624/1000\n570/570 [==============================] - 11s 18ms/step - loss: 201.1103 - synergy_loss: 21.3075 - class_loss: 0.1607\nEpoch 625/1000\n570/570 [==============================] - 11s 19ms/step - loss: 201.1635 - synergy_loss: 21.5793 - class_loss: 0.1666\nEpoch 626/1000\n570/570 [==============================] - 10s 18ms/step - loss: 200.7411 - synergy_loss: 21.3741 - class_loss: 0.1617\nEpoch 627/1000\n570/570 [==============================] - 10s 18ms/step - loss: 200.0288 - synergy_loss: 20.8995 - class_loss: 0.1619\nEpoch 628/1000\n570/570 [==============================] - 11s 19ms/step - loss: 199.8055 - synergy_loss: 20.9120 - class_loss: 0.1630\nEpoch 629/1000\n570/570 [==============================] - 10s 18ms/step - loss: 199.9041 - synergy_loss: 21.2467 - class_loss: 0.1616\nEpoch 630/1000\n570/570 [==============================] - 11s 18ms/step - loss: 200.2068 - synergy_loss: 21.7702 - class_loss: 0.1613\nEpoch 631/1000\n570/570 [==============================] - 11s 19ms/step - loss: 199.2639 - synergy_loss: 21.0401 - class_loss: 0.1633\nEpoch 632/1000\n570/570 [==============================] - 10s 18ms/step - loss: 199.0784 - synergy_loss: 21.0854 - class_loss: 0.1623\nEpoch 633/1000\n570/570 [==============================] - 10s 18ms/step - loss: 198.7372 - synergy_loss: 20.9863 - class_loss: 0.1581\nEpoch 634/1000\n570/570 [==============================] - 11s 19ms/step - loss: 198.6667 - synergy_loss: 21.1395 - class_loss: 0.1583\nEpoch 635/1000\n570/570 [==============================] - 10s 18ms/step - loss: 198.0058 - synergy_loss: 20.7102 - class_loss: 0.1579\nEpoch 636/1000\n570/570 [==============================] - 10s 18ms/step - loss: 197.9604 - synergy_loss: 20.8924 - class_loss: 0.1613\nEpoch 637/1000\n570/570 [==============================] - 11s 19ms/step - loss: 197.9211 - synergy_loss: 21.0837 - class_loss: 0.1600\nEpoch 638/1000\n570/570 [==============================] - 10s 18ms/step - loss: 197.9018 - synergy_loss: 21.2937 - class_loss: 0.1585\nEpoch 639/1000\n570/570 [==============================] - 10s 18ms/step - loss: 197.7524 - synergy_loss: 21.3453 - class_loss: 0.1572\nEpoch 640/1000\n570/570 [==============================] - 11s 19ms/step - loss: 197.6020 - synergy_loss: 21.3819 - class_loss: 0.1586\nEpoch 641/1000\n570/570 [==============================] - 10s 18ms/step - loss: 196.6240 - synergy_loss: 20.6420 - class_loss: 0.1563\nEpoch 642/1000\n570/570 [==============================] - 10s 18ms/step - loss: 196.4908 - synergy_loss: 20.7352 - class_loss: 0.1557\nEpoch 643/1000\n570/570 [==============================] - 11s 19ms/step - loss: 196.4884 - synergy_loss: 20.9535 - class_loss: 0.1582\nEpoch 644/1000\n570/570 [==============================] - 10s 18ms/step - loss: 196.3088 - synergy_loss: 21.0062 - class_loss: 0.1571\nEpoch 645/1000\n570/570 [==============================] - 10s 18ms/step - loss: 195.9953 - synergy_loss: 20.9163 - class_loss: 0.1550\nEpoch 646/1000\n570/570 [==============================] - 11s 19ms/step - loss: 195.8472 - synergy_loss: 20.9837 - class_loss: 0.1548\nEpoch 647/1000\n570/570 [==============================] - 10s 18ms/step - loss: 194.9664 - synergy_loss: 20.3337 - class_loss: 0.1511\nEpoch 648/1000\n570/570 [==============================] - 10s 18ms/step - loss: 195.2097 - synergy_loss: 20.8049 - class_loss: 0.1530\nEpoch 649/1000\n570/570 [==============================] - 11s 19ms/step - loss: 194.5750 - synergy_loss: 20.3993 - class_loss: 0.1551\nEpoch 650/1000\n570/570 [==============================] - 10s 18ms/step - loss: 194.4427 - synergy_loss: 20.5018 - class_loss: 0.1551\nEpoch 651/1000\n570/570 [==============================] - 10s 18ms/step - loss: 194.4154 - synergy_loss: 20.7166 - class_loss: 0.1545\nEpoch 652/1000\n570/570 [==============================] - 11s 19ms/step - loss: 194.0143 - synergy_loss: 20.5408 - class_loss: 0.1547\nEpoch 653/1000\n570/570 [==============================] - 10s 18ms/step - loss: 193.9584 - synergy_loss: 20.7117 - class_loss: 0.1504\nEpoch 654/1000\n570/570 [==============================] - 10s 18ms/step - loss: 194.7073 - synergy_loss: 21.6087 - class_loss: 0.1568\nEpoch 655/1000\n570/570 [==============================] - 11s 19ms/step - loss: 193.0433 - synergy_loss: 20.1852 - class_loss: 0.1501\nEpoch 656/1000\n570/570 [==============================] - 10s 18ms/step - loss: 192.8677 - synergy_loss: 20.2317 - class_loss: 0.1523\nEpoch 657/1000\n570/570 [==============================] - 10s 18ms/step - loss: 192.8928 - synergy_loss: 20.4888 - class_loss: 0.1552\nEpoch 658/1000\n570/570 [==============================] - 11s 19ms/step - loss: 192.5726 - synergy_loss: 20.4003 - class_loss: 0.1538\nEpoch 659/1000\n570/570 [==============================] - 10s 18ms/step - loss: 192.2852 - synergy_loss: 20.3303 - class_loss: 0.1491\nEpoch 660/1000\n570/570 [==============================] - 11s 19ms/step - loss: 191.8683 - synergy_loss: 20.1712 - class_loss: 0.1486\nEpoch 661/1000\n570/570 [==============================] - 11s 20ms/step - loss: 191.9296 - synergy_loss: 20.4625 - class_loss: 0.1489\nEpoch 662/1000\n570/570 [==============================] - 11s 19ms/step - loss: 191.6372 - synergy_loss: 20.3945 - class_loss: 0.1482\nEpoch 663/1000\n570/570 [==============================] - 11s 19ms/step - loss: 192.0453 - synergy_loss: 21.0210 - class_loss: 0.1503\nEpoch 664/1000\n570/570 [==============================] - 11s 19ms/step - loss: 190.8445 - synergy_loss: 20.0587 - class_loss: 0.1458\nEpoch 665/1000\n570/570 [==============================] - 11s 19ms/step - loss: 190.3186 - synergy_loss: 19.7556 - class_loss: 0.1465\nEpoch 666/1000\n570/570 [==============================] - 11s 18ms/step - loss: 191.0069 - synergy_loss: 20.6764 - class_loss: 0.1465\nEpoch 667/1000\n570/570 [==============================] - 11s 19ms/step - loss: 190.2554 - synergy_loss: 20.1441 - class_loss: 0.1470\nEpoch 668/1000\n570/570 [==============================] - 11s 19ms/step - loss: 190.0853 - synergy_loss: 20.2164 - class_loss: 0.1470\nEpoch 669/1000\n570/570 [==============================] - 11s 19ms/step - loss: 190.1139 - synergy_loss: 20.4727 - class_loss: 0.1466\nEpoch 670/1000\n570/570 [==============================] - 11s 19ms/step - loss: 189.1732 - synergy_loss: 19.7641 - class_loss: 0.1489\nEpoch 671/1000\n570/570 [==============================] - 11s 19ms/step - loss: 189.4101 - synergy_loss: 20.2335 - class_loss: 0.1482\nEpoch 672/1000\n570/570 [==============================] - 10s 18ms/step - loss: 188.7640 - synergy_loss: 19.8342 - class_loss: 0.1442\nEpoch 673/1000\n570/570 [==============================] - 11s 19ms/step - loss: 188.8249 - synergy_loss: 20.1108 - class_loss: 0.1465\nEpoch 674/1000\n570/570 [==============================] - 10s 18ms/step - loss: 188.9158 - synergy_loss: 20.4222 - class_loss: 0.1471\nEpoch 675/1000\n570/570 [==============================] - 10s 18ms/step - loss: 188.3285 - synergy_loss: 20.0635 - class_loss: 0.1458\nEpoch 676/1000\n570/570 [==============================] - 11s 19ms/step - loss: 188.6928 - synergy_loss: 20.6546 - class_loss: 0.1461\nEpoch 677/1000\n570/570 [==============================] - 10s 18ms/step - loss: 187.7376 - synergy_loss: 19.9047 - class_loss: 0.1445\nEpoch 678/1000\n570/570 [==============================] - 10s 18ms/step - loss: 187.5395 - synergy_loss: 19.9336 - class_loss: 0.1502\nEpoch 679/1000\n570/570 [==============================] - 11s 19ms/step - loss: 187.3081 - synergy_loss: 19.9384 - class_loss: 0.1444\nEpoch 680/1000\n570/570 [==============================] - 10s 18ms/step - loss: 187.2699 - synergy_loss: 20.1302 - class_loss: 0.1445\nEpoch 681/1000\n570/570 [==============================] - 10s 18ms/step - loss: 186.7709 - synergy_loss: 19.8538 - class_loss: 0.1433\nEpoch 682/1000\n570/570 [==============================] - 11s 19ms/step - loss: 186.3987 - synergy_loss: 19.7102 - class_loss: 0.1397\nEpoch 683/1000\n570/570 [==============================] - 10s 18ms/step - loss: 186.5822 - synergy_loss: 20.1155 - class_loss: 0.1430\nEpoch 684/1000\n570/570 [==============================] - 10s 18ms/step - loss: 186.1655 - synergy_loss: 19.9217 - class_loss: 0.1432\nEpoch 685/1000\n570/570 [==============================] - 11s 19ms/step - loss: 185.7262 - synergy_loss: 19.7158 - class_loss: 0.1411\nEpoch 686/1000\n570/570 [==============================] - 10s 18ms/step - loss: 186.0795 - synergy_loss: 20.2860 - class_loss: 0.1450\nEpoch 687/1000\n570/570 [==============================] - 10s 18ms/step - loss: 185.1898 - synergy_loss: 19.6093 - class_loss: 0.1426\nEpoch 688/1000\n570/570 [==============================] - 11s 19ms/step - loss: 184.7237 - synergy_loss: 19.3908 - class_loss: 0.1376\nEpoch 689/1000\n570/570 [==============================] - 10s 18ms/step - loss: 184.7527 - synergy_loss: 19.6434 - class_loss: 0.1391\nEpoch 690/1000\n570/570 [==============================] - 10s 18ms/step - loss: 184.2509 - synergy_loss: 19.3709 - class_loss: 0.1384\nEpoch 691/1000\n570/570 [==============================] - 11s 19ms/step - loss: 183.7007 - synergy_loss: 19.0718 - class_loss: 0.1388\nEpoch 692/1000\n570/570 [==============================] - 10s 18ms/step - loss: 184.2614 - synergy_loss: 19.8579 - class_loss: 0.1396\nEpoch 693/1000\n570/570 [==============================] - 10s 18ms/step - loss: 183.6444 - synergy_loss: 19.4627 - class_loss: 0.1374\nEpoch 694/1000\n570/570 [==============================] - 11s 19ms/step - loss: 183.4455 - synergy_loss: 19.5082 - class_loss: 0.1394\nEpoch 695/1000\n570/570 [==============================] - 10s 18ms/step - loss: 183.3123 - synergy_loss: 19.6094 - class_loss: 0.1377\nEpoch 696/1000\n570/570 [==============================] - 10s 18ms/step - loss: 183.5933 - synergy_loss: 20.0958 - class_loss: 0.1421\nEpoch 697/1000\n570/570 [==============================] - 11s 19ms/step - loss: 182.7257 - synergy_loss: 19.4587 - class_loss: 0.1412\nEpoch 698/1000\n570/570 [==============================] - 10s 18ms/step - loss: 182.7619 - synergy_loss: 19.7228 - class_loss: 0.1390\nEpoch 699/1000\n570/570 [==============================] - 10s 18ms/step - loss: 182.6295 - synergy_loss: 19.8247 - class_loss: 0.1346\nEpoch 700/1000\n570/570 [==============================] - 11s 19ms/step - loss: 182.0795 - synergy_loss: 19.4910 - class_loss: 0.1384\nEpoch 701/1000\n570/570 [==============================] - 11s 18ms/step - loss: 181.3837 - synergy_loss: 19.0234 - class_loss: 0.1399\nEpoch 702/1000\n570/570 [==============================] - 10s 18ms/step - loss: 181.1920 - synergy_loss: 19.0829 - class_loss: 0.1347\nEpoch 703/1000\n570/570 [==============================] - 10s 18ms/step - loss: 181.2290 - synergy_loss: 19.3583 - class_loss: 0.1357\nEpoch 704/1000\n570/570 [==============================] - 11s 19ms/step - loss: 181.0195 - synergy_loss: 19.3742 - class_loss: 0.1369\nEpoch 705/1000\n570/570 [==============================] - 10s 18ms/step - loss: 180.7214 - synergy_loss: 19.3199 - class_loss: 0.1370\nEpoch 706/1000\n570/570 [==============================] - 10s 18ms/step - loss: 180.5217 - synergy_loss: 19.3339 - class_loss: 0.1394\nEpoch 707/1000\n570/570 [==============================] - 11s 19ms/step - loss: 180.1173 - synergy_loss: 19.1796 - class_loss: 0.1354\nEpoch 708/1000\n570/570 [==============================] - 10s 18ms/step - loss: 180.0295 - synergy_loss: 19.3181 - class_loss: 0.1369\nEpoch 709/1000\n570/570 [==============================] - 10s 18ms/step - loss: 179.8229 - synergy_loss: 19.3528 - class_loss: 0.1321\nEpoch 710/1000\n570/570 [==============================] - 10s 18ms/step - loss: 179.3059 - synergy_loss: 19.0509 - class_loss: 0.1359\nEpoch 711/1000\n570/570 [==============================] - 10s 18ms/step - loss: 179.4880 - synergy_loss: 19.4659 - class_loss: 0.1357\nEpoch 712/1000\n570/570 [==============================] - 10s 18ms/step - loss: 179.8199 - synergy_loss: 19.9844 - class_loss: 0.1353\nEpoch 713/1000\n570/570 [==============================] - 11s 18ms/step - loss: 178.5538 - synergy_loss: 18.9542 - class_loss: 0.1369\nEpoch 714/1000\n570/570 [==============================] - 10s 18ms/step - loss: 178.3167 - synergy_loss: 18.9505 - class_loss: 0.1345\nEpoch 715/1000\n570/570 [==============================] - 10s 18ms/step - loss: 178.1372 - synergy_loss: 19.0046 - class_loss: 0.1336\nEpoch 716/1000\n570/570 [==============================] - 11s 19ms/step - loss: 178.5183 - synergy_loss: 19.5956 - class_loss: 0.1316\nEpoch 717/1000\n570/570 [==============================] - 11s 19ms/step - loss: 177.6187 - synergy_loss: 18.9279 - class_loss: 0.1339\nEpoch 718/1000\n570/570 [==============================] - 10s 18ms/step - loss: 177.7694 - synergy_loss: 19.3051 - class_loss: 0.1335\nEpoch 719/1000\n570/570 [==============================] - 11s 19ms/step - loss: 178.2074 - synergy_loss: 19.9378 - class_loss: 0.1327\nEpoch 720/1000\n570/570 [==============================] - 10s 18ms/step - loss: 177.2995 - synergy_loss: 19.2489 - class_loss: 0.1312\nEpoch 721/1000\n570/570 [==============================] - 10s 18ms/step - loss: 177.1087 - synergy_loss: 19.2740 - class_loss: 0.1284\nEpoch 722/1000\n570/570 [==============================] - 11s 19ms/step - loss: 176.4622 - synergy_loss: 18.8488 - class_loss: 0.1293\nEpoch 723/1000\n570/570 [==============================] - 10s 18ms/step - loss: 176.2381 - synergy_loss: 18.8380 - class_loss: 0.1296\nEpoch 724/1000\n570/570 [==============================] - 10s 18ms/step - loss: 176.2678 - synergy_loss: 19.0812 - class_loss: 0.1337\nEpoch 725/1000\n570/570 [==============================] - 11s 19ms/step - loss: 175.7407 - synergy_loss: 18.7897 - class_loss: 0.1290\nEpoch 726/1000\n570/570 [==============================] - 10s 18ms/step - loss: 176.0074 - synergy_loss: 19.2741 - class_loss: 0.1312\nEpoch 727/1000\n570/570 [==============================] - 10s 18ms/step - loss: 175.4570 - synergy_loss: 18.9323 - class_loss: 0.1298\nEpoch 728/1000\n570/570 [==============================] - 11s 18ms/step - loss: 175.1810 - synergy_loss: 18.8856 - class_loss: 0.1259\nEpoch 729/1000\n570/570 [==============================] - 10s 18ms/step - loss: 174.8132 - synergy_loss: 18.7355 - class_loss: 0.1311\nEpoch 730/1000\n570/570 [==============================] - 10s 18ms/step - loss: 174.6730 - synergy_loss: 18.8287 - class_loss: 0.1320\nEpoch 731/1000\n570/570 [==============================] - 11s 19ms/step - loss: 174.8277 - synergy_loss: 19.2173 - class_loss: 0.1296\nEpoch 732/1000\n570/570 [==============================] - 10s 18ms/step - loss: 174.7271 - synergy_loss: 19.2925 - class_loss: 0.1338\nEpoch 733/1000\n570/570 [==============================] - 10s 18ms/step - loss: 173.9644 - synergy_loss: 18.7505 - class_loss: 0.1298\nEpoch 734/1000\n570/570 [==============================] - 11s 18ms/step - loss: 173.9488 - synergy_loss: 18.9545 - class_loss: 0.1270\nEpoch 735/1000\n570/570 [==============================] - 10s 18ms/step - loss: 173.0891 - synergy_loss: 18.3212 - class_loss: 0.1264\nEpoch 736/1000\n570/570 [==============================] - 10s 18ms/step - loss: 173.2231 - synergy_loss: 18.6763 - class_loss: 0.1268\nEpoch 737/1000\n570/570 [==============================] - 10s 18ms/step - loss: 173.0571 - synergy_loss: 18.7190 - class_loss: 0.1316\nEpoch 738/1000\n570/570 [==============================] - 10s 18ms/step - loss: 172.9943 - synergy_loss: 18.8757 - class_loss: 0.1244\nEpoch 739/1000\n570/570 [==============================] - 10s 18ms/step - loss: 172.7905 - synergy_loss: 18.8995 - class_loss: 0.1261\nEpoch 740/1000\n570/570 [==============================] - 11s 20ms/step - loss: 172.2324 - synergy_loss: 18.5573 - class_loss: 0.1277\nEpoch 741/1000\n570/570 [==============================] - 10s 18ms/step - loss: 172.2108 - synergy_loss: 18.7619 - class_loss: 0.1311\nEpoch 742/1000\n570/570 [==============================] - 10s 18ms/step - loss: 171.9616 - synergy_loss: 18.7231 - class_loss: 0.1250\nEpoch 743/1000\n570/570 [==============================] - 11s 19ms/step - loss: 171.8852 - synergy_loss: 18.8601 - class_loss: 0.1278\nEpoch 744/1000\n570/570 [==============================] - 10s 18ms/step - loss: 172.1986 - synergy_loss: 19.3400 - class_loss: 0.1233\nEpoch 745/1000\n570/570 [==============================] - 10s 18ms/step - loss: 171.3688 - synergy_loss: 18.7413 - class_loss: 0.1237\nEpoch 746/1000\n570/570 [==============================] - 11s 19ms/step - loss: 171.2687 - synergy_loss: 18.8362 - class_loss: 0.1256\nEpoch 747/1000\n570/570 [==============================] - 10s 18ms/step - loss: 171.0401 - synergy_loss: 18.8279 - class_loss: 0.1210\nEpoch 748/1000\n570/570 [==============================] - 10s 18ms/step - loss: 170.7822 - synergy_loss: 18.7665 - class_loss: 0.1249\nEpoch 749/1000\n570/570 [==============================] - 11s 19ms/step - loss: 170.4581 - synergy_loss: 18.6548 - class_loss: 0.1249\nEpoch 750/1000\n570/570 [==============================] - 10s 18ms/step - loss: 170.2164 - synergy_loss: 18.6231 - class_loss: 0.1244\nEpoch 751/1000\n570/570 [==============================] - 10s 18ms/step - loss: 169.8880 - synergy_loss: 18.5097 - class_loss: 0.1235\nEpoch 752/1000\n570/570 [==============================] - 11s 19ms/step - loss: 169.3110 - synergy_loss: 18.1608 - class_loss: 0.1244\nEpoch 753/1000\n570/570 [==============================] - 11s 19ms/step - loss: 169.3699 - synergy_loss: 18.4333 - class_loss: 0.1237\nEpoch 754/1000\n570/570 [==============================] - 11s 19ms/step - loss: 169.1689 - synergy_loss: 18.4630 - class_loss: 0.1238\nEpoch 755/1000\n570/570 [==============================] - 12s 20ms/step - loss: 168.9597 - synergy_loss: 18.4650 - class_loss: 0.1254\nEpoch 756/1000\n570/570 [==============================] - 10s 18ms/step - loss: 168.4844 - synergy_loss: 18.2172 - class_loss: 0.1219\nEpoch 757/1000\n570/570 [==============================] - 11s 19ms/step - loss: 168.3609 - synergy_loss: 18.3157 - class_loss: 0.1201\nEpoch 758/1000\n570/570 [==============================] - 11s 20ms/step - loss: 168.1333 - synergy_loss: 18.3270 - class_loss: 0.1204\nEpoch 759/1000\n570/570 [==============================] - 10s 18ms/step - loss: 168.4932 - synergy_loss: 18.8878 - class_loss: 0.1235\nEpoch 760/1000\n570/570 [==============================] - 11s 18ms/step - loss: 167.9351 - synergy_loss: 18.5438 - class_loss: 0.1204\nEpoch 761/1000\n570/570 [==============================] - 11s 20ms/step - loss: 167.6591 - synergy_loss: 18.4804 - class_loss: 0.1228\nEpoch 762/1000\n570/570 [==============================] - 11s 19ms/step - loss: 168.1432 - synergy_loss: 19.1518 - class_loss: 0.1224\nEpoch 763/1000\n570/570 [==============================] - 11s 19ms/step - loss: 166.6948 - synergy_loss: 17.9152 - class_loss: 0.1220\nEpoch 764/1000\n570/570 [==============================] - 11s 19ms/step - loss: 166.8618 - synergy_loss: 18.3074 - class_loss: 0.1212\nEpoch 765/1000\n570/570 [==============================] - 10s 18ms/step - loss: 166.5746 - synergy_loss: 18.2412 - class_loss: 0.1232\nEpoch 766/1000\n570/570 [==============================] - 10s 18ms/step - loss: 166.4597 - synergy_loss: 18.3466 - class_loss: 0.1209\nEpoch 767/1000\n570/570 [==============================] - 11s 19ms/step - loss: 165.7661 - synergy_loss: 17.8648 - class_loss: 0.1210\nEpoch 768/1000\n570/570 [==============================] - 10s 18ms/step - loss: 166.0505 - synergy_loss: 18.3593 - class_loss: 0.1209\nEpoch 769/1000\n570/570 [==============================] - 10s 18ms/step - loss: 165.5122 - synergy_loss: 18.0416 - class_loss: 0.1185\nEpoch 770/1000\n570/570 [==============================] - 11s 19ms/step - loss: 165.5311 - synergy_loss: 18.2770 - class_loss: 0.1172\nEpoch 771/1000\n570/570 [==============================] - 10s 18ms/step - loss: 165.2024 - synergy_loss: 18.1552 - class_loss: 0.1195\nEpoch 772/1000\n570/570 [==============================] - 10s 18ms/step - loss: 165.5575 - synergy_loss: 18.7139 - class_loss: 0.1205\nEpoch 773/1000\n570/570 [==============================] - 11s 19ms/step - loss: 164.8559 - synergy_loss: 18.2463 - class_loss: 0.1191\nEpoch 774/1000\n570/570 [==============================] - 10s 18ms/step - loss: 164.5813 - synergy_loss: 18.1739 - class_loss: 0.1195\nEpoch 775/1000\n570/570 [==============================] - 10s 18ms/step - loss: 164.6278 - synergy_loss: 18.4378 - class_loss: 0.1194\nEpoch 776/1000\n570/570 [==============================] - 11s 19ms/step - loss: 164.6515 - synergy_loss: 18.6555 - class_loss: 0.1179\nEpoch 777/1000\n570/570 [==============================] - 10s 18ms/step - loss: 163.9309 - synergy_loss: 18.1542 - class_loss: 0.1161\nEpoch 778/1000\n570/570 [==============================] - 10s 18ms/step - loss: 163.5574 - synergy_loss: 17.9944 - class_loss: 0.1174\nEpoch 779/1000\n570/570 [==============================] - 11s 19ms/step - loss: 163.0200 - synergy_loss: 17.6891 - class_loss: 0.1166\nEpoch 780/1000\n570/570 [==============================] - 10s 18ms/step - loss: 162.8558 - synergy_loss: 17.7391 - class_loss: 0.1173\nEpoch 781/1000\n570/570 [==============================] - 10s 18ms/step - loss: 163.1183 - synergy_loss: 18.2124 - class_loss: 0.1188\nEpoch 782/1000\n570/570 [==============================] - 11s 19ms/step - loss: 162.7079 - synergy_loss: 18.0194 - class_loss: 0.1183\nEpoch 783/1000\n570/570 [==============================] - 10s 18ms/step - loss: 162.7327 - synergy_loss: 18.2701 - class_loss: 0.1143\nEpoch 784/1000\n570/570 [==============================] - 10s 18ms/step - loss: 161.8830 - synergy_loss: 17.6351 - class_loss: 0.1181\nEpoch 785/1000\n570/570 [==============================] - 11s 19ms/step - loss: 162.2887 - synergy_loss: 18.2355 - class_loss: 0.1211\nEpoch 786/1000\n570/570 [==============================] - 10s 18ms/step - loss: 161.7135 - synergy_loss: 17.8847 - class_loss: 0.1168\nEpoch 787/1000\n570/570 [==============================] - 10s 18ms/step - loss: 161.6919 - synergy_loss: 18.0682 - class_loss: 0.1191\nEpoch 788/1000\n570/570 [==============================] - 11s 19ms/step - loss: 161.6879 - synergy_loss: 18.2805 - class_loss: 0.1162\nEpoch 789/1000\n570/570 [==============================] - 10s 18ms/step - loss: 161.3558 - synergy_loss: 18.1554 - class_loss: 0.1138\nEpoch 790/1000\n570/570 [==============================] - 10s 18ms/step - loss: 160.9892 - synergy_loss: 18.0021 - class_loss: 0.1157\nEpoch 791/1000\n570/570 [==============================] - 11s 19ms/step - loss: 160.5278 - synergy_loss: 17.7666 - class_loss: 0.1153\nEpoch 792/1000\n570/570 [==============================] - 10s 18ms/step - loss: 160.2416 - synergy_loss: 17.6831 - class_loss: 0.1160\nEpoch 793/1000\n570/570 [==============================] - 10s 18ms/step - loss: 160.1495 - synergy_loss: 17.8164 - class_loss: 0.1144\nEpoch 794/1000\n570/570 [==============================] - 11s 19ms/step - loss: 159.9619 - synergy_loss: 17.8444 - class_loss: 0.1178\nEpoch 795/1000\n570/570 [==============================] - 10s 18ms/step - loss: 159.8235 - synergy_loss: 17.9075 - class_loss: 0.1139\nEpoch 796/1000\n570/570 [==============================] - 10s 18ms/step - loss: 159.5338 - synergy_loss: 17.8420 - class_loss: 0.1143\nEpoch 797/1000\n570/570 [==============================] - 11s 19ms/step - loss: 159.0847 - synergy_loss: 17.6120 - class_loss: 0.1150\nEpoch 798/1000\n570/570 [==============================] - 10s 18ms/step - loss: 158.9905 - synergy_loss: 17.7438 - class_loss: 0.1133\nEpoch 799/1000\n570/570 [==============================] - 10s 18ms/step - loss: 159.1016 - synergy_loss: 18.0538 - class_loss: 0.1136\nEpoch 800/1000\n570/570 [==============================] - 11s 19ms/step - loss: 158.7852 - synergy_loss: 17.9405 - class_loss: 0.1134\nEpoch 801/1000\n570/570 [==============================] - 10s 18ms/step - loss: 158.3643 - synergy_loss: 17.7434 - class_loss: 0.1162\nEpoch 802/1000\n570/570 [==============================] - 10s 18ms/step - loss: 158.5427 - synergy_loss: 18.1126 - class_loss: 0.1141\nEpoch 803/1000\n570/570 [==============================] - 11s 19ms/step - loss: 157.6027 - synergy_loss: 17.3764 - class_loss: 0.1123\nEpoch 804/1000\n570/570 [==============================] - 10s 18ms/step - loss: 157.5078 - synergy_loss: 17.4919 - class_loss: 0.1122\nEpoch 805/1000\n570/570 [==============================] - 10s 18ms/step - loss: 157.6439 - synergy_loss: 17.8138 - class_loss: 0.1163\nEpoch 806/1000\n570/570 [==============================] - 11s 19ms/step - loss: 157.2043 - synergy_loss: 17.6003 - class_loss: 0.1126\nEpoch 807/1000\n570/570 [==============================] - 10s 18ms/step - loss: 156.6711 - synergy_loss: 17.2894 - class_loss: 0.1095\nEpoch 808/1000\n570/570 [==============================] - 10s 18ms/step - loss: 156.4801 - synergy_loss: 17.3110 - class_loss: 0.1123\nEpoch 809/1000\n570/570 [==============================] - 11s 19ms/step - loss: 156.7346 - synergy_loss: 17.7789 - class_loss: 0.1136\nEpoch 810/1000\n570/570 [==============================] - 10s 18ms/step - loss: 156.3223 - synergy_loss: 17.5792 - class_loss: 0.1110\nEpoch 811/1000\n570/570 [==============================] - 10s 18ms/step - loss: 156.0529 - synergy_loss: 17.5162 - class_loss: 0.1150\nEpoch 812/1000\n570/570 [==============================] - 10s 18ms/step - loss: 155.6614 - synergy_loss: 17.3602 - class_loss: 0.1114\nEpoch 813/1000\n570/570 [==============================] - 11s 19ms/step - loss: 155.7050 - synergy_loss: 17.6144 - class_loss: 0.1103\nEpoch 814/1000\n570/570 [==============================] - 10s 18ms/step - loss: 155.3794 - synergy_loss: 17.4717 - class_loss: 0.1104\nEpoch 815/1000\n570/570 [==============================] - 10s 18ms/step - loss: 155.6649 - synergy_loss: 17.9550 - class_loss: 0.1100\nEpoch 816/1000\n570/570 [==============================] - 11s 19ms/step - loss: 155.0808 - synergy_loss: 17.5641 - class_loss: 0.1138\nEpoch 817/1000\n570/570 [==============================] - 10s 18ms/step - loss: 154.7301 - synergy_loss: 17.4435 - class_loss: 0.1080\nEpoch 818/1000\n570/570 [==============================] - 10s 18ms/step - loss: 154.5073 - synergy_loss: 17.4290 - class_loss: 0.1123\nEpoch 819/1000\n570/570 [==============================] - 11s 19ms/step - loss: 154.4438 - synergy_loss: 17.5696 - class_loss: 0.1070\nEpoch 820/1000\n570/570 [==============================] - 10s 18ms/step - loss: 154.0605 - synergy_loss: 17.3941 - class_loss: 0.1118\nEpoch 821/1000\n570/570 [==============================] - 10s 18ms/step - loss: 154.3027 - synergy_loss: 17.8408 - class_loss: 0.1096\nEpoch 822/1000\n570/570 [==============================] - 11s 19ms/step - loss: 153.5802 - synergy_loss: 17.3242 - class_loss: 0.1065\nEpoch 823/1000\n570/570 [==============================] - 10s 18ms/step - loss: 153.3263 - synergy_loss: 17.2881 - class_loss: 0.1092\nEpoch 824/1000\n570/570 [==============================] - 10s 18ms/step - loss: 153.2406 - synergy_loss: 17.4109 - class_loss: 0.1086\nEpoch 825/1000\n570/570 [==============================] - 11s 19ms/step - loss: 153.1897 - synergy_loss: 17.5460 - class_loss: 0.1067\nEpoch 826/1000\n570/570 [==============================] - 10s 18ms/step - loss: 152.3712 - synergy_loss: 16.9492 - class_loss: 0.1081\nEpoch 827/1000\n570/570 [==============================] - 10s 18ms/step - loss: 152.8563 - synergy_loss: 17.6426 - class_loss: 0.1117\nEpoch 828/1000\n570/570 [==============================] - 11s 19ms/step - loss: 152.3110 - synergy_loss: 17.2929 - class_loss: 0.1100\nEpoch 829/1000\n570/570 [==============================] - 10s 18ms/step - loss: 151.9488 - synergy_loss: 17.1517 - class_loss: 0.1081\nEpoch 830/1000\n570/570 [==============================] - 10s 18ms/step - loss: 151.6859 - synergy_loss: 17.0954 - class_loss: 0.1086\nEpoch 831/1000\n570/570 [==============================] - 11s 19ms/step - loss: 151.8361 - synergy_loss: 17.4479 - class_loss: 0.1066\nEpoch 832/1000\n570/570 [==============================] - 11s 19ms/step - loss: 152.1580 - synergy_loss: 17.9366 - class_loss: 0.1069\nEpoch 833/1000\n570/570 [==============================] - 10s 18ms/step - loss: 151.4060 - synergy_loss: 17.3669 - class_loss: 0.1088\nEpoch 834/1000\n570/570 [==============================] - 11s 19ms/step - loss: 151.1035 - synergy_loss: 17.2843 - class_loss: 0.1078\nEpoch 835/1000\n570/570 [==============================] - 10s 18ms/step - loss: 150.8763 - synergy_loss: 17.2340 - class_loss: 0.1077\nEpoch 836/1000\n570/570 [==============================] - 10s 18ms/step - loss: 150.6533 - synergy_loss: 17.2255 - class_loss: 0.1059\nEpoch 837/1000\n570/570 [==============================] - 11s 19ms/step - loss: 150.5043 - synergy_loss: 17.2804 - class_loss: 0.1082\nEpoch 838/1000\n570/570 [==============================] - 10s 18ms/step - loss: 150.2467 - synergy_loss: 17.2200 - class_loss: 0.1064\nEpoch 839/1000\n570/570 [==============================] - 10s 18ms/step - loss: 150.0912 - synergy_loss: 17.2521 - class_loss: 0.1078\nEpoch 840/1000\n570/570 [==============================] - 11s 19ms/step - loss: 149.9953 - synergy_loss: 17.3535 - class_loss: 0.1075\nEpoch 841/1000\n570/570 [==============================] - 10s 18ms/step - loss: 149.7667 - synergy_loss: 17.3279 - class_loss: 0.1073\nEpoch 842/1000\n570/570 [==============================] - 10s 18ms/step - loss: 149.5794 - synergy_loss: 17.3424 - class_loss: 0.1067\nEpoch 843/1000\n570/570 [==============================] - 11s 19ms/step - loss: 149.0133 - synergy_loss: 16.9722 - class_loss: 0.1046\nEpoch 844/1000\n570/570 [==============================] - 10s 18ms/step - loss: 148.8447 - synergy_loss: 17.0087 - class_loss: 0.1031\nEpoch 845/1000\n570/570 [==============================] - 10s 18ms/step - loss: 148.5532 - synergy_loss: 16.9349 - class_loss: 0.1005\nEpoch 846/1000\n570/570 [==============================] - 11s 19ms/step - loss: 148.3674 - synergy_loss: 16.9367 - class_loss: 0.1055\nEpoch 847/1000\n570/570 [==============================] - 10s 18ms/step - loss: 148.1335 - synergy_loss: 16.9126 - class_loss: 0.0993\nEpoch 848/1000\n570/570 [==============================] - 10s 18ms/step - loss: 148.8291 - synergy_loss: 17.7673 - class_loss: 0.1064\nEpoch 849/1000\n570/570 [==============================] - 11s 19ms/step - loss: 148.0241 - synergy_loss: 17.1532 - class_loss: 0.1042\nEpoch 850/1000\n570/570 [==============================] - 10s 18ms/step - loss: 147.6665 - synergy_loss: 16.9976 - class_loss: 0.1046\nEpoch 851/1000\n570/570 [==============================] - 10s 18ms/step - loss: 147.3385 - synergy_loss: 16.8803 - class_loss: 0.1019\nEpoch 852/1000\n570/570 [==============================] - 11s 19ms/step - loss: 147.2315 - synergy_loss: 16.9875 - class_loss: 0.1020\nEpoch 853/1000\n570/570 [==============================] - 10s 18ms/step - loss: 146.9736 - synergy_loss: 16.9152 - class_loss: 0.1057\nEpoch 854/1000\n570/570 [==============================] - 10s 18ms/step - loss: 147.0122 - synergy_loss: 17.1489 - class_loss: 0.1054\nEpoch 855/1000\n570/570 [==============================] - 11s 20ms/step - loss: 146.7254 - synergy_loss: 17.0676 - class_loss: 0.1026\nEpoch 856/1000\n570/570 [==============================] - 10s 18ms/step - loss: 146.6109 - synergy_loss: 17.1448 - class_loss: 0.1054\nEpoch 857/1000\n570/570 [==============================] - 10s 18ms/step - loss: 146.2785 - synergy_loss: 17.0047 - class_loss: 0.1024\nEpoch 858/1000\n570/570 [==============================] - 11s 19ms/step - loss: 146.2055 - synergy_loss: 17.1183 - class_loss: 0.1044\nEpoch 859/1000\n570/570 [==============================] - 10s 18ms/step - loss: 145.6581 - synergy_loss: 16.7845 - class_loss: 0.1012\nEpoch 860/1000\n570/570 [==============================] - 10s 18ms/step - loss: 145.6222 - synergy_loss: 16.9466 - class_loss: 0.1007\nEpoch 861/1000\n570/570 [==============================] - 11s 19ms/step - loss: 145.5888 - synergy_loss: 17.1146 - class_loss: 0.1019\nEpoch 862/1000\n570/570 [==============================] - 10s 18ms/step - loss: 145.0871 - synergy_loss: 16.8102 - class_loss: 0.1009\nEpoch 863/1000\n570/570 [==============================] - 10s 18ms/step - loss: 145.1156 - synergy_loss: 17.0311 - class_loss: 0.1018\nEpoch 864/1000\n570/570 [==============================] - 11s 19ms/step - loss: 144.9690 - synergy_loss: 17.1016 - class_loss: 0.1005\nEpoch 865/1000\n570/570 [==============================] - 10s 18ms/step - loss: 144.7948 - synergy_loss: 17.0926 - class_loss: 0.1057\nEpoch 866/1000\n570/570 [==============================] - 10s 18ms/step - loss: 144.3157 - synergy_loss: 16.8322 - class_loss: 0.0996\nEpoch 867/1000\n570/570 [==============================] - 11s 19ms/step - loss: 143.8884 - synergy_loss: 16.6170 - class_loss: 0.1005\nEpoch 868/1000\n570/570 [==============================] - 10s 18ms/step - loss: 144.0866 - synergy_loss: 17.0098 - class_loss: 0.1000\nEpoch 869/1000\n570/570 [==============================] - 10s 18ms/step - loss: 144.0322 - synergy_loss: 17.1368 - class_loss: 0.0988\nEpoch 870/1000\n570/570 [==============================] - 11s 19ms/step - loss: 143.4285 - synergy_loss: 16.7427 - class_loss: 0.0995\nEpoch 871/1000\n570/570 [==============================] - 10s 18ms/step - loss: 143.0351 - synergy_loss: 16.5513 - class_loss: 0.1031\nEpoch 872/1000\n570/570 [==============================] - 10s 18ms/step - loss: 143.4586 - synergy_loss: 17.1610 - class_loss: 0.1019\nEpoch 873/1000\n570/570 [==============================] - 11s 19ms/step - loss: 143.0230 - synergy_loss: 16.9000 - class_loss: 0.0989\nEpoch 874/1000\n570/570 [==============================] - 10s 18ms/step - loss: 142.6906 - synergy_loss: 16.7616 - class_loss: 0.1002\nEpoch 875/1000\n570/570 [==============================] - 10s 18ms/step - loss: 142.7052 - synergy_loss: 16.9502 - class_loss: 0.0998\nEpoch 876/1000\n570/570 [==============================] - 11s 19ms/step - loss: 142.6296 - synergy_loss: 17.0820 - class_loss: 0.0983\nEpoch 877/1000\n570/570 [==============================] - 10s 18ms/step - loss: 141.9420 - synergy_loss: 16.5909 - class_loss: 0.0988\nEpoch 878/1000\n570/570 [==============================] - 11s 19ms/step - loss: 141.9381 - synergy_loss: 16.7876 - class_loss: 0.0984\nEpoch 879/1000\n570/570 [==============================] - 11s 19ms/step - loss: 141.6899 - synergy_loss: 16.7145 - class_loss: 0.1006\nEpoch 880/1000\n570/570 [==============================] - 11s 19ms/step - loss: 141.5900 - synergy_loss: 16.8276 - class_loss: 0.0969\nEpoch 881/1000\n570/570 [==============================] - 10s 18ms/step - loss: 141.3129 - synergy_loss: 16.7280 - class_loss: 0.0974\nEpoch 882/1000\n570/570 [==============================] - 11s 19ms/step - loss: 141.0430 - synergy_loss: 16.6519 - class_loss: 0.0962\nEpoch 883/1000\n570/570 [==============================] - 10s 18ms/step - loss: 140.7040 - synergy_loss: 16.4939 - class_loss: 0.0967\nEpoch 884/1000\n570/570 [==============================] - 10s 18ms/step - loss: 140.4511 - synergy_loss: 16.4479 - class_loss: 0.0979\nEpoch 885/1000\n570/570 [==============================] - 11s 19ms/step - loss: 140.6880 - synergy_loss: 16.8796 - class_loss: 0.1003\nEpoch 886/1000\n570/570 [==============================] - 10s 18ms/step - loss: 140.6323 - synergy_loss: 17.0094 - class_loss: 0.0980\nEpoch 887/1000\n570/570 [==============================] - 10s 18ms/step - loss: 139.7259 - synergy_loss: 16.2962 - class_loss: 0.0983\nEpoch 888/1000\n570/570 [==============================] - 11s 18ms/step - loss: 139.9496 - synergy_loss: 16.7065 - class_loss: 0.0998\nEpoch 889/1000\n570/570 [==============================] - 10s 18ms/step - loss: 140.1455 - synergy_loss: 17.0708 - class_loss: 0.0982\nEpoch 890/1000\n570/570 [==============================] - 10s 18ms/step - loss: 139.1986 - synergy_loss: 16.3258 - class_loss: 0.0987\nEpoch 891/1000\n570/570 [==============================] - 11s 18ms/step - loss: 139.0791 - synergy_loss: 16.4026 - class_loss: 0.0982\nEpoch 892/1000\n570/570 [==============================] - 10s 18ms/step - loss: 139.1859 - synergy_loss: 16.7031 - class_loss: 0.0973\nEpoch 893/1000\n570/570 [==============================] - 10s 18ms/step - loss: 139.4443 - synergy_loss: 17.1280 - class_loss: 0.0979\nEpoch 894/1000\n570/570 [==============================] - 11s 19ms/step - loss: 139.4954 - synergy_loss: 17.3170 - class_loss: 0.0999\nEpoch 895/1000\n570/570 [==============================] - 10s 18ms/step - loss: 138.6305 - synergy_loss: 16.6562 - class_loss: 0.0998\nEpoch 896/1000\n570/570 [==============================] - 10s 18ms/step - loss: 138.4373 - synergy_loss: 16.6457 - class_loss: 0.0978\nEpoch 897/1000\n570/570 [==============================] - 10s 18ms/step - loss: 138.2820 - synergy_loss: 16.6757 - class_loss: 0.1009\nEpoch 898/1000\n570/570 [==============================] - 11s 19ms/step - loss: 137.9944 - synergy_loss: 16.5741 - class_loss: 0.0971\nEpoch 899/1000\n570/570 [==============================] - 10s 18ms/step - loss: 137.6994 - synergy_loss: 16.4790 - class_loss: 0.0937\nEpoch 900/1000\n570/570 [==============================] - 10s 18ms/step - loss: 137.4836 - synergy_loss: 16.4475 - class_loss: 0.0968\nEpoch 901/1000\n570/570 [==============================] - 11s 19ms/step - loss: 137.9879 - synergy_loss: 17.1077 - class_loss: 0.0968\nEpoch 902/1000\n570/570 [==============================] - 10s 18ms/step - loss: 136.8839 - synergy_loss: 16.2025 - class_loss: 0.0933\nEpoch 903/1000\n570/570 [==============================] - 10s 18ms/step - loss: 137.1198 - synergy_loss: 16.6201 - class_loss: 0.0955\nEpoch 904/1000\n570/570 [==============================] - 11s 19ms/step - loss: 137.7980 - synergy_loss: 17.3761 - class_loss: 0.1007\nEpoch 905/1000\n570/570 [==============================] - 10s 18ms/step - loss: 136.2369 - synergy_loss: 16.0296 - class_loss: 0.0971\nEpoch 906/1000\n570/570 [==============================] - 10s 18ms/step - loss: 136.3281 - synergy_loss: 16.3139 - class_loss: 0.0944\nEpoch 907/1000\n570/570 [==============================] - 11s 19ms/step - loss: 136.4086 - synergy_loss: 16.5704 - class_loss: 0.0986\nEpoch 908/1000\n570/570 [==============================] - 10s 18ms/step - loss: 136.3332 - synergy_loss: 16.6568 - class_loss: 0.0946\nEpoch 909/1000\n570/570 [==============================] - 10s 18ms/step - loss: 135.8499 - synergy_loss: 16.3657 - class_loss: 0.0950\nEpoch 910/1000\n570/570 [==============================] - 11s 19ms/step - loss: 136.1168 - synergy_loss: 16.8013 - class_loss: 0.0966\nEpoch 911/1000\n570/570 [==============================] - 10s 18ms/step - loss: 135.7758 - synergy_loss: 16.6170 - class_loss: 0.0941\nEpoch 912/1000\n570/570 [==============================] - 10s 18ms/step - loss: 135.2910 - synergy_loss: 16.3233 - class_loss: 0.0955\nEpoch 913/1000\n570/570 [==============================] - 11s 19ms/step - loss: 134.9529 - synergy_loss: 16.1617 - class_loss: 0.0937\nEpoch 914/1000\n570/570 [==============================] - 10s 18ms/step - loss: 134.8983 - synergy_loss: 16.2954 - class_loss: 0.0939\nEpoch 915/1000\n570/570 [==============================] - 10s 18ms/step - loss: 134.9964 - synergy_loss: 16.5574 - class_loss: 0.0947\nEpoch 916/1000\n570/570 [==============================] - 11s 19ms/step - loss: 134.3848 - synergy_loss: 16.1289 - class_loss: 0.0923\nEpoch 917/1000\n570/570 [==============================] - 10s 18ms/step - loss: 134.6567 - synergy_loss: 16.5873 - class_loss: 0.0960\nEpoch 918/1000\n570/570 [==============================] - 10s 18ms/step - loss: 134.5345 - synergy_loss: 16.6526 - class_loss: 0.0963\nEpoch 919/1000\n570/570 [==============================] - 11s 19ms/step - loss: 133.7571 - synergy_loss: 16.0596 - class_loss: 0.0948\nEpoch 920/1000\n570/570 [==============================] - 10s 18ms/step - loss: 134.0414 - synergy_loss: 16.5095 - class_loss: 0.0976\nEpoch 921/1000\n570/570 [==============================] - 10s 18ms/step - loss: 133.6244 - synergy_loss: 16.2856 - class_loss: 0.0942\nEpoch 922/1000\n570/570 [==============================] - 11s 19ms/step - loss: 133.2797 - synergy_loss: 16.1302 - class_loss: 0.0945\nEpoch 923/1000\n570/570 [==============================] - 10s 18ms/step - loss: 133.2725 - synergy_loss: 16.2984 - class_loss: 0.0951\nEpoch 924/1000\n570/570 [==============================] - 11s 18ms/step - loss: 132.9681 - synergy_loss: 16.1859 - class_loss: 0.0929\nEpoch 925/1000\n570/570 [==============================] - 11s 19ms/step - loss: 133.0616 - synergy_loss: 16.4544 - class_loss: 0.0937\nEpoch 926/1000\n570/570 [==============================] - 10s 18ms/step - loss: 132.9347 - synergy_loss: 16.4839 - class_loss: 0.0936\nEpoch 927/1000\n570/570 [==============================] - 10s 18ms/step - loss: 132.6240 - synergy_loss: 16.3482 - class_loss: 0.0937\nEpoch 928/1000\n570/570 [==============================] - 11s 19ms/step - loss: 132.3133 - synergy_loss: 16.2164 - class_loss: 0.0924\nEpoch 929/1000\n570/570 [==============================] - 10s 18ms/step - loss: 132.2356 - synergy_loss: 16.3302 - class_loss: 0.0917\nEpoch 930/1000\n570/570 [==============================] - 10s 18ms/step - loss: 132.1219 - synergy_loss: 16.3837 - class_loss: 0.0905\nEpoch 931/1000\n570/570 [==============================] - 11s 19ms/step - loss: 131.9244 - synergy_loss: 16.3616 - class_loss: 0.0931\nEpoch 932/1000\n570/570 [==============================] - 10s 18ms/step - loss: 132.1398 - synergy_loss: 16.7292 - class_loss: 0.0970\nEpoch 933/1000\n570/570 [==============================] - 10s 18ms/step - loss: 131.4266 - synergy_loss: 16.1983 - class_loss: 0.0933\nEpoch 934/1000\n570/570 [==============================] - 11s 19ms/step - loss: 131.3845 - synergy_loss: 16.3333 - class_loss: 0.0907\nEpoch 935/1000\n570/570 [==============================] - 10s 18ms/step - loss: 131.2313 - synergy_loss: 16.3655 - class_loss: 0.0948\nEpoch 936/1000\n570/570 [==============================] - 11s 19ms/step - loss: 130.6014 - synergy_loss: 15.9085 - class_loss: 0.0904\nEpoch 937/1000\n570/570 [==============================] - 11s 19ms/step - loss: 130.6561 - synergy_loss: 16.1451 - class_loss: 0.0914\nEpoch 938/1000\n570/570 [==============================] - 10s 18ms/step - loss: 130.6481 - synergy_loss: 16.3053 - class_loss: 0.0918\nEpoch 939/1000\n570/570 [==============================] - 10s 18ms/step - loss: 130.5360 - synergy_loss: 16.3562 - class_loss: 0.0938\nEpoch 940/1000\n570/570 [==============================] - 11s 19ms/step - loss: 130.2711 - synergy_loss: 16.2759 - class_loss: 0.0893\nEpoch 941/1000\n570/570 [==============================] - 10s 18ms/step - loss: 130.2547 - synergy_loss: 16.4197 - class_loss: 0.0919\nEpoch 942/1000\n570/570 [==============================] - 10s 18ms/step - loss: 129.6392 - synergy_loss: 15.9718 - class_loss: 0.0925\nEpoch 943/1000\n570/570 [==============================] - 11s 19ms/step - loss: 129.6154 - synergy_loss: 16.1211 - class_loss: 0.0891\nEpoch 944/1000\n570/570 [==============================] - 10s 18ms/step - loss: 129.3594 - synergy_loss: 16.0471 - class_loss: 0.0909\nEpoch 945/1000\n570/570 [==============================] - 10s 18ms/step - loss: 129.0457 - synergy_loss: 15.9139 - class_loss: 0.0888\nEpoch 946/1000\n570/570 [==============================] - 11s 19ms/step - loss: 129.2916 - synergy_loss: 16.3217 - class_loss: 0.0916\nEpoch 947/1000\n570/570 [==============================] - 10s 18ms/step - loss: 129.0598 - synergy_loss: 16.2528 - class_loss: 0.0901\nEpoch 948/1000\n570/570 [==============================] - 10s 18ms/step - loss: 128.6474 - synergy_loss: 16.0116 - class_loss: 0.0917\nEpoch 949/1000\n570/570 [==============================] - 11s 19ms/step - loss: 128.7016 - synergy_loss: 16.2504 - class_loss: 0.0886\nEpoch 950/1000\n570/570 [==============================] - 10s 18ms/step - loss: 128.6097 - synergy_loss: 16.3135 - class_loss: 0.0922\nEpoch 951/1000\n570/570 [==============================] - 10s 18ms/step - loss: 127.9847 - synergy_loss: 15.8691 - class_loss: 0.0874\nEpoch 952/1000\n570/570 [==============================] - 11s 19ms/step - loss: 128.7346 - synergy_loss: 16.7682 - class_loss: 0.0912\nEpoch 953/1000\n570/570 [==============================] - 10s 18ms/step - loss: 127.8174 - synergy_loss: 15.9997 - class_loss: 0.0884\nEpoch 954/1000\n570/570 [==============================] - 10s 18ms/step - loss: 127.5396 - synergy_loss: 15.9019 - class_loss: 0.0886\nEpoch 955/1000\n570/570 [==============================] - 11s 19ms/step - loss: 127.2132 - synergy_loss: 15.7546 - class_loss: 0.0930\nEpoch 956/1000\n570/570 [==============================] - 10s 18ms/step - loss: 127.6883 - synergy_loss: 16.3899 - class_loss: 0.0892\nEpoch 957/1000\n570/570 [==============================] - 10s 18ms/step - loss: 127.5070 - synergy_loss: 16.3593 - class_loss: 0.0906\nEpoch 958/1000\n570/570 [==============================] - 11s 19ms/step - loss: 127.5758 - synergy_loss: 16.5601 - class_loss: 0.0900\nEpoch 959/1000\n570/570 [==============================] - 10s 18ms/step - loss: 126.6663 - synergy_loss: 15.8309 - class_loss: 0.0890\nEpoch 960/1000\n570/570 [==============================] - 10s 18ms/step - loss: 126.6079 - synergy_loss: 15.9424 - class_loss: 0.0887\nEpoch 961/1000\n570/570 [==============================] - 11s 19ms/step - loss: 126.0918 - synergy_loss: 15.5964 - class_loss: 0.0914\nEpoch 962/1000\n570/570 [==============================] - 10s 18ms/step - loss: 126.5142 - synergy_loss: 16.1869 - class_loss: 0.0902\nEpoch 963/1000\n570/570 [==============================] - 10s 18ms/step - loss: 126.4522 - synergy_loss: 16.2798 - class_loss: 0.0877\nEpoch 964/1000\n570/570 [==============================] - 11s 19ms/step - loss: 126.1492 - synergy_loss: 16.1086 - class_loss: 0.0860\nEpoch 965/1000\n570/570 [==============================] - 10s 18ms/step - loss: 125.8722 - synergy_loss: 16.0193 - class_loss: 0.0880\nEpoch 966/1000\n570/570 [==============================] - 10s 18ms/step - loss: 125.3782 - synergy_loss: 15.6933 - class_loss: 0.0843\nEpoch 967/1000\n570/570 [==============================] - 11s 19ms/step - loss: 125.3695 - synergy_loss: 15.8492 - class_loss: 0.0872\nEpoch 968/1000\n570/570 [==============================] - 11s 18ms/step - loss: 125.4802 - synergy_loss: 16.1039 - class_loss: 0.0885\nEpoch 969/1000\n570/570 [==============================] - 10s 18ms/step - loss: 125.3103 - synergy_loss: 16.1046 - class_loss: 0.0874\nEpoch 970/1000\n570/570 [==============================] - 11s 19ms/step - loss: 124.9516 - synergy_loss: 15.9045 - class_loss: 0.0896\nEpoch 971/1000\n570/570 [==============================] - 10s 18ms/step - loss: 124.7011 - synergy_loss: 15.8352 - class_loss: 0.0874\nEpoch 972/1000\n570/570 [==============================] - 10s 18ms/step - loss: 124.7992 - synergy_loss: 16.1080 - class_loss: 0.0868\nEpoch 973/1000\n570/570 [==============================] - 11s 19ms/step - loss: 124.4035 - synergy_loss: 15.8666 - class_loss: 0.0880\nEpoch 974/1000\n570/570 [==============================] - 10s 18ms/step - loss: 124.4301 - synergy_loss: 16.0483 - class_loss: 0.0900\nEpoch 975/1000\n570/570 [==============================] - 10s 18ms/step - loss: 123.8702 - synergy_loss: 15.6523 - class_loss: 0.0885\nEpoch 976/1000\n570/570 [==============================] - 11s 19ms/step - loss: 124.1316 - synergy_loss: 16.0786 - class_loss: 0.0855\nEpoch 977/1000\n570/570 [==============================] - 10s 18ms/step - loss: 123.9717 - synergy_loss: 16.0333 - class_loss: 0.0867\nEpoch 978/1000\n570/570 [==============================] - 10s 18ms/step - loss: 123.3036 - synergy_loss: 15.5550 - class_loss: 0.0880\nEpoch 979/1000\n570/570 [==============================] - 11s 19ms/step - loss: 123.2994 - synergy_loss: 15.7158 - class_loss: 0.0887\nEpoch 980/1000\n570/570 [==============================] - 10s 18ms/step - loss: 123.2785 - synergy_loss: 15.8521 - class_loss: 0.0867\nEpoch 981/1000\n570/570 [==============================] - 10s 18ms/step - loss: 122.9762 - synergy_loss: 15.7138 - class_loss: 0.0856\nEpoch 982/1000\n570/570 [==============================] - 11s 19ms/step - loss: 122.6735 - synergy_loss: 15.5708 - class_loss: 0.0897\nEpoch 983/1000\n570/570 [==============================] - 10s 18ms/step - loss: 122.7663 - synergy_loss: 15.8273 - class_loss: 0.0868\nEpoch 984/1000\n570/570 [==============================] - 10s 18ms/step - loss: 122.6139 - synergy_loss: 15.8342 - class_loss: 0.0871\nEpoch 985/1000\n570/570 [==============================] - 10s 18ms/step - loss: 122.3294 - synergy_loss: 15.7220 - class_loss: 0.0862\nEpoch 986/1000\n570/570 [==============================] - 10s 18ms/step - loss: 121.9087 - synergy_loss: 15.4579 - class_loss: 0.0859\nEpoch 987/1000\n570/570 [==============================] - 10s 18ms/step - loss: 121.9030 - synergy_loss: 15.6318 - class_loss: 0.0846\nEpoch 988/1000\n570/570 [==============================] - 10s 18ms/step - loss: 121.9557 - synergy_loss: 15.8304 - class_loss: 0.0840\nEpoch 989/1000\n570/570 [==============================] - 11s 19ms/step - loss: 121.4970 - synergy_loss: 15.5439 - class_loss: 0.0860\nEpoch 990/1000\n570/570 [==============================] - 10s 18ms/step - loss: 121.3188 - synergy_loss: 15.5246 - class_loss: 0.0826\nEpoch 991/1000\n570/570 [==============================] - 10s 18ms/step - loss: 121.1982 - synergy_loss: 15.5653 - class_loss: 0.0851\nEpoch 992/1000\n570/570 [==============================] - 11s 19ms/step - loss: 121.3193 - synergy_loss: 15.8257 - class_loss: 0.0856\nEpoch 993/1000\n570/570 [==============================] - 11s 18ms/step - loss: 121.1065 - synergy_loss: 15.7638 - class_loss: 0.0853\nEpoch 994/1000\n570/570 [==============================] - 10s 18ms/step - loss: 120.9084 - synergy_loss: 15.7230 - class_loss: 0.0834\nEpoch 995/1000\n570/570 [==============================] - 11s 19ms/step - loss: 120.6237 - synergy_loss: 15.6161 - class_loss: 0.0851\nEpoch 996/1000\n570/570 [==============================] - 10s 18ms/step - loss: 120.1538 - synergy_loss: 15.2937 - class_loss: 0.0832\nEpoch 997/1000\n570/570 [==============================] - 10s 18ms/step - loss: 120.1987 - synergy_loss: 15.5129 - class_loss: 0.0818\nEpoch 998/1000\n570/570 [==============================] - 11s 19ms/step - loss: 120.1963 - synergy_loss: 15.6590 - class_loss: 0.0827\nEpoch 999/1000\n570/570 [==============================] - 10s 18ms/step - loss: 119.9109 - synergy_loss: 15.5307 - class_loss: 0.0867\nEpoch 1000/1000\n570/570 [==============================] - 10s 18ms/step - loss: 120.0978 - synergy_loss: 15.8769 - class_loss: 0.0866\n283/283 [==============================] - 3s 5ms/step\nmsynergy_mean_squared_error 228.11581\nmclass_mean_squared_error 9.150663\nmsynergy_r2_score 0.5818054761234672\nmsynergy_pear (array([0.7647125053938936], dtype=object), 0.0)\nmsynergy_spear SpearmanrResult(correlation=0.7622801684054136, pvalue=0.0)\nmsclass_roc_curve 0.9551563009635212\nmclass_accuracy_scorer 0.890686274509804\nmclass_cohen_kappa_score 0.6206395057939785\nmclass_precision_score 0.9556451612903226\nmclass_average_precision_score 0.8875894863271211\n","output_type":"stream"}]},{"cell_type":"code","source":"   \n\nfrom IPython.display import FileLink    \nnp.savetxt('npred_syn2.csv', ap11 ,delimiter=',')\nFileLink(r'npred_syn2.csv')\n\nnp.savetxt('npred_cls2.csv', ap22 ,delimiter=',') \nFileLink(r'npred_cls2.csv')\n \nnp.savetxt('ntest_syn2.csv', test_synergy1 ,delimiter=',')\nFileLink(r'ntest_syn2.csv')\n\nnp.savetxt('ntest_cls2.csv', test_class1 ,delimiter=',')\nFileLink(r'ntest_cls2.csv')\n\n","metadata":{"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/ntest_cls2.csv","text/html":"<a href='ntest_cls2.csv' target='_blank'>ntest_cls2.csv</a><br>"},"metadata":{}}]},{"cell_type":"markdown","source":"##### ","metadata":{}}]}